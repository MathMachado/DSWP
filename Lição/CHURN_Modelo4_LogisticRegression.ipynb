{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVAKStbUuN98L4aPKuSNox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cryssoga/DSWP/blob/master/Li%C3%A7%C3%A3o/CHURN_Modelo4_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlVX_Gr4c7mi"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url_train = 'https://raw.githubusercontent.com/cryssoga/DSWP/master/Li%C3%A7%C3%A3o/train.csv'\n",
        "url_test = 'https://raw.githubusercontent.com/cryssoga/DSWP/master/Li%C3%A7%C3%A3o/test.csv'\n",
        "url_sample_submission = 'https://raw.githubusercontent.com/cryssoga/DSWP/master/Li%C3%A7%C3%A3o/sample_submission.csv'\n",
        "\n",
        "# Lendo/carregando os dataframes:\n",
        "\n",
        "df_train = pd.read_csv(url_train)\n",
        "df_test = pd.read_csv(url_test)\n",
        "df_sample_submission = pd.read_csv(url_sample_submission)\n",
        "\n",
        "# 'TotalCharges' de object para numeric, mas antes tratando os campos ' ':\n",
        "\n",
        "df_train[df_train['TotalCharges']==' ']\n",
        "df_train['TotalCharges'] = df_train['TotalCharges'].replace(' ', '0')\n",
        "df_train['TotalCharges'] = pd.to_numeric(df_train['TotalCharges'])\n",
        "\n",
        "df_test[df_test['TotalCharges']==' ']\n",
        "df_test['TotalCharges'] = df_test['TotalCharges'].replace(' ', '0')\n",
        "df_test['TotalCharges'] = pd.to_numeric(df_test['TotalCharges'])\n",
        "\n",
        "# Tratando os missing values nas variáveis 'PaymentMethod' (mais frequente), 'Dependents' (mais frequente) e 'tenure' (mediana)\n",
        "\n",
        "df_train['PaymentMethod'].fillna('Eletronic check', inplace=True)\n",
        "df_train['Dependents'].fillna('No',inplace = True)\n",
        "df_train['tenure'].fillna(df_train['tenure'].median(), inplace=True)\n",
        "\n",
        "df_test['PaymentMethod'].fillna('Eletronic check',inplace = True)\n",
        "df_test['Dependents'].fillna('No', inplace=True)\n",
        "df_test['tenure'].fillna(df_test['tenure'].median(),inplace = True)\n",
        "\n",
        "# Dividindo df_train em df_X_train e df_y_train:\n",
        "df_X_train = df_train.drop('Churn',axis=1)\n",
        "df_X_train.head()\n",
        "df_y_train = df_train['Churn']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb6ZikXjdwoF"
      },
      "source": [
        "X_treinamento = df_X_train\n",
        "y_treinamento = df_y_train\n",
        "X_teste = df_test\n",
        "y_teste = df_sample_submission['Churn']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy7wRioJAebQ"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "le = LabelEncoder()\n",
        "    # 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
        "    # 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "    # 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
        "    # 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
        "\n",
        "X_treinamento['gender_le'] = le.fit_transform(X_treinamento['gender'])\n",
        "X_treinamento['SeniorCitizen_le'] = le.fit_transform(X_treinamento['SeniorCitizen'])\n",
        "X_treinamento['Partner_le'] = le.fit_transform(X_treinamento['Partner'])\n",
        "X_treinamento['Dependents_le'] = le.fit_transform(X_treinamento['Dependents'])\n",
        "X_treinamento['PhoneService_le'] = le.fit_transform(X_treinamento['PhoneService'])\n",
        "X_treinamento['MultipleLines_le'] = le.fit_transform(X_treinamento['MultipleLines'])\n",
        "X_treinamento['InternetService_le'] = le.fit_transform(X_treinamento['InternetService'])\n",
        "X_treinamento['OnlineSecurity_le'] = le.fit_transform(X_treinamento['OnlineSecurity'])\n",
        "X_treinamento['OnlineBackup_le'] = le.fit_transform(X_treinamento['OnlineBackup'])\n",
        "X_treinamento['DeviceProtection_le'] = le.fit_transform(X_treinamento['DeviceProtection'])\n",
        "X_treinamento['TechSupport_le'] = le.fit_transform(X_treinamento['TechSupport'])\n",
        "X_treinamento['StreamingTV_le'] = le.fit_transform(X_treinamento['StreamingTV'])\n",
        "X_treinamento['StreamingMovies_le'] = le.fit_transform(X_treinamento['StreamingMovies'])\n",
        "X_treinamento['Contract_le'] = le.fit_transform(X_treinamento['Contract'])\n",
        "X_treinamento['PaperlessBilling_le'] = le.fit_transform(X_treinamento['PaperlessBilling'])\n",
        "X_treinamento['PaymentMethod_le'] = le.fit_transform(X_treinamento['PaymentMethod'])\n",
        "\n",
        "X_teste['gender_le'] = le.fit_transform(X_teste['gender'])\n",
        "X_teste['SeniorCitizen_le'] = le.fit_transform(X_teste['SeniorCitizen'])\n",
        "X_teste['Partner_le'] = le.fit_transform(X_teste['Partner'])\n",
        "X_teste['Dependents_le'] = le.fit_transform(X_teste['Dependents'])\n",
        "X_teste['PhoneService_le'] = le.fit_transform(X_teste['PhoneService'])\n",
        "X_teste['MultipleLines_le'] = le.fit_transform(X_teste['MultipleLines'])\n",
        "X_teste['InternetService_le'] = le.fit_transform(X_teste['InternetService'])\n",
        "X_teste['OnlineSecurity_le'] = le.fit_transform(X_teste['OnlineSecurity'])\n",
        "X_teste['OnlineBackup_le'] = le.fit_transform(X_teste['OnlineBackup'])\n",
        "X_teste['DeviceProtection_le'] = le.fit_transform(X_teste['DeviceProtection'])\n",
        "X_teste['TechSupport_le'] = le.fit_transform(X_teste['TechSupport'])\n",
        "X_teste['StreamingTV_le'] = le.fit_transform(X_teste['StreamingTV'])\n",
        "X_teste['StreamingMovies_le'] = le.fit_transform(X_teste['StreamingMovies'])\n",
        "X_teste['Contract_le'] = le.fit_transform(X_teste['Contract'])\n",
        "X_teste['PaperlessBilling_le'] = le.fit_transform(X_teste['PaperlessBilling'])\n",
        "X_teste['PaymentMethod_le'] = le.fit_transform(X_teste['PaymentMethod'])\n",
        "\n",
        "X_train = X_treinamento.copy()\n",
        "X_test = X_teste.copy()\n",
        "\n",
        "X_treinamento = X_treinamento.drop(['id','gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
        "                                 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                                 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
        "                                 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod'],axis=1)\n",
        "\n",
        "X_teste = X_teste.drop(['id','gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
        "                                 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                                 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
        "                                 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod'],axis=1)\n",
        "l_colunas = X_treinamento.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOcEuEMrfBEh"
      },
      "source": [
        "# Definindo variáveis globais:\n",
        "i_CV = 10     # número de Cross-Validations (CV)\n",
        "i_Seed = 20111974\n",
        "\n",
        "from sklearn.model_selection import cross_val_score     # para CV (Cross-Validation)\n",
        "from sklearn.model_selection import GridSearchCV        # para GridSearchCV (otimizar os parâmetros dos modelos preditivos)\n",
        "from time import time                                   # requerida na função gridsearchcv\n",
        "from sklearn.ensemble import RandomForestClassifier     # para o RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix            # para plotar a confusion matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyKeG24repEU"
      },
      "source": [
        "# Função para Cross-Validation:\n",
        "\n",
        "def funcao_cross_val_score(modelo, X_treinamento, y_treinamento, CV):\n",
        "    \n",
        "    #versão com cross_val_score:\n",
        "    a_scores_CV = cross_val_score(modelo, X_treinamento, y_treinamento, cv = CV)\n",
        "    print(f'Média das Acurácias calculadas pelo CV....: {100*round(a_scores_CV.mean(),4)}')\n",
        "    print(f'std médio das Acurácias calculadas pelo CV: {100*round(a_scores_CV.std(),4)}')\n",
        "    return a_scores_CV  # é um array com os scores a cada iteração do CV\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aceuye7hmqx"
      },
      "source": [
        "# Função para Confusion Matrix:\n",
        "\n",
        "def mostra_confusion_matrix(cf, \n",
        "                            group_names = None, \n",
        "                            categories = 'auto', \n",
        "                            count = True, \n",
        "                            percent = True, \n",
        "                            cbar = True, \n",
        "                            xyticks = False, \n",
        "                            xyplotlabels = True, \n",
        "                            sum_stats = True, \n",
        "                            figsize = (8, 8), \n",
        "                            cmap = 'Blues'):\n",
        "    '''\n",
        "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
        "    Arguments\n",
        "    ---------\n",
        "    cf:            confusion matrix to be passed in\n",
        "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
        "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
        "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
        "    normalize:     If True, show the proportions for each category. Default is True.\n",
        "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
        "                   Default is True.\n",
        "    xyticks:       If True, show x and y ticks. Default is True.\n",
        "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
        "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
        "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
        "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
        "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "    '''\n",
        "\n",
        "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
        "    blanks = ['' for i in range(cf.size)]\n",
        "\n",
        "    if group_names and len(group_names)==cf.size:\n",
        "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
        "    else:\n",
        "        group_labels = blanks\n",
        "\n",
        "    if count:\n",
        "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
        "    else:\n",
        "        group_counts = blanks\n",
        "\n",
        "    if percent:\n",
        "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
        "    else:\n",
        "        group_percentages = blanks\n",
        "\n",
        "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
        "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
        "\n",
        "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
        "    if sum_stats:\n",
        "        #Accuracy is sum of diagonal divided by total observations\n",
        "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
        "\n",
        "        #if it is a binary confusion matrix, show some more stats\n",
        "        if len(cf)==2:\n",
        "            #Metrics for Binary Confusion Matrices\n",
        "            precision = cf[1,1] / sum(cf[:,1])\n",
        "            recall    = cf[1,1] / sum(cf[1,:])\n",
        "            f1_score  = 2*precision*recall / (precision + recall)\n",
        "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(accuracy,precision,recall,f1_score)\n",
        "        else:\n",
        "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
        "    else:\n",
        "        stats_text = \"\"\n",
        "\n",
        "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
        "    if figsize==None:\n",
        "        #Get default figure size if not set\n",
        "        figsize = plt.rcParams.get('figure.figsize')\n",
        "\n",
        "    if xyticks==False:\n",
        "        #Do not show categories if xyticks is False\n",
        "        categories=False\n",
        "\n",
        "    # MAKE THE HEATMAP VISUALIZATION\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
        "\n",
        "    if xyplotlabels:\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label' + stats_text)\n",
        "    else:\n",
        "        plt.xlabel(stats_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq1aiuUsfKdU"
      },
      "source": [
        "# função para GridSearchCV:\n",
        "\n",
        "def GridSearchOptimizer(modelo, ml_Opt, d_Parametros, X_treinamento, y_treinamento, X_teste, y_teste, i_CV, l_colunas):\n",
        "    ml_GridSearchCV = GridSearchCV(modelo, d_Parametros, cv = i_CV, n_jobs = -1, verbose= 10, scoring = 'accuracy')\n",
        "    start = time()\n",
        "    ml_GridSearchCV.fit(X_treinamento, y_treinamento)\n",
        "    tempo_elapsed = time()-start\n",
        "    #print(f\"\\nGridSearchCV levou {tempo_elapsed:.2f} segundos.\")\n",
        "\n",
        "    # Parâmetros que otimizam a classificação:\n",
        "    print(f'\\nParametros otimizados: {ml_GridSearchCV.best_params_}')\n",
        "    \n",
        "    if ml_Opt == 'ml_DT2':\n",
        "        print(f'\\nDecisionTreeClassifier *********************************************************************************************************')\n",
        "        ml_Opt = DecisionTreeClassifier(criterion= ml_GridSearchCV.best_params_['criterion'], \n",
        "                                        max_depth= ml_GridSearchCV.best_params_['max_depth'],\n",
        "                                        max_leaf_nodes= ml_GridSearchCV.best_params_['max_leaf_nodes'],\n",
        "                                        min_samples_split= ml_GridSearchCV.best_params_['min_samples_leaf'],\n",
        "                                        min_samples_leaf= ml_GridSearchCV.best_params_['min_samples_split'], \n",
        "                                        random_state= i_Seed)\n",
        "        \n",
        "    elif ml_Opt == 'ml_RF2':\n",
        "        print(f'\\nRandomForestClassifier *********************************************************************************************************')\n",
        "        ml_Opt = RandomForestClassifier(bootstrap= ml_GridSearchCV.best_params_['bootstrap'], \n",
        "                                        max_depth= ml_GridSearchCV.best_params_['max_depth'],\n",
        "                                        max_features= ml_GridSearchCV.best_params_['max_features'],\n",
        "                                        min_samples_leaf= ml_GridSearchCV.best_params_['min_samples_leaf'],\n",
        "                                        min_samples_split= ml_GridSearchCV.best_params_['min_samples_split'],\n",
        "                                        n_estimators= ml_GridSearchCV.best_params_['n_estimators'],\n",
        "                                        random_state= i_Seed)\n",
        "        \n",
        "    elif ml_Opt == 'ml_AB2':\n",
        "        print(f'\\nAdaBoostClassifier *********************************************************************************************************')\n",
        "        ml_Opt = AdaBoostClassifier(algorithm='SAMME.R', \n",
        "                                    base_estimator=RandomForestClassifier(bootstrap = False, \n",
        "                                                                          max_depth = 10, \n",
        "                                                                          max_features = 'auto', \n",
        "                                                                          min_samples_leaf = 1, \n",
        "                                                                          min_samples_split = 2, \n",
        "                                                                          n_estimators = 400), \n",
        "                                    learning_rate = ml_GridSearchCV.best_params_['learning_rate'], \n",
        "                                    n_estimators = ml_GridSearchCV.best_params_['n_estimators'], \n",
        "                                    random_state = i_Seed)\n",
        "        \n",
        "    elif ml_Opt == 'ml_GB2':\n",
        "        print(f'\\nGradientBoostingClassifier *********************************************************************************************************')\n",
        "        ml_Opt = GradientBoostingClassifier(learning_rate = ml_GridSearchCV.best_params_['learning_rate'], \n",
        "                                            n_estimators = ml_GridSearchCV.best_params_['n_estimators'], \n",
        "                                            max_depth = ml_GridSearchCV.best_params_['max_depth'], \n",
        "                                            min_samples_split = ml_GridSearchCV.best_params_['min_samples_split'], \n",
        "                                            min_samples_leaf = ml_GridSearchCV.best_params_['min_samples_leaf'], \n",
        "                                            max_features = ml_GridSearchCV.best_params_['max_features'])\n",
        "        \n",
        "    elif ml_Opt == 'ml_XGB2':\n",
        "        print(f'\\nXGBoostingClassifier *********************************************************************************************************')\n",
        "        ml_Opt = XGBoostingClassifier(learning_rate= ml_GridSearchCV.best_params_['learning_rate'], \n",
        "                                      max_depth= ml_GridSearchCV.best_params_['max_depth'], \n",
        "                                      colsample_bytree= ml_GridSearchCV.best_params_['colsample_bytree'], \n",
        "                                      subsample= ml_GridSearchCV.best_params_['subsample'], \n",
        "                                      gamma= ml_GridSearchCV.best_params_['gamma'], \n",
        "                                      min_child_weight= ml_GridSearchCV.best_params_['min_child_weight'])\n",
        "        \n",
        "    # Treina novamente usando os parametros otimizados...\n",
        "    ml_Opt.fit(X_treinamento, y_treinamento)\n",
        "\n",
        "    # Cross-Validation com 10 folds\n",
        "    print(f'\\n********* CROSS-VALIDATION ***********')\n",
        "    a_scores_CV = funcao_cross_val_score(ml_Opt, X_treinamento, y_treinamento, i_CV)\n",
        "\n",
        "    # Faz predições com os parametros otimizados...\n",
        "    y_pred = ml_Opt.predict(X_teste)\n",
        "  \n",
        "    # Importância das COLUNAS\n",
        "    print(f'\\n********* IMPORTÂNCIA DAS COLUNAS ***********')\n",
        "    df_importancia_variaveis = pd.DataFrame(zip(l_colunas, ml_Opt.feature_importances_), columns= ['coluna', 'importancia'])\n",
        "    df_importancia_variaveis = df_importancia_variaveis.sort_values(by= ['importancia'], ascending=False)\n",
        "    print(df_importancia_variaveis)\n",
        "\n",
        "    # Matriz de Confusão\n",
        "    print(f'\\n********* CONFUSION MATRIX - PARAMETER TUNNING ***********')\n",
        "    cf_matrix = confusion_matrix(y_teste, y_pred)\n",
        "    cf_labels = ['True_Negative', 'False_Positive', 'False_Negative', 'True_Positive']\n",
        "    cf_categories = ['Zero', 'One']\n",
        "    mostra_confusion_matrix(cf_matrix, group_names = cf_labels, categories = cf_categories)\n",
        "\n",
        "    return ml_Opt, ml_GridSearchCV.best_params_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adif316XBrff"
      },
      "source": [
        "# Função para Decision Tree:\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier # Library para Decision Tree (Classificação)\n",
        "\n",
        "ml_DT = DecisionTreeClassifier(criterion = 'gini', \n",
        "                               splitter = 'best', \n",
        "                               max_depth = None, \n",
        "                               min_samples_split = 2, \n",
        "                               min_samples_leaf = 1, \n",
        "                               min_weight_fraction_leaf = 0.0, \n",
        "                               max_features = None, \n",
        "                               random_state = i_Seed, \n",
        "                               max_leaf_nodes = None, \n",
        "                               min_impurity_decrease = 0.0, \n",
        "                               min_impurity_split = None, \n",
        "                               class_weight = None, \n",
        "                               presort = False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AuycftiX1Yv"
      },
      "source": [
        "## MODELO 3: RANDOM FOREST, CV e GridSearchRF (X_treinamento, X_teste)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lp_dDyAjgNn",
        "outputId": "dd5ba280-5f0e-4ef3-a312-1fa590ed00c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# RandomForestClassifier\n",
        "\n",
        "# Instancia...\n",
        "ml_RF= RandomForestClassifier(n_estimators=200, min_samples_split= 2, max_features=\"auto\",random_state = i_Seed)\n",
        "\n",
        "# Treina...\n",
        "ml_RF.fit(X_treinamento, y_treinamento)\n",
        "\n",
        "a_scores_CV = funcao_cross_val_score(ml_RF, X_treinamento, y_treinamento, i_CV)\n",
        "\n",
        "# Faz predições...\n",
        "y_pred = ml_RF.predict(X_teste)\n",
        "\n",
        "# Dicionário de parâmetros para o parameter tunning.\n",
        "d_hiperparametros_RF= {'bootstrap': [True, False],\n",
        "                  'max_depth': [10,None],#[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        "                  'max_features': ['auto', 'sqrt'],\n",
        "                  'min_samples_leaf': [1, 2, 4],\n",
        "                  'min_samples_split': [2,5], #[2, 5, 10],\n",
        "                  'n_estimators': [200,400]}#[200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
        "\n",
        "ml_RF2, best_params = GridSearchOptimizer(ml_RF, 'ml_RF2', d_hiperparametros_RF, X_treinamento, y_treinamento, X_teste, y_teste, i_CV, l_colunas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Média das Acurácias calculadas pelo CV....: 79.86\n",
            "std médio das Acurácias calculadas pelo CV: 1.97\n",
            "Fitting 10 folds for each of 96 candidates, totalling 960 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    3.0s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    4.7s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    9.5s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   15.9s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   27.0s\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   31.8s\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   45.9s\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   53.8s\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  3.1min\n",
            "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  3.9min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed:  4.7min\n",
            "[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed:  5.2min\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:  5.7min\n",
            "[Parallel(n_jobs=-1)]: Done 309 tasks      | elapsed:  6.2min\n",
            "[Parallel(n_jobs=-1)]: Done 334 tasks      | elapsed:  6.8min\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed:  7.4min\n",
            "[Parallel(n_jobs=-1)]: Done 388 tasks      | elapsed:  7.9min\n",
            "[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed:  8.7min\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPYjrNwhHdyH",
        "outputId": "5e4d4aef-a5e7-4d6e-bee1-b75eee2aa487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJX00Fx1HpFr"
      },
      "source": [
        "#y_pred = ml_RF.predict(X_teste)\n",
        "\n",
        "#y_teste_submit = ml_RF.predict(df_teste_submit)\n",
        "df_submit = pd.DataFrame(zip(df_test['id'],y_pred), columns = ['id','Churn'])\n",
        "df_submit.to_csv('submit.csv',index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRlxN-bQNkqv",
        "outputId": "c976a7d1-bd00-406c-e419-414fb3b41caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df_submit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Churn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5027</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1733</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5384</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6554</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>364</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1404</th>\n",
              "      <td>4897</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1405</th>\n",
              "      <td>6940</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1406</th>\n",
              "      <td>804</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1407</th>\n",
              "      <td>1143</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1408</th>\n",
              "      <td>5773</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1409 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  Churn\n",
              "0     5027      0\n",
              "1     1733      0\n",
              "2     5384      0\n",
              "3     6554      0\n",
              "4      364      0\n",
              "...    ...    ...\n",
              "1404  4897      0\n",
              "1405  6940      0\n",
              "1406   804      0\n",
              "1407  1143      1\n",
              "1408  5773      0\n",
              "\n",
              "[1409 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzxVvpflNqhq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}