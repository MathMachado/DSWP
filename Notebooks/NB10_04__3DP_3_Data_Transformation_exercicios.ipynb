{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NB10_04__3DP_3_Data_Transformation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Celso-Omoto/DSWP/blob/master/Notebooks/NB10_04__3DP_3_Data_Transformation_exercicios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CgDLvphxfcX"
      },
      "source": [
        "<center><h1><b><i>3DP_3 - DATA TRANSFORMATION</i></b></h1></center>\n",
        "\n",
        "* **Objetivo**: Preparar os dados para o Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvW689ZBxbxH"
      },
      "source": [
        "# **AGENDA**:\n",
        "\n",
        "> Consulte **Table of contents**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNiuYCCxGe8v"
      },
      "source": [
        "# **Melhorias da sessão**\n",
        "* Desenvolver a sessão sobe WOE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TdSY74U0XS9"
      },
      "source": [
        "___\n",
        "# **Referências**\n",
        "* [Why, How and When to Scale your Features](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e)\n",
        "* [Demonstrating the different strategies of KBinsDiscretizer](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_strategies.html#sphx-glr-auto-examples-preprocessing-plot-discretization-strategies-py);\n",
        "* [Why do we need feature scaling in Machine Learning and how to do it using SciKit Learn?](https://medium.com/@contactsunny/why-do-we-need-feature-scaling-in-machine-learning-and-how-to-do-it-using-scikit-learn-d8314206fe73)\n",
        "* [Importance of Feature Scaling](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py) --> Muito importante por demonstrar os efeitos e a importância de se transformar as colunas numéricas.\n",
        "* [Feature discretization](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_classification.html#sphx-glr-auto-examples-preprocessing-plot-discretization-classification-py) --> Mostra o impacto na acurácia dos modelos com e sem discretização. Ou seja, discretizar faz sentido!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9DGifbWSmW3"
      },
      "source": [
        "___\n",
        "# **Machine Learning com Python (Scikit-Learn)**\n",
        "\n",
        "![Scikit-Learn](https://github.com/MathMachado/Materials/blob/master/scikit-learn-1.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg82Iouo_Qm2"
      },
      "source": [
        "# Porque dimensionar (Scale), padronizar (Standardize) e normalizar (Normalize) importa?\n",
        "* Porque muitos algoritmos de Machine Learning performam melhor ou convergem mais rápido quando os atributos/colunas/variáveis estão na mesma escala e possuem distribuição \"próxima\" da Normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-chlATnKSza"
      },
      "source": [
        "## Carregar as bibliotecas (genéricas) Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQGVQB18-tM_"
      },
      "source": [
        "#!pip install category_encoders\n",
        "#!pip install update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FJxrZckYxk6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import category_encoders as ce # library para aplicação do WOE - Weight Of Evidence para avaliar importância dos atributos\n",
        "\n",
        "# remove warnings to keep notebook clean\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyuWQM2NTMls"
      },
      "source": [
        "pd.options.display.float_format = '{:.2f}'.format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0fuDyI8_UPf"
      },
      "source": [
        "## Carregar os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oRWtarakgMY"
      },
      "source": [
        "### Dataframe gerado aleatoriamente - variáveis com distribuição Normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BXPXo3k0VDI"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "\n",
        "i_N = 10000\n",
        "\n",
        "df_A1 = pd.DataFrame({\n",
        "    'coluna1': np.random.normal(0, 2, i_N), # Observem que a média das colunas são distintas\n",
        "    'coluna2': np.random.normal(50, 3, i_N),\n",
        "    'coluna3': np.random.normal(-5, 5, i_N),\n",
        "    'coluna4': np.random.normal(-10, 10, i_N)\n",
        "})\n",
        "\n",
        "df_A1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ST1JnoRZKm"
      },
      "source": [
        "**Dica**: Podemos usar outras distribuições (se quisermos), como a Exponential (mostrada abaixo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUqjo5QcQH99"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "\n",
        "df_A2 = pd.DataFrame({\n",
        "    'coluna1': np.random.normal(0, 2, i_N),\n",
        "    'coluna2': np.random.normal(50, 3, i_N),\n",
        "    'coluna3': np.random.exponential(1, i_N), # coluna3 tem distribuição Exponential\n",
        "    'coluna4': np.random.normal(-10, 10, i_N)\n",
        "})\n",
        "\n",
        "df_A2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8MZNLbUkp8R"
      },
      "source": [
        "### Dataframe gerado aleatoriamente 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR-fDDujcTup"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "dados, classe = make_classification(n_samples = i_N, n_features = 4, n_informative = 3, n_redundant = 1, n_classes = 3)\n",
        "\n",
        "df_A3 = pd.DataFrame({'coluna1': dados[:,0],\n",
        "                      'coluna2':dados[:,1],\n",
        "                      'coluna3':dados[:,2],\n",
        "                      'coluna4':dados[:,3]}) #, 'coluna5':classe})\n",
        "\n",
        "df_A3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq1cnpwLKvjS"
      },
      "source": [
        "df_A4 = pd.DataFrame({ \n",
        "    'coluna1': np.random.beta(5, 1, i_N) * 25, \n",
        "    'coluna2': np.random.exponential(10, i_N),\n",
        "    'coluna3': np.random.normal(10, 2, i_N),\n",
        "    'coluna4': np.random.normal(10, 10, i_N), \n",
        "})\n",
        "\n",
        "df_A4.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7sXQjvYRfhb"
      },
      "source": [
        "#### Extração de amostras para compararmos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjVHsnnHRkIo"
      },
      "source": [
        "df_A1_test = df_A1.sample(n = 100)\n",
        "df_A2_test = df_A2.sample(n = 100)\n",
        "df_A3_test = df_A3.sample(n = 100)\n",
        "df_A4_test = df_A4.sample(n = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0v0uXFRl-yG"
      },
      "source": [
        "___\n",
        "# **Transformações**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkzTO0fdz93b"
      },
      "source": [
        "## (1) StandardScaler\n",
        "* StandardScaler é a transformação que centraliza os dados através da remoção da média (dos dados) e, na sequência, redimensiona (scale) através da divisão pelo desvio-padrão;\n",
        "* Após a transformação, os dados terão média zero e desvio-padrão 1;\n",
        "* Assume que os dados (as colunas a serem transformadas) são normalmente distribuidos ;\n",
        "* Se os dados não possuem distribuição Normal, então esta não é uma boa transformação a se aplicar.\n",
        "\n",
        "$$z_{i}= \\frac{x_{i}-mean(x)}{std(x)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1UOOWeQ0R_Y"
      },
      "source": [
        "### Exemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1Lzx3xN6wpZ"
      },
      "source": [
        "df_A3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cPq_7Vu2HCS"
      },
      "source": [
        "Histograma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYW9WwBC3hd_"
      },
      "source": [
        "plt.figure(figsize = (12, 8))\n",
        "plt.hist(df_A1['coluna3'], color = 'blue', edgecolor = 'black', bins = int(180/5))\n",
        "\n",
        "# Adiciona títulos e labels\n",
        "plt.title('Histograma da coluna3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ogcQvvT5zK"
      },
      "source": [
        "plt.figure(figsize = (12, 8))\n",
        "plt.hist(df_A2['coluna3'], color = 'blue', edgecolor = 'black', bins = int(180/5))\n",
        "\n",
        "# Adiciona títulos e labels\n",
        "plt.title('Histograma da coluna3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrgxkESc-Uaq"
      },
      "source": [
        "Considere o gráfico a seguir:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7dHTF1W-Xsn"
      },
      "source": [
        "df_A1.plot(kind = 'kde') # KDE (= kernel Density Estimate) ajuda-nos a visualizar a distribuição dos dados, análogo ao histograma."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMS72n14-hDO"
      },
      "source": [
        "Qual a interpretação para o gráfico acima?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izqGNcNILdaX"
      },
      "source": [
        "df_A1.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEkAqlZg-p0v"
      },
      "source": [
        "A seguir, a transformação StandardScaler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4u3T_BX-oc_"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voFQ4odSzzPZ"
      },
      "source": [
        "O ideal é termos um array com as preditoras, da seguinte forma:\n",
        "X = [coluna1, coluna2, ..., colunaN]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPa4-SCt-ynX"
      },
      "source": [
        "np.set_printoptions(precision = 3)\n",
        "\n",
        "A1_scale = StandardScaler().fit_transform(df_A1) # Combinação dos métodos fit() + transform()\n",
        "\n",
        "A1_scale_fit = StandardScaler().fit(df_A1) # Aplica o fit() separadamente\n",
        "A1_scale_transform = A1_scale_fit.transform(df_A1) # Aplica o transform() separadamente.\n",
        "A1_scale_fit_transform = StandardScaler().fit(df_A1).transform(df_A1) # Aplica fit().transform() encadeado\n",
        "\n",
        "A2_scale = StandardScaler().fit_transform(df_A2)\n",
        "\n",
        "A3_scale = StandardScaler().fit_transform(df_A3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGR9-bG0q-SI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioZ_IN3Z6d39"
      },
      "source": [
        "Observe abaixo que A1_scale = A1_scale_transform = A1_scale_fit_transform --> São arrays multidimensionais (do tipo NumPy)!\n",
        "\n",
        "**é importante salvar as medidas de StandardScaler e outros para não ser necessário reprocessar os valores para todo processamento.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4xQR4cu5D1J"
      },
      "source": [
        "A1_scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6GtN2KF4E_A"
      },
      "source": [
        "A1_scale_transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q2bvSqb6T4g"
      },
      "source": [
        "A1_scale_fit_transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIhaErnA46Fi"
      },
      "source": [
        "Transformando em dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAhRvPze44JW"
      },
      "source": [
        "df_A1_scale = pd.DataFrame(A1_scale, columns = ['coluna1', 'coluna2', 'coluna3', 'coluna4'])\n",
        "df_A2_scale = pd.DataFrame(A2_scale, columns = ['coluna1', 'coluna2', 'coluna3', 'coluna4'])\n",
        "df_A3_scale = pd.DataFrame(A3_scale, columns = ['coluna1', 'coluna2', 'coluna3', 'coluna4'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmQp8wDO_E88"
      },
      "source": [
        "Agora compare esse novo gráfico abaixo --> Vemos que os dados transformados tem distribuição Normal(0, 1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csfqRhDH2zUb"
      },
      "source": [
        "df_A1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-krh1pDg22RF"
      },
      "source": [
        "df_A1_scale.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2fTPWsm_Hq3"
      },
      "source": [
        "df_A1_scale.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oN-829l3277"
      },
      "source": [
        "df_A2.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqh8L5BeUHT-"
      },
      "source": [
        "df_A2_scale.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvz6O1zk4XNE"
      },
      "source": [
        "df_A3.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffU-fQxCUSmm"
      },
      "source": [
        "df_A3_scale.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y24MOLL83w9j"
      },
      "source": [
        "### Exercício: Calcular a média e o desvio-padrão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Aa25gVlSdOi"
      },
      "source": [
        "df_A1.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXZUiZImSmOE"
      },
      "source": [
        "df_A1_scale.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIUQw5dpRwvA"
      },
      "source": [
        "#### Correlação das colunas\n",
        "* Observe que as correlações entre as variáveis não se alteram com as transformações."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj1UerjORq9q"
      },
      "source": [
        "df_A1.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp6vPK0aR_p0"
      },
      "source": [
        "df_A1_scale.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fuURrao_M0c"
      },
      "source": [
        "Qual a conclusão?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0A9U7rs_RAT"
      },
      "source": [
        "## (2) MinMaxScaler\n",
        "* **Transformação muito popular e utilizada**.\n",
        "* Transforma os dados para o intervalo (0, 1);\n",
        "* Se StandardScaler não é aplicável, então essa transformação funciona bem.\n",
        "* Sensível aos outliers. Portanto, o ideal é que os outliers sejam tratados previamente.\n",
        "* Uma transformação similar à MinMaxScaler() é MaxAbsScaler() que redimensiona os dados no intervalo [-1, 1], centralizado em 0(zero)\n",
        "\n",
        "$$z_{i}= \\frac{x_{i}-min(x)}{max(x)-min(x)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0HbeuP-AU_p"
      },
      "source": [
        "### Exemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgeLckzxAWaC"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_W9bTO2AbEg"
      },
      "source": [
        "df_A1.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJRFbUpBAg5J"
      },
      "source": [
        "A1_MinMaxScaler = MinMaxScaler().fit_transform(df_A1)\n",
        "df_A1_MinMaxScaler = pd.DataFrame(A1_MinMaxScaler,columns = ['coluna1', 'coluna2', 'coluna3', 'coluna4'])\n",
        "\n",
        "# Gráfico\n",
        "df_A1_MinMaxScaler.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g8GA4LTA40U"
      },
      "source": [
        "Qual a conclusão?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z6D3vfnB9Nm"
      },
      "source": [
        "## (3) RobustScaler\n",
        "* Transformação ideal para dados com outliers.\n",
        "\n",
        "$$z_{i}= \\frac{x_{i}-Q_{1}(x)}{Q_{3}(x)-Q_{1}(x)}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3oyuxLeCW1D"
      },
      "source": [
        "df_A1.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeDF7-w_CcBy"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLoqSKijCf2v"
      },
      "source": [
        "A1_RobustScaler = RobustScaler().fit_transform(df_A1)\n",
        "df_A1_RobustScaler = pd.DataFrame(A1_RobustScaler, columns = ['coluna1', 'coluna2', 'coluna3', 'coluna4'])\n",
        "\n",
        "# Gráfico\n",
        "df_A1_RobustScaler.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YVMgt-WEFif"
      },
      "source": [
        "## Encoding Variáveis Categóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHYvLc8T_jxQ"
      },
      "source": [
        "### Encoding Variáveis Ordinais\n",
        "* Exemplo: Variáveis com valores ordinais: baixo, médio ou alto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1BgGiGdSTcG"
      },
      "source": [
        "#### Gera um dataframe como exemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdVahfJAEkuO"
      },
      "source": [
        "# Aqui vou usar a função randint - Retorna números inteiros aleatórios incluindo o número inferior e excluindo o superior.\n",
        "\n",
        "l_idade= [np.random.randint(20, 40), np.random.randint(20, 40), np.random.randint(20, 40), np.random.randint(20, 40), np.random.randint(20, 40),\n",
        "         np.random.randint(20, 40), np.random.randint(20, 40), np.random.randint(20, 40), np.random.randint(20, 40), np.random.randint(20, 40)]\n",
        "\n",
        "l_salario = ['baixo', 'medio', 'alto']\n",
        "l_salario2 = np.random.choice(l_salario, 10, p = [0.6, 0.3, 0.1])\n",
        "\n",
        "df_A4 = pd.DataFrame({\n",
        "    'idade': l_idade,\n",
        "    'salario': l_salario2})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_15P2eUHSBY"
      },
      "source": [
        "df_A4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1g9pEuyHe2q"
      },
      "source": [
        "Neste exemplo, vamos redefinir a variável categórical ordinal 'Salario' da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkwFuEa8HnMV"
      },
      "source": [
        "df_A4['salario_cat'] = df_A4['salario'].map({'baixo': 1, 'medio': 2, 'alto': 3})\n",
        "df_A4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlaIFiWIIPAl"
      },
      "source": [
        "### Encoding Variáveis Nominais\n",
        "* Exemplo: Variáveis com valores nominais: Sexo (Feminino, Masculino).\n",
        "\n",
        "* Use One-Hot Encoding ou pd.get.dummies()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffNoJQbgJRoY"
      },
      "source": [
        "Vamos utilizar o dataframe criado no passo anterior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMCoUWZOI7c0"
      },
      "source": [
        "df_A4['salario'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdIEyBkaJeN8"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MwK4cUEKeK4"
      },
      "source": [
        "#### Aplicar LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X6VXDsHJiII"
      },
      "source": [
        "le = LabelEncoder()\n",
        "df_A4['salario_le'] = le.fit_transform(df_A4['salario'])\n",
        "df_A4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY80x59J8Ham"
      },
      "source": [
        "df_A4['salario'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgv2Zz07Kqfj"
      },
      "source": [
        "#### Aplicar pd.get.dummies()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSZRIEs6K5sP"
      },
      "source": [
        "dummies = pd.get_dummies(df_A4['salario'])\n",
        "df_A4 = pd.concat([df_A4, dummies], axis = 1)\n",
        "df_A4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY8GZ-HlNOgm"
      },
      "source": [
        "*texto em itálico*# **Wrap Up**\n",
        "\n",
        "\n",
        "* Use MinMaxScaler como transformação default, pois esta transformação não distorce os dados;\n",
        "* Use RobustScaler se seus dados/coluna/variável possui outliers e gostaríamos de reduzir o efeito/impacto destes outliers. Entretanto, o melhor tratamento é estudar os outliers cuidadosamente e tratá-los adequadamente;\n",
        "* Use StandardScaler se seus dados/colunas/variáveis possuem distribuição Normal (ou pelo menos se aproxima bem da distribuição Normal)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwh0alhdgrE3"
      },
      "source": [
        "___\n",
        "# **Exercícios**\n",
        "> Para cada um dos dataframes a seguir, aplique os seguintes steps:\n",
        "\n",
        "* Padronizar o nome das colunas\n",
        "    * Eliminar espaços entre os nomes das colunas;\n",
        "    * Eliminar caracteres especiais dos nomes das colunas;\n",
        "    * Renomear as colunas com lower() (ou upper());\n",
        "* Aplicar a trasformação StandardScaler e MinMaxScaler em cada uma das colunas do dataframe;\n",
        "* DataViz - Mostrar a distribuição das colunas para compararmos os resultados antes e depois das transformações.\n",
        "* As correlações das colunas mudam com as transformações?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSTKrd992LtI"
      },
      "source": [
        "## Exercício 1 - Iris --> **Resolvido**\n",
        "* [Aqui](https://en.wikipedia.org/wiki/Iris_flower_data_set) você obterá mais informações sobre o dataframe iris. Confira."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mThqvGGr2Vuk"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X= iris['data']\n",
        "y= iris['target']\n",
        "\n",
        "df_iris = pd.DataFrame(np.c_[X, y], columns= np.append(iris['feature_names'], ['target']))\n",
        "df_iris['target2'] = df_iris['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "df_iris.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU5FaJhdYblP"
      },
      "source": [
        "df_iris.columns = [c.replace(' ', '_') for c in df_iris.columns]\n",
        "df_iris.columns = [c.replace('_(cm)', '') for c in df_iris.columns]\n",
        "df_iris.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGmZjd_Y79lY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9DPAakJZQHH"
      },
      "source": [
        "df_iris.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYYmVq68Y8bB"
      },
      "source": [
        "# Aplica a transformação:\n",
        "df_iris_MinMaxScaler = MinMaxScaler().fit_transform(df_iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
        "\n",
        "# Transformando em Dataframe:\n",
        "df_iris_MinMaxScaler = pd.DataFrame(df_iris_MinMaxScaler, columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
        "\n",
        "# Gráfico\n",
        "df_iris_MinMaxScaler.plot(kind = 'kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwPH8-258JrF"
      },
      "source": [
        "aplicar as outras transformações e comparar os gráficos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caFkC6oCmUKK"
      },
      "source": [
        "## Exercício 2 - Breast Cancer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhOM-Z9zmf-f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X= cancer['data']\n",
        "y= cancer['target']\n",
        "\n",
        "df_A1_cancer = pd.DataFrame(np.c_[X, y], columns= np.append(cancer['feature_names'], ['target']))\n",
        "df_A1_cancer['target'] = df_A1_cancer['target'].map({0: 'malign', 1: 'benign'})\n",
        "df_A1_cancer.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qruqUDqnvMc"
      },
      "source": [
        "## Exercício 3 - Boston Housing Price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trxK8YXNnsam"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "\n",
        "boston = load_boston()\n",
        "X= boston['data']\n",
        "y= boston['target']\n",
        "\n",
        "df_A1_boston = pd.DataFrame(np.c_[X, y], columns= np.append(boston['feature_names'], ['target']))\n",
        "df_A1_boston.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzu0Dz33c8ds"
      },
      "source": [
        "## Exercícios 4 - Diabetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6ahBZmqc_-1"
      },
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "diabetes = load_diabetes()\n",
        "X= diabetes['data']\n",
        "y= diabetes['target']\n",
        "\n",
        "df_A1_diabetes = pd.DataFrame(np.c_[X, y], columns= np.append(diabetes['feature_names'], ['target']))\n",
        "df_A1_diabetes.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyunIr6oaWEl"
      },
      "source": [
        "## Exercícios 6 - 120 years of Olympic history: athletes and results\n",
        "* [120 years of Olympic history: athletes and results](https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results)\n",
        "    * Trate adequadamente as variáveis 'sex', 'season', 'team', 'city', 'sport' e 'medal';\n",
        "    * Aplique as transformações que acabamos de estudar nos campos/colunas numéricas 'height' e 'weight'. Cuidado com os Missing Values contidos nas variáveis!\n",
        "    * Verifique/avalie o impacto dos outliers nestas colunas.\n",
        "    * Neste caso, qual transformação é mais adequado diante dos outliers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5fDp1Ib_Dg8"
      },
      "source": [
        "# WOE - Weight Of Evidence\n",
        "* As vantagens da transformação WOE são\n",
        "    * Lida bem com NaN's;\n",
        "    * Lida bem com outliers;\n",
        "    * A transformação é baseada no valor logarítmico das distribuições.\n",
        "    * Usando a técnica de binning apropriada, pode estabelecer uma relação monotônica (aumentar ou diminuir) entre a variável dependente e independente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXEsP96A9TSd"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "#import category_encoders as ce # library para aplicação do WOE - Weight Of Evidence para avaliar importância dos atributos\n",
        "\n",
        "# remove warnings to keep notebook clean\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGdOGDZAHu-V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2W9PXAc-vHY"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g45JU2LXHwkz"
      },
      "source": [
        "url = '/content/drive/My Drive/athlete_events.csv'\n",
        "import pandas as pd\n",
        "df_olympics = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLxrnkJw_f7m"
      },
      "source": [
        "pd.read_csv('/content/drive/My Drive/file/d/athlete_events.zip', compression='zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcPQhh04E2du"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}