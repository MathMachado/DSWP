{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bias_vs_Variance.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNAj9QQSB6FZ/LRT4t4ubec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathMachado/DSWP/blob/master/Notebooks/Bias_vs_Variance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIMVJo4jtuH8"
      },
      "source": [
        "# Bias x Variance\n",
        "* Ao construirmos modelo de Machine Learning, nosso objetivo é construir um modelo performático (boa acurácia, por exemplo) para estimar a variável-target e que possua baixo viés (Bias) e baixa variância (Variance).\n",
        "* É um trade-off: se você reduzir Bias, vai aumentar a variância; se você reduzir a variância, vai aumentar o bias.\n",
        "* Bias (viés) e Variance (variância) são os principais parâmetros a serem ajustados durante o treinamento de um modelo de aprendizado de máquina.\n",
        "* Quando falamos sobre predição dos modelos de Machine Learning, os erros podem ser decompostos em dois componentes:\n",
        "    * erro produzido/devido ao bias (viés);\n",
        "    * erro produzido/devido à variância."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j78s9TaAuyVi"
      },
      "source": [
        "## Erro produzido pelo Bias (underfitting)\n",
        "* É a distância entre as previsões do modelo ($\\hat{y}$) e os valores reais ($y$). Esse tipo de erro acontece quando o algoritmo simplifica muito o modelo e não aprende os padrões pois **não levou em consideração todas as features**.\n",
        "* Bias é a habilidade do modelo em capturar os padrões presentes nos dados de treinamento.\n",
        "\n",
        "* **Como detectar Bias do seu modelo**?\n",
        "    * Para detectar o Bias, calcule o erro do modelo no dataframe de treinamento. Quando o modelo possui um erro (dataframe de treinamento) alto, é sinal de alto Bias.\n",
        "\n",
        "* **Como reduzir Bias**: adicionar mais features, mas encontre um equilíbrio entre Bias e Variância."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR2cVbKK37W8"
      },
      "source": [
        "![Bias](https://github.com/MathMachado/Materials/blob/master/Bias.PNG?raw=true)\n",
        "\n",
        "Fonte: [Bias-Variance tradeoff in Machine Learning models: A practical example](https://towardsdatascience.com/bias-variance-tradeoff-in-machine-learning-models-a-practical-example-cf02fb95b15d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBwxonOpvusz"
      },
      "source": [
        "## Erro produzido pela Variância (overfitting)\n",
        "* Erro ocasionado quando o modelo se especializa nos dados de treinamento a ponto de memorizá-los ao invés de aprender com eles. \n",
        "* Este tipo de modelo com alta variância não é flexível para generalizar os dados do dataframe de teste/validação.\n",
        "* Mede o quão longe as estimativas/previsões estão da média de todas as previsões no dataframe de teste.\n",
        "* **Como detectar Variance do seu modelo**?\n",
        "    * Calcule os erros do modelo no dataframe de teste. Quando o modelo tem alto erro (no dataframe de teste), é um sinal de alta variância.\n",
        "* **Como reduzir Variance**?\n",
        "    * Uma maneira de reduzir a variação é construir o modelo com mais dados de treinamento. O modelo terá mais exemplos para aprender e melhorar sua capacidade de generalizar suas previsões.\n",
        "    * Se não há muitos dados disponíveis para construir o modelo:\n",
        "        * Alternativa 1: experimente construir modelos usando [Bootstrap aggregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating), geralmente chamada de **bagging**.\n",
        "        * Alternativa 2: Experimente Feature Selection para selecionar os melhores features ou Principal Components Analysis, que são técnicas para reduzir a dimensionalidade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIbFL8sZ4JN7"
      },
      "source": [
        "![Variance](https://github.com/MathMachado/Materials/blob/master/Variance.PNG?raw=true)\n",
        "\n",
        "Fonte: [Bias-Variance tradeoff in Machine Learning models: A practical example](https://towardsdatascience.com/bias-variance-tradeoff-in-machine-learning-models-a-practical-example-cf02fb95b15d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZbsCGVNzfx5"
      },
      "source": [
        "O exemplo abaixo foi extraído de https://medium.com/towards-artificial-intelligence/bias-variance-tradeoff-illustration-using-pylab-202943bf4c78."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX7xt4rwttX5"
      },
      "source": [
        "import pylab\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWy0bx2nxeT0"
      },
      "source": [
        "#create the dataset\n",
        "t = np.linspace(0, 1, 11)\n",
        "h = np.array([1.67203, 1.79792, 2.37791,2.66408,2.11245, 2.43969,1.88843, 1.59447,1.79634,1.07810,0.21066])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3xyZjJoxmfj"
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "# fig 1\n",
        "plt.subplot(131)\n",
        "#perform linear fit using pylab\n",
        "degree = 1\n",
        "model=pylab.polyfit(t,h,degree)\n",
        "est_h=pylab.polyval(model,t)\n",
        "#calculating R-squared value\n",
        "R2 = 1 - ((h-est_h)**2).sum()/((h-h.mean())**2).sum()\n",
        "#plot of observed and modeled data\n",
        "pylab.scatter(t,h, c='b', label='observed')\n",
        "pylab.plot(t,est_h, c='r', label='predicted:' + ' R2' '='+ ' ' + str(round(R2,4)))\n",
        "pylab.xlabel('t(s)')\n",
        "pylab.ylabel('h(m)')\n",
        "pylab.title('linear model is not good (underfit)')\n",
        "pylab.legend()\n",
        "\n",
        "# fig 2\n",
        "plt.subplot(132)\n",
        "#perform quadratic fit using pylab\n",
        "degree = 2\n",
        "model=pylab.polyfit(t,h,degree)\n",
        "est_h=pylab.polyval(model,t)\n",
        "#calculating R-squared value\n",
        "R2 = 1 - ((h-est_h)**2).sum()/((h-h.mean())**2).sum()\n",
        "#plot of observed and modeled data\n",
        "pylab.scatter(t,h, c='b', label='observed')\n",
        "pylab.plot(t,est_h, c='r', label='predicted:' + ' R2' '='+ ' ' + str(round(R2,4)))\n",
        "pylab.xlabel('t(s)')\n",
        "pylab.ylabel('h(m)')\n",
        "pylab.title('quadratic model is what we need')\n",
        "pylab.legend()\n",
        "# fig 3\n",
        "plt.subplot(133)\n",
        "#perform higher-degree fit using pylab\n",
        "degree = 10\n",
        "model=pylab.polyfit(t,h,degree)\n",
        "est_h=pylab.polyval(model,t)\n",
        "#calculating R-squared value\n",
        "R2 = 1 - ((h-est_h)**2).sum()/((h-h.mean())**2).sum()\n",
        "#plot of observed and modeled data\n",
        "pylab.scatter(t,h, c='b', label='observed')\n",
        "pylab.plot(t,est_h, c='r', label='predicted:' + ' R2' '='+ ' ' + str(round(R2,4)))\n",
        "pylab.xlabel('t(s)')\n",
        "pylab.ylabel('h(m)')\n",
        "pylab.title('degree=10 captures random error (overfit)')\n",
        "pylab.legend()\n",
        "pylab.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyANGq1yxv-H"
      },
      "source": [
        "### Conclusão da figura 1 (Regressão Linear)\n",
        "* $R^{2} = 0.3953$ --> Baixo\n",
        "* Se a regressão for um bom modelo, deveríamos ter $R^{2}$ próximo de 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54nDGfRuyPAC"
      },
      "source": [
        "### Conclusão da figura 2 (Modelo Quadrático)\n",
        "* $R^{2} = 0.8895$ --> Alto\n",
        "* O modelo quadrático é uma opção/alternativa melhor que a regressão linear, pois apresenta melhor $R^{2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HlvUirEyviz"
      },
      "source": [
        "### Conclusão da figura 3 (Modelo Polinomial)\n",
        "* $R^{2} = 1$ --> Alto\n",
        "* Mas será que esse modelo será capaz de capturar os próximos dados?\n",
        "* Geralmente, um modelo mais simples, com poucos parâmetros é mais fácil de interpretar e capturar a complexidade presente nos dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4j365PV0sh_"
      },
      "source": [
        "## A necessidade das amostras de treinamento e teste\n",
        "* Treinamento: ensina ao modelo como prever a variável-target.\n",
        "    * Se o modelo for expert em prever o target somente no dataframe de treinamento, então não será útil para prever outros dados.\n",
        "* Teste: usado para testar a qualidade (acurácia) do modelo. Ou seja, o modelo é bom em prever além dos dados que foram usados no processo de treinamento/aprendizagem.\n",
        "    * Com o dataframe de teste você concluirá se o modelo generaliza as predições além dos dados de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFjejXpK7I7T"
      },
      "source": [
        "## Exemplo 2\n",
        "* Exemplo extraído de [Bias-Variance tradeoff in Machine Learning models: A practical example](https://towardsdatascience.com/bias-variance-tradeoff-in-machine-learning-models-a-practical-example-cf02fb95b15d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgbTg2om8CAC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRJ11z6szEV9"
      },
      "source": [
        "dataset_size = 5000\n",
        "# Generate a random dataset and that follows a quadratic distribution\n",
        "random_x = np.random.randn(dataset_size)\n",
        "random_y = ((-5 * random_x ** 4) + (-3 * random_x ** 3) + 10 * random_x ** 2 + 2.5 ** random_x + 0.5).reshape(dataset_size, 1)\n",
        "# Hold out 20% of the dataset for training\n",
        "test_size = int(np.round(dataset_size * 0.2, 0))\n",
        "# Split dataset into training and testing sets\n",
        "x_train = random_x[:-test_size]\n",
        "y_train = random_y[:-test_size]\n",
        "x_test = random_x[-test_size:]\n",
        "y_test = random_y[-test_size:]\n",
        "# Plot the training set data\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "# removing to and right border\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "# adding major gridlines\n",
        "ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
        "ax.scatter(x_train, y_train, color = '#021E73')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1enHOeg77QiR"
      },
      "source": [
        "# Fit model\n",
        "# A first degree polynomial is the same as a simple regression line\n",
        "linear_regression_model = np.polyfit(x_train, y_train, deg=1)\n",
        "# Predicting values for the test set\n",
        "linear_model_predictions = np.polyval(linear_regression_model, x_test)\n",
        "# Plot linear regression line\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "# removing to and right border\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "# adding major gridlines\n",
        "ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
        "ax.scatter(random_x, random_y, color='#021E73')\n",
        "plt.plot(x_test, linear_model_predictions, color='#F2B950', linewidth=3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCYic5e47WlB"
      },
      "source": [
        "# A few auxiliary methods\n",
        "def get_bias(predicted_values, true_values):\n",
        "    return np.round(np.mean((predicted_values - true_values) ** 2), 0)\n",
        "\n",
        "def get_variance(values):\n",
        "    return np.round(np.var(values), 0)\n",
        "\n",
        "def get_metrics(target_train, target_test, model_train_predictions, model_test_predictions):\n",
        "    training_mse = mean_squared_error(target_train, model_train_predictions)\n",
        "    test_mse = mean_squared_error(target_test, model_test_predictions)\n",
        "    bias = get_bias(model_test_predictions, target_test)\n",
        "    variance = get_variance(model_test_predictions)\n",
        "    return [training_mse, test_mse, bias, variance]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BkObqiU8lWY"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28XT-2lU8dZD"
      },
      "source": [
        "# Fit simple linear regression model\n",
        "# A first degree polynomial is the same as a simple regression line\n",
        "linear_regression_model = np.polyfit(x_train, y_train, deg=1)\n",
        "# Predicting values for the test set\n",
        "linear_model_predictions = np.polyval(linear_regression_model, x_test)\n",
        "# Predicting values for the training set\n",
        "training_linear_model_predictions = np.polyval(linear_regression_model, x_train)\n",
        "# Calculate for simple linear model\n",
        "# 1. Training set MSE\n",
        "# 2. Test set MSE\n",
        "# 3. Bias\n",
        "# 4. Variance\n",
        "linear_training_mse, linear_test_mse, linear_bias, linear_variance = get_metrics(y_train, y_test, training_linear_model_predictions, linear_model_predictions)\n",
        "print('Simple linear model')\n",
        "print('Training MSE %0.f' % linear_training_mse)\n",
        "print('Test MSE %0.f' % linear_test_mse)\n",
        "print('Bias %0.f' % linear_bias)\n",
        "print('Variance %0.f' % linear_variance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8aZlReu7cTN"
      },
      "source": [
        "#############################\n",
        "# Fit 2nd degree polynomial #\n",
        "#############################\n",
        "# Fit model\n",
        "polynomial_2nd_model = np.polyfit(x_train, y_train, deg=2)\n",
        "# Used to plot the predictions of the polynomial model and inspect coefficients\n",
        "p_2nd = np.poly1d(polynomial_2nd_model.reshape(1, 3)[0])\n",
        "print('Coefficients %s\\n' % p_2nd)\n",
        "# Predicting values for the test set\n",
        "polynomial_2nd_predictions = np.polyval(polynomial_2nd_model, x_test)\n",
        "# Predicting values for the training set\n",
        "training_polynomial_2nd_predictions = np.polyval(polynomial_2nd_model, x_train)\n",
        "# Calculate for 2nd degree polynomial model\n",
        "# 1. Training set MSE\n",
        "# 2. Test set MSE\n",
        "# 3. Bias\n",
        "# 4. Variance\n",
        "polynomial_2nd_training_mse, polynomial_2nd_test_mse, polynomial_2nd_bias, polynomial_2nd_variance = get_metrics(y_train, y_test, training_polynomial_2nd_predictions, polynomial_2nd_predictions)\n",
        "print('2nd degree polynomial')\n",
        "print('Training MSE %0.f' % polynomial_2nd_training_mse)\n",
        "print('Test MSE %0.f' % polynomial_2nd_test_mse)\n",
        "print('Bias %0.f' % polynomial_2nd_bias)\n",
        "print('Variance %0.f' % polynomial_2nd_variance)\n",
        "# Plot 2nd degree polynomial\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "# removing to and right border\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "# Adding major gridlines\n",
        "ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
        "x_linspace = np.linspace(min(random_x), max(random_x), num=len(polynomial_2nd_predictions))\n",
        "plt.scatter(random_x, random_y, color='#021E73')\n",
        "plt.plot(x_linspace, p_2nd(x_linspace), '-', color='#F2B950', linewidth=3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsEhfz-z7fZu"
      },
      "source": [
        "#############################\n",
        "# Fit 3rd degree polynomial #\n",
        "#############################\n",
        "print('3rd degree polynomial')\n",
        "# Fit model\n",
        "polynomial_3rd_model = np.polyfit(x_train, y_train, deg=3)\n",
        "# Used to plot the predictions of the polynomial model and inspect coefficients\n",
        "p_3rd = np.poly1d(polynomial_3rd_model.reshape(1, 4)[0])\n",
        "print('Coefficients %s' % p_3rd)\n",
        "# Predict values for the test set\n",
        "polynomial_3rd_predictions = np.polyval(polynomial_3rd_model, x_test)\n",
        "# Predict values for the training set\n",
        "training_polynomial_3rd_predictions = np.polyval(polynomial_3rd_model, x_train)\n",
        "# Calculate for 3rd degree polynomial model\n",
        "# 1. Training set MSE\n",
        "# 2. Test set MSE\n",
        "# 3. Bias\n",
        "# 4. Variance\n",
        "polynomial_3rd_training_mse, polynomial_3rd_test_mse, polynomial_3rd_bias, polynomial_3rd_variance = get_metrics(y_train, y_test, training_polynomial_3rd_predictions, polynomial_3rd_predictions)\n",
        "print('\\nTraining MSE %0.f' % polynomial_3rd_training_mse)\n",
        "print('Test MSE %0.f' % polynomial_3rd_test_mse)\n",
        "print('Bias %0.f' % polynomial_3rd_bias)\n",
        "print('Variance %0.f' % polynomial_3rd_variance)\n",
        "\n",
        "# Plot 3rd degree polynomial\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "# removing to and right border\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "# Adding major gridlines\n",
        "ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
        "x_linspace = np.linspace(min(random_x), max(random_x), num=len(polynomial_3rd_predictions))\n",
        "plt.scatter(random_x, random_y, color='#021E73')\n",
        "plt.plot(x_linspace, p_3rd(x_linspace), '-', color='#F2B950', linewidth=3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I2bYGgV7k-1"
      },
      "source": [
        "### Conclusão\n",
        "* Podemos ver o trade-off entre Bias e Variance. Conforme aumentamos a complexidade de um modelo, o viés diminuiu, enquanto a variância aumentou.\n",
        "![Bias_vs_Variance](https://github.com/MathMachado/Materials/blob/master/Bias_vs_Variance.PNG?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIN4YANt9ZK3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}