{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "NB19_Redes_Neurais__V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rnlima8/DSWP/blob/master/Notebooks/NB19_Redes_Neurais_RNL2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShVXyGj9wkgN"
      },
      "source": [
        "<center><h1><b><i>REDES NEURAIS ARTIFICIAIS (COMPREHENSIVE GUIDE) </i></b></h1></center>\n",
        "\n",
        "# Porque Cientistas de Dados desejam aprender e dominar Redes Neurais?\n",
        "\n",
        "* Redes Neurais têm a capacidade de aprender, modelar e resolver problemas não-lineares e complexos apresentados pela vida real.\n",
        "* Você já deve ter ouvido falar em Inteligência Artificial, _self-drive cars_, _Deep Learning_, _Computer Vision_ e _Neural Language Processing_ (NLP). Todos estes assuntos estão estreitamente relacionados às Redes Neurais. Por exemplo, _Deep Learning_ são Redes Neurais com muitas _Hidden Layers_.\n",
        "\n",
        "Este curso aborda os principais tópicos para você dominar Redes Neurais. Além disso, vamos falar das melhores práticas e atacar as principais dúvidas dos alunos em relação às Redes Neurais. Portanto, ao final deste curso você será capaz de:\n",
        "\n",
        "* desenvolver suas próprias Redes Neurais;\n",
        "* aplicar o algoritmo correto para cada tipo de problema;\n",
        "* aplicar as funções de ativação corretamene para cada tipo de problema e camada;\n",
        "* aprender o necessário de Tensorflow/Keras para Redes Neurais;\n",
        "* Aprender os comandos necessários do Python/NumPy para desenvolvimento de Redes Neurais;\n",
        "* aplicar a métrica ideal para cada tipo de problema;\n",
        "* entender como as Redes Neurais aprendem (_Backpropagation_);\n",
        "\n",
        "# **AGENDA**\n",
        "\n",
        "* Introdução às Redes Neurais;\n",
        "* _Activation Function_;\n",
        "* _Loss Function_;\n",
        "* Métricas para medir a performance das Redes Neurais;\n",
        "* _Dropout_;\n",
        "* _Backpropagation_;\n",
        "* _Gradient Descent_;\n",
        "* _Perceptron_ (Redes Neurais com 1 única camada);\n",
        "* Exemplo 1: Redes Neurais _Perceptron_ para os operadores lógicos E, OU e XOR;\n",
        "* Redes Neurais Multicamada;\n",
        "* Exemplo prático: Rede Neural para identificar o sexo a partir de peso e altura;\n",
        "* Aplicações de Rede Neural:\n",
        "    * Aplicação 1 - Rede Neural para identificar espécies de flores (Iris Dataframe);\n",
        "    * Aplicação 2 - Rede Neural para identificar o tipo do vinho (_tinto or White_);\n",
        "    * Aplicação 3 - Rede Neural para identificar Câncer de Mama (_Breast Cancer_ Dataframe);\n",
        "    * Aplicação 4 - Rede Neural para identificar Diabetes (Diabetes Dataframe);\n",
        "    * Aplicação 5 - Rede Neural para prever os preços das casas em Boston (_Boston House Price Prediction_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYQ4cDfcPu4e"
      },
      "source": [
        "___\n",
        "# **NOTAS E OBSERVAÇÕES**\n",
        "\n",
        "1. Contemplar o uso de StratifiedKFold;\n",
        "2. Inserir aqui o exemplo das notas falsas, enviado pela Mónica;\n",
        "3. Deixar alguma coisa que foi resolvida como exercício.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgX6n2VDyY1O"
      },
      "source": [
        "___\n",
        "# **REFERÊNCIAS**\n",
        "- [An Introduction to Neural Networks](http://www.cs.stir.ac.uk/~lss/NNIntro/InvSlides.html)\n",
        "- [An Introduction to Image Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)\n",
        "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)\n",
        "- [Forward propagation in neural networks — Simplified math and code version](https://towardsdatascience.com/forward-propagation-in-neural-networks-simplified-math-and-code-version-bbcfef6f9250)\n",
        "- [Understanding Neural Networks: From Activation Function To Back Propagation](https://medium.com/fintechexplained/neural-networks-activation-function-to-back-propagation-understanding-neural-networks-bdd036c3f29f)\n",
        "- [Understanding Gradient Descent](https://medium.com/analytics-vidhya/understanding-gradient-descent-8dd88a4c60e6) - Explica detalhadamente como funciona o _Gradient Descent_ no processo de otimização dos pesos $W$;\n",
        "- [Backpropagation step by step](https://medium.com/swlh/backpropagation-step-by-step-13f2b6c0b414) - Eu usei esse artigo para reajustar os pesos $W$;\n",
        "- [Perceptron Learning Algorithm: A Graphical Explanation Of Why It Works](https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975);\n",
        "- [Math behind Perceptrons](https://medium.com/@iamask09/math-behind-perceptrons-7241d5dadbfc);\n",
        "- [Neural Network: A Complete Beginners Guide from Scratch](https://medium.com/gadictos/neural-network-a-complete-beginners-guide-from-scratch-cf1fc9d5cd12);\n",
        "- [Calculating the Backpropagation of a Network](https://medium.com/towards-artificial-intelligence/calculating-back-propagation-of-a-network-1febbcaa2b5d);\n",
        "- [Let’s build a simple Neural Net!](https://becominghuman.ai/lets-build-a-simple-neural-net-f4474256647f) - O autor constroi uma Rede Neural simples, sem _Hidden Layers_;\n",
        "- [Coding Neural Network — Forward Propagation and Backpropagtion](https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76);\n",
        "- [The Simplest Neural Network: Understanding the non-linearity](https://towardsdatascience.com/the-simplest-neural-network-understanding-the-non-linearity-10846d7d0141) - Ótimo texto para entender a não-linearidade;\n",
        "- [Implementing the XOR Gate using Backpropagation in Neural Networks](https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d) - Usei este texto para resolver o problema do XOR;\n",
        "- [Neural Representation of AND, OR, NOT, XOR and XNOR Logic Gates (Perceptron Algorithm)](https://medium.com/@stanleydukor/neural-representation-of-and-or-not-xor-and-xnor-logic-gates-perceptron-algorithm-b0275375fea1) - Eu usei este material para resolver o problema dos operadores E, OU e XOR;\n",
        "- [Solving XOR with a single Perceptron](https://medium.com/@lucaspereira0612/solving-xor-with-a-single-perceptron-34539f395182);\n",
        "- [Machine Learning 101 — Artificial Neural Networks](https://towardsdatascience.com/machine-learning-101-artificial-neural-networks-3-46ccb04cba30) - Cálculos realizados passo a passo;\n",
        "- [Neural Network from scratch in Python](https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65) - Este artigo mostra a matemática por trás das Redes Neurais;\n",
        "- [Classical Neural Net: Why/Which Activations Functions?](https://towardsdatascience.com/classical-neural-net-why-which-activations-functions-401159ba01c4) - Artigo que discute as principais funções de ativação;\n",
        "- [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0);\n",
        "- [Mind: How to Build a Neural Network (Part One)](https://becominghuman.ai/mind-how-to-build-a-neural-network-part-one-67b6aea4ce20);\n",
        "- [How to build a simple Neural Network from scratch with Python](https://towardsdatascience.com/how-to-build-a-simple-neural-network-from-scratch-with-python-9f011896d2f3);\n",
        "- [Comparison of Activation Functions for Deep Neural Networks](https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a);\n",
        "- [MAE and RMSE — Which Metric is Better?](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d) - Ótimo artigo, pois discute qual métrica é melhor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2StZkTpOZbYo"
      },
      "source": [
        "___\n",
        "# **MACHINE LEARNING DEVELOPMENT LYFECYCLE**\n",
        "\n",
        "CRISP-DM significa _Cross Industry Standard Process for Data Mining_ ou processos ou fases para desenvolvimento de projetos relacionados à _Data Mining_ e que tem sido muito utilizados pelos Cientistas de Dados para desenvolvimento de modelos predictivos.\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/CRISP-DM.png?raw=true\" alt=\"Drawing\" width= \"600\"/>\n",
        "\n",
        "Fonte: [The steps to a successful machine learning project](https://emba.epfl.ch/2018/04/10/steps-successful-machine-learning-project/)\n",
        "\n",
        "Sugiro a leitura do artigo [Why using CRISP-DM will make you a better Data Scientist](https://towardsdatascience.com/why-using-crisp-dm-will-make-you-a-better-data-scientist-66efe5b72686)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsCbZd2epfxo"
      },
      "source": [
        "___\n",
        "# **INTRODUÇÃO ÀS REDES NEURAIS**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqqB2vaHXMGt"
      },
      "source": [
        "* Redes Neurais aprendem com as experiências passadas, imitando o funcionamento dos neurônios humanos no processo de aprendizagem;\n",
        "* podem e são amplamente utilizadas nas seguintes situações (aplicações):\n",
        "    * Reconhecimento Facial;\n",
        "    * Processamento de Linguagem Natural (NLP);\n",
        "    * _Self-drive car_;\n",
        "    * Visão computacional;\n",
        "    * Detecção de padrões (doenças, tumores e etc) em imagens;\n",
        "* Ideal para o cenário onde temos muitos dados (_Big Data_) e para resolver problemas complexos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzylPHA7BP0x"
      },
      "source": [
        "___\n",
        "# **_PERCEPTRON_** (Rede Neural com 1 única camada)\n",
        "\n",
        "* **_PERCEPTRON_** é um algoritmo de _Machine Learning_ da classe _Supervised Learning_ para classificação binária, inventado em 1958 por Frank Rosenblatt;\n",
        "* Arquitetura de Rede Neural mais simples existente, com 1 única camada;\n",
        "\n",
        "**Daí, uma pergunta importante**: Se _Perceptron_ é um tipo de Rede Neural simples que nasceu na década de 1950, então porque devemos estudá-la? Porque não focar no estudo de Redes Neurais mais complexas e atuais?\n",
        "\n",
        "*E a resposta é**: porque _Perceptron_ nos permite entender claramente os aspectos matemáticos das Redes Neurais. Com isso quero dizer que ao entendermos _Perceptrons_, fica mais fácil entender outros tipos de Redes Neurais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5YNraza6jum"
      },
      "source": [
        "## Exemplo de Perceptron\n",
        "\n",
        "A seguir, arquitetura do _Perceptron_: várias entradas (_Inputs_) e 1 camada de saída (_Output Layer_) binária (0 ou 1).\n",
        "\n",
        "* OL significa **O**utput **L**ayer ==> Valor que queremos estimar, ou seja, $\\hat{y}$.\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_Fig1.png?raw=true\" alt=\"Drawing\" width= \"1000\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5LVgImx78xY"
      },
      "source": [
        "A **FUNÇÃO DE ATIVAÇÃO** $f(S)$ acima é conhecida como **_STEP FUNCTION_** e como podemos ver, retorna uma resposta binária (0 ou 1) que depende do valor de $S$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84zFWve4FkcY"
      },
      "source": [
        "A seguir, implementação usando NumPy:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htVV-GpgBnw3"
      },
      "source": [
        "[**Python**] - Importar o NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBYyZ5ZiByH4"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8sR77a4B8Uf"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimas para 3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj2dioDTaZl-"
      },
      "source": [
        "np.set_printoptions(precision = 3)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ6Sw4uuCggF"
      },
      "source": [
        "[**Python**] - Definir os pesos $W$ e as entradas (_inputs_) $X$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2m6BxQ_DLFV"
      },
      "source": [
        "# Pesos W:\n",
        "W = np.array([0.1, 0.3, 0.2, 0.4])\n",
        "\n",
        "# Entradas X:\n",
        "X = np.array([1, -3, 2, 3])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBnZP5MKCg8m"
      },
      "source": [
        "[**Python**] - Desenvolver a função soma $S$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMGuWhAhDaim"
      },
      "source": [
        "def Soma(X, W):\n",
        "    S = X.dot(W) # Faz a seguinte operação: S = X1*W1 + X2*W2 + X3*W3 + X4*W4\n",
        "    return S"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMxMJ05kDhmi"
      },
      "source": [
        "[**Python**] - Desenvolver a função de ativação _Step Function_ $f(S)$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRLYPJl0aZmg"
      },
      "source": [
        "def ativacao_StepFunction(S):\n",
        "    if S >= 1:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4g85O2jDu6S"
      },
      "source": [
        "[**Python**] - Calcular $S = Soma(X, W)$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoUMvvzlaZm-",
        "outputId": "079b1989-ac58-42ed-a881-1bde2f19ef9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "S = Soma(X, W)\n",
        "S"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8000000000000003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LzlyDNaD5yB"
      },
      "source": [
        "[**Python**] - Calcular $f(S)$, ou seja, $f = ativacao_StepFunction(S)$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IIe4vIjaZnE",
        "outputId": "fe8c25a1-aafe-4e1c-df08-7523fa986e71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f = ativacao_StepFunction(S)\n",
        "f"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrRG8e8dDTc_"
      },
      "source": [
        "# **EXEMPLO 1: DESENVOLVER UMA REDE NEURAL _PERCEPTRON_ PARA OS OPERADORES LÓGICOS E, OU E XOR**\n",
        "\n",
        "Os exemplos a seguir foram inspirados e adaptado de:\n",
        "* [Perceptron: Theory and Practice](https://medium.com/data-alchemist/perceptron-theory-and-practice-e71733ed3fa5)\n",
        "* [The Perceptron — A Building Block of Neural Networks](https://blog.usejournal.com/the-perceptron-the-building-block-of-neural-networks-5a428d3f451d) - Este artigo mostra detalhadamente os cálculos\n",
        "* [Mind: How to Build a Neural Network (Part One)](https://becominghuman.ai/mind-how-to-build-a-neural-network-part-one-67b6aea4ce20)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeZBP3TQN2_1"
      },
      "source": [
        "## Exemplo 1.1: Rede Neural _Perceptron_ para o Operador Lógico E\n",
        "\n",
        "Considere o dataframe a seguir:\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$) |\n",
        "|---|---|---|---|\n",
        "| 0 | 0 | 0 | 0 |\n",
        "| 1 | 0 | 1 | 0 |\n",
        "| 2 | 1 | 0 | 0 |\n",
        "| 3 | 1 | 1 | 1 |\n",
        "\n",
        "O dataframe acima representa o operador lógico E (https://en.wikipedia.org/wiki/Truth_table):\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$)|\n",
        "|---|---|---|---|\n",
        "| 0 | F | F | F |\n",
        "| 1 | F | T | F |\n",
        "| 2 | T | F | F |\n",
        "| 3 | T | T | T |\n",
        "\n",
        "\n",
        "Considere $W= [W_{1}, W_{2}]= (0, 0)$ como pesos iniciais e a função de ativação $F(S)$ _Step Function_ dada abaixo:\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/StepFunction.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psJh-MUgFAge"
      },
      "source": [
        "A seguir, os cálculos manuais da primeira iteração:\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Perceptron_Operador_E.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8x3EvFUQBsU"
      },
      "source": [
        "Os Erros $E_{i}$ são calculados com a fórmula: $E_{i}= ValorReal_{i} - ValorCalculado_{i}= y_{i}-\\hat{Y}_{i}$. A seguir, resumo dos cálculos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5dryrbGBesj"
      },
      "source": [
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$) | Soma | ValorCalculado ($\\hat{Y}_{i}$) | Erro |\n",
        "|---|---|---|---|---|---|---|\n",
        "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
        "| 1 | 0 | 1 | 0 | 0 | 0 | 0 |\n",
        "| 2 | 1 | 0 | 0 | 0 | 0 | 0 |\n",
        "| 3 | 1 | 1 | 1 | 0 | 0 | 1 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkcRy2RYGLVw"
      },
      "source": [
        "### Erro Total ($E_{T}$)\n",
        "\n",
        "$$E_{T}= \\sum_{i=1}^{n}E_{i}= E_{1}+E_{2}+...+E_{n}$$\n",
        "\n",
        "No nosso caso, temos que $E_{T}= 0+0+0+1= 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzVxmr9OTfGB"
      },
      "source": [
        "### Fórmula para ajustar os pesos $W$\n",
        "A fórmula a seguir será utilizada para ajustar os pesos $W$:\n",
        "\n",
        "$$W_{n+1}= W_{n} + \\alpha*(X*E_{T})$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1bEDxMhToIj"
      },
      "source": [
        "### Taxa de Aprendizagem ($\\alpha$)\n",
        "* $\\alpha$ é a taxa de aprendizado (_Learning Rate_ em inglês) e diz respeito à velocidade de aprendizagem da Rede Neural.\n",
        "    * Quanto MENOR o valor de $\\alpha$ $\\Longrightarrow$ mais devagar e demorada será a convergência para o mínimo global;\n",
        "    * Quanto MAIOR o valor de $\\alpha$ $\\Longrightarrow$ mais rápido será a convergência para o mínimo, **mas sem a garantia de convergência para o mínimo global**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drGfgCIZY4aV"
      },
      "source": [
        "Para ajustar os pesos $W$, vamos utilizar $\\alpha= 0.1$. Fórmula:\n",
        "\n",
        "$$W_{n+1}= W_{n} + \\alpha*(X*E_{T})$$\n",
        "\n",
        "A seguir, os novos pesos $W$ para a próxima iteração da Rede Neural _Perceptron_:\n",
        "\n",
        "\\begin{align}\n",
        "W_{1}&= 0+ 0.1*1*1= 0.1 \\\\\n",
        "W_{2}&= 0+ 0.1*1*1= 0.1 \\\\\n",
        "\\end{align}\n",
        "\n",
        "Portanto, na próxima iteração vamos utilizar os pesos $W= [W_{1}, W_{2}]= [0.1, 0.1]$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33xLPLo-Pq0Y"
      },
      "source": [
        "A seguir, os cálculos manuais para a segunda iteração da Rede Neural:\n",
        "\n",
        "Função de ativação $f(S)$ _Step Function_:\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/StepFunction.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3EsH8pN9wJ6"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Perceptron_Operador_E_I2.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZiiOu1AyW2N"
      },
      "source": [
        "A seguir resumo dos cálculos para a segunda iteração:\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$) | Soma | ValorCalculado ($\\hat{Y}_{i}$) | Erro |\n",
        "|---|---|---|---|---|---|---|\n",
        "| 0 | 0 | 0 | 0 | 0.0 | 0 | 0 |\n",
        "| 1 | 0 | 1 | 0 | 0.1 | 0 | 0 |\n",
        "| 2 | 1 | 0 | 0 | 0.1 | 0 | 0 |\n",
        "| 3 | 1 | 1 | 1 | 0.2 | 0 | 1 |\n",
        "\n",
        "Daí, $E_{T}= 0+0+0+1= 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAXO38uqUobn"
      },
      "source": [
        "### Ajuste dos pesos $W$\n",
        "Fórmula para ajustar $W$:\n",
        "\n",
        "$$W_{n+1}= W_{n} + \\alpha*(X*E_{T})$$\n",
        "\n",
        "A seguir, os novos pesos $W$ para a próxima iteração da Rede Neural _Perceptron_:\n",
        "\n",
        "\\begin{align}\n",
        "W_{1}&= 0.1+ 0.1*1*1= 0.2 \\\\\n",
        "W_{2}&= 0.1+ 0.1*1*1= 0.2 \\\\\n",
        "\\end{align}\n",
        "\n",
        "Portanto, na próxima iteração vamos utilizar os pesos $W= [W_{1}, W_{2}]= [0.2, 0.2]$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX48iRa5VLyk"
      },
      "source": [
        "Esse processo iterativo é realizado até que se encontre os pesos $W$ que nos dê 100% de acurácia. A título de exemplo, considere $W= [W_{1}, W_{2}]= [0.5, 0.5]$:\n",
        "\n",
        "Função de ativação $f(S)$ _Step Function_:\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/StepFunction.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZfroCc994oz"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Perceptron_Operador_E_I5.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McKYXohzXzzA"
      },
      "source": [
        "Como podem ver, o Erro Total $E_{T}= 0$, pois temos 100% de acertos (acurácia) usando $W= [W_{1}, W_{2}]= [0.5, 0.5]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp_tR7h0btDm"
      },
      "source": [
        "### Implementar o **_PERCEPTRON_** no Python usando NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ix5vCKaEWdx"
      },
      "source": [
        "[**Python**] - Importar NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x62R_y89ElPA"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYvLGlgZEXWu"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimais:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEScd0_LEtJc"
      },
      "source": [
        "np.set_printoptions(precision = 3)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8hLz6GAEYCo"
      },
      "source": [
        "[**Python**] - Definir os pesos $W$, entradas (_inputs_) $X$ e Output $Y$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD66QeoqXEU3"
      },
      "source": [
        "# Pesos W:\n",
        "W = np.array([0.0, 0.0])\n",
        "\n",
        "# Entradas X:\n",
        "X = np.array([[0, 0], [0,1], [1, 0], [1, 1]])\n",
        "\n",
        "# Output Y:\n",
        "Y = np.array([[0], [0], [0], [1]])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alRRwxsUvIU6",
        "outputId": "c676e692-0ec6-4a2f-e0b9-8958d94bc431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [0, 1],\n",
              "       [1, 0],\n",
              "       [1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB5n2WNUvND3",
        "outputId": "be295ed2-9023-4f9e-ffa9-938fcc3472bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Y"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jH1EMfdEYwN"
      },
      "source": [
        "[**Python**] - Definir a Taxa de Aprendizagem $\\alpha$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd2k0S-BXEU_"
      },
      "source": [
        "alpha = 0.1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvGa7d8LEZD2"
      },
      "source": [
        "[**Python**] - Desenvolver a função para treinar a Rede Neural\n",
        "> Esta função tenta encontrar os pesos $W$ que levem a 100% de acurácia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVl0XwBuXEVC"
      },
      "source": [
        "def Treinar_RNA(X, Y, W, alpha):\n",
        "    ET= 1 # ET= Erro Total\n",
        "    N= 0\n",
        "    while ((ET != 0) and (N < 100)):\n",
        "        ET= 0\n",
        "        for i in range(len(Y)):\n",
        "            S = X[i].dot(W)\n",
        "            f = ativacao_StepFunction(S)\n",
        "            E= Y[i]-f\n",
        "            ET+= E\n",
        "            for j in range(len(W)):\n",
        "                W[j]= W[j] + alpha*(X[i][j]*E)\n",
        "                print(f'Peso Ajustado: {W[j]}')\n",
        "        print(f'Erro Total: {ET}')\n",
        "        N+= 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdI7EHnFF4yo"
      },
      "source": [
        "[**Python**] - Evocar a função Treinar_RNA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHM5tXEdXEVF",
        "outputId": "b06b304e-f586-40d6-b2a9-21c1df9c00a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Treinar_RNA(X, Y, W, alpha)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Erro Total: [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPKEML9cDD0E"
      },
      "source": [
        "## Exemplo 1.2: Rede Neural _Perceptron_ para o Operador Lógico OU\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSQnOjDWC7Ta"
      },
      "source": [
        "Considere o dataframe a seguir:\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$) |\n",
        "|---|---|---|---|\n",
        "| 0 | 0 | 0 | 0 |\n",
        "| 1 | 0 | 1 | 1 |\n",
        "| 2 | 1 | 0 | 1 |\n",
        "| 3 | 1 | 1 | 1 |\n",
        "\n",
        "O dataframe acima representa o operador lógico OU (https://en.wikipedia.org/wiki/Truth_table):\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$)|\n",
        "|---|---|---|---|\n",
        "| 0 | F | F | F |\n",
        "| 1 | F | T | T |\n",
        "| 2 | T | F | T |\n",
        "| 3 | T | T | T |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kID13PxSGN6h"
      },
      "source": [
        "[**Python**] - Definir os pesos $W$, entradas (_inputs_) $X$ e Output $Y$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmuuIX2PGN6l"
      },
      "source": [
        "# Pesos W:\n",
        "W = np.array([0.0, 0.0])\n",
        "\n",
        "# Entradas X:\n",
        "X = np.array([[0, 0], [0,1], [1, 0], [1, 1]])\n",
        "\n",
        "# Output Y:\n",
        "Y = np.array([[0], [1], [1], [1]])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDzdS6FX2LOC",
        "outputId": "11adc523-c476-4049-8ed7-83862f5eb274",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [0, 1],\n",
              "       [1, 0],\n",
              "       [1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar0dk1eQ2MOD",
        "outputId": "2ca01af6-9199-467b-e4b7-0b1c5eb217b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Y"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agZX698KGeVK"
      },
      "source": [
        "[**Python**] - Evocar a função Treinar_RNA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GF_W4u0GeVM",
        "outputId": "29455e49-9b52-4128-eb45-de17abe73f13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Treinar_RNA(X, Y, W, alpha)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Erro Total: [3]\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [3]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Erro Total: [2]\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.6\n",
            "Peso Ajustado: 0.6\n",
            "Peso Ajustado: 0.6\n",
            "Peso Ajustado: 0.6\n",
            "Peso Ajustado: 0.6\n",
            "Erro Total: [2]\n",
            "Peso Ajustado: 0.6\n",
            "Peso Ajustado: 0.6\n",
            "Peso Ajustado: 0.6\n",
            "Peso Ajustado: 0.7\n",
            "Peso Ajustado: 0.7\n",
            "Peso Ajustado: 0.7\n",
            "Peso Ajustado: 0.7\n",
            "Peso Ajustado: 0.7\n",
            "Erro Total: [2]\n",
            "Peso Ajustado: 0.7\n",
            "Peso Ajustado: 0.7\n",
            "Peso Ajustado: 0.7\n",
            "Peso Ajustado: 0.7999999999999999\n",
            "Peso Ajustado: 0.7999999999999999\n",
            "Peso Ajustado: 0.7999999999999999\n",
            "Peso Ajustado: 0.7999999999999999\n",
            "Peso Ajustado: 0.7999999999999999\n",
            "Erro Total: [2]\n",
            "Peso Ajustado: 0.7999999999999999\n",
            "Peso Ajustado: 0.7999999999999999\n",
            "Peso Ajustado: 0.7999999999999999\n",
            "Peso Ajustado: 0.8999999999999999\n",
            "Peso Ajustado: 0.8999999999999999\n",
            "Peso Ajustado: 0.8999999999999999\n",
            "Peso Ajustado: 0.8999999999999999\n",
            "Peso Ajustado: 0.8999999999999999\n",
            "Erro Total: [2]\n",
            "Peso Ajustado: 0.8999999999999999\n",
            "Peso Ajustado: 0.8999999999999999\n",
            "Peso Ajustado: 0.8999999999999999\n",
            "Peso Ajustado: 0.9999999999999999\n",
            "Peso Ajustado: 0.9999999999999999\n",
            "Peso Ajustado: 0.9999999999999999\n",
            "Peso Ajustado: 0.9999999999999999\n",
            "Peso Ajustado: 0.9999999999999999\n",
            "Erro Total: [2]\n",
            "Peso Ajustado: 0.9999999999999999\n",
            "Peso Ajustado: 0.9999999999999999\n",
            "Peso Ajustado: 0.9999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Erro Total: [2]\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Peso Ajustado: 1.0999999999999999\n",
            "Erro Total: [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2dZAVVFEpCw"
      },
      "source": [
        "## Exemplo 1.3: Rede Neural _Perceptron_ para o Operador Lógico XOR\n",
        "\n",
        "Problema proposto e demonstrado por Rumelhart et al. (1985)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaZIyvvEEpC5"
      },
      "source": [
        "Considere o dataframe a seguir:\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$)|\n",
        "|---|---|---|---|\n",
        "| 0 | 0 | 0 | 0 |\n",
        "| 1 | 0 | 1 | 1 |\n",
        "| 2 | 1 | 0 | 1 |\n",
        "| 3 | 1 | 1 | 0 |\n",
        "\n",
        "O dataframe acima representa o operador lógico XOR (https://pt.wikipedia.org/wiki/Ou_exclusivo):\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$) |\n",
        "|---|---|---|---|\n",
        "| 0 | F | F | F |\n",
        "| 1 | F | T | T |\n",
        "| 2 | T | F | T |\n",
        "| 3 | T | T | F |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rc3hc2RGneF"
      },
      "source": [
        "[**Python**] - Definir os pesos $W$, entradas (_inputs_) $X$ e Output $Y$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8fAgk3RGneH"
      },
      "source": [
        "# Pesos W:\n",
        "W = np.array([0.0, 0.0])\n",
        "\n",
        "# Entradas X:\n",
        "X = np.array([[0, 0], [0,1], [1, 0], [1, 1]])\n",
        "\n",
        "# Output Y:\n",
        "Y = np.array([[0], [1], [1], [0]])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFKaIhua3Mr6",
        "outputId": "23f3e776-3acc-4dc1-ee84-7570edf137ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [0, 1],\n",
              "       [1, 0],\n",
              "       [1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm-X-dXX3NZW",
        "outputId": "7f9d70e8-06f4-40ca-aa23-e6a68b89f7d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Y"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znRL2XozGneM"
      },
      "source": [
        "[**Python**] - Evocar a função Treinar_RNA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8leYHZVGneM",
        "outputId": "4b3776cb-c851-4241-c239-8976be163439",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Treinar_RNA(X, Y, W, alpha)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.0\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Erro Total: [2]\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.1\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Erro Total: [2]\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.2\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Erro Total: [2]\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.30000000000000004\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [2]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.5\n",
            "Peso Ajustado: 0.4\n",
            "Peso Ajustado: 0.4\n",
            "Erro Total: [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Eu5cVvxM60i"
      },
      "source": [
        "## Porque conseguimos pesos $W$ para os Operadores Lógicos E e OU e não para XOR?\n",
        "\n",
        "* Operadores E e OR: Linearmente Separáveis;\n",
        "* Operador XOR: Linearmente NÃO-Separável.\n",
        "\n",
        "[Lucas Araújo](https://medium.com/@lucaspereira0612/solving-xor-with-a-single-perceptron-34539f395182) diz em seu artigo [Solving XOR with a single Perceptron](https://medium.com/@lucaspereira0612/solving-xor-with-a-single-perceptron-34539f395182) que:\n",
        "\n",
        "\"Everyone who has ever studied about neural networks has probably already read that a single perceptron can’t represent the boolean XOR function. The book Artificial Intelligence: A Modern Approach, the leading textbook in AI, says: “[XOR] is not linearly separable so the perceptron cannot learn it”.\n",
        "\n",
        "As figuras abaixo demonstram clarmente os conceitos \"linearmente separáveis\" e \"NÃO-linearmente separável\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUrFCMUjFtR1"
      },
      "source": [
        "### Representação gráfica do Operador Lógico E\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$) |\n",
        "|---|---|---|---|\n",
        "| 0 | 0 | 0 | 0 |\n",
        "| 1 | 0 | 1 | 0 |\n",
        "| 2 | 1 | 0 | 0 |\n",
        "| 3 | 1 | 1 | 1 |\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Grafico_Operador_E.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9v07MdMF42e"
      },
      "source": [
        "### Representação gráfica do Operador Lógico OU\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$) |\n",
        "|---|---|---|---|\n",
        "| 0 | 0 | 0 | 0 |\n",
        "| 1 | 0 | 1 | 1 |\n",
        "| 2 | 1 | 0 | 1 |\n",
        "| 3 | 1 | 1 | 1 |\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Grafico_Operador_OU.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Qp1J6LGBe9"
      },
      "source": [
        "### Representação gráfica do Operador Lógico XOR\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$)|\n",
        "|---|---|---|---|\n",
        "| 0 | 0 | 0 | 0 |\n",
        "| 1 | 0 | 1 | 1 |\n",
        "| 2 | 1 | 0 | 1 |\n",
        "| 3 | 1 | 1 | 0 |\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Grafico_Operador_XOR.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaQm7zZJbNAc"
      },
      "source": [
        "___\n",
        "# **O QUE APRENDEMOS ATÉ AQUI?**\n",
        "\n",
        "* Redes Neurais tentam ajustar os pesos $W$ para tentar melhorar a taxa de acerto. Ou seja, a Rede Neural aprende com os dados através do ajuste iterativo dos pesos $W$;\n",
        "* Treinar uma Rede Neural é uma tarefa computacionalmente intensivo, pois o algoritmo tenta encontrar os pesos $W$ que apresentam melhor acurácia. Para um dataframe grande, o custo conputacional do aprendizado pode ser alto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_T35rXZOB4G"
      },
      "source": [
        "___\n",
        "# **REDES NEURAIS MULTICAMADA**\n",
        "\n",
        "* Pelo menos 1 _Hidden Layer_. Observe a Rede Neural a seguir contendo 20 neurônios distribuídos da seguinte forma:\n",
        "\n",
        "    * Número de neurônios na camada de entrada (_Input Layer_): 4;\n",
        "    * 3 camadas escondidas (_Hidden Layers_) com 5 neurônios cada, totalizando 15 neurônios:\n",
        "        * Número de neurônios na _Hidden Layer 1_: 5;\n",
        "        * Número de neurônios na _Hidden Layer 2_: 5;\n",
        "        * Número de neurônios na _Hidden Layer 3_: 5;\n",
        "    * Número de neurônios na camada de saída (_Output Layer_): 1;\n",
        "* _Fully connected layer_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dXBXuh2-Tuo"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_Multicamada.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK4O_Y_l2vev"
      },
      "source": [
        "## Função _Sigmoid_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_nn8zELXEVf"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Sigmoid_Limite.png?raw=true\" alt=\"Drawing\" width= \"800\"/>\n",
        "\n",
        "Consulte [e (constante matemática)](https://pt.wikipedia.org/wiki/E_(constante_matem%C3%A1tica)) para saber mais sobre a constante de Euler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOWwWR7hOmir"
      },
      "source": [
        "## Número de _Hidden Layers_\n",
        "\n",
        "Pesquisadores apontam que 1 única _Hidden Layer_ é suficiente para a grande maioria dos problemas e que usualmente cada _Hidden Layer_ possui o mesmo número de neurônios. Experimentos mostram que mais _Hidden Layers_ implica em maior tempo para treinar a Rede Neural. No entanto, [Heaton Research](https://www.heatonresearch.com/2017/06/01/hidden-layers.html), mostra que:\n",
        "\n",
        "![Determinining_number_Hidden_Layers](https://github.com/MathMachado/Materials/blob/master/Determinining_number_Hidden_Layers.png?raw=true)\n",
        "\n",
        "Fonte: [Heaton Research](https://www.heatonresearch.com/2017/06/01/hidden-layers.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4_1JCbcPRrn"
      },
      "source": [
        "## Número de neurônios na camada de entrada (_Input Layer_): $N_{I}$\n",
        "\n",
        "$N_{I}$= Número de colunas (ou variáveis) no dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk-lhwhffUZz"
      },
      "source": [
        "### Número de neurônios na camada de saída (_Output Layer_): $N_{O}$\n",
        "\n",
        "* Se a Rede Neural é uma regressão, então o número de neurônios na _Output Layer_ é 1, pois o _output_ de uma regressão é um valor;\n",
        "* Se a Rede Neural é uma classificação e usamos uma função de ativação probabilística (como _softmax_, por exemplo), então o número de neurônios na _Output Layer_ é igual ao número de classes que queremos prever. Por exemplo, no problema de classificar espécies no dataframe IRIS temos 3 espécies (versicolor, virginica e setosa). Ao utilizarmos a função de ativação _softmax_, então teremos 3 neurônios na _Output Layer_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsrrdLpSfYm9"
      },
      "source": [
        "## Número de neurônios na camada escondida (_Hidden Layer_): $N_{H}$\n",
        "\n",
        "Determinar o número de neurônios na _Hidden Layer_ tem sido um exercício de tentativa e erro, mas alguns experimentos tem demonstrado que o número adequado de neurônios na _Hidden Layer_ pode ser obtido através da expressão a seguir:\n",
        "\n",
        "$$N_{H}= \\frac{N_{I}+N_{O}}{2}$$\n",
        "\n",
        "No entanto, o artigo [How to choose the number of hidden layers and nodes in a feedforward neural network?](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw) sugere o uso da seguinte expressão:\n",
        "\n",
        "$$N_H= \\frac{N}{\\alpha(N_{I}+N_{O})}$$\n",
        "\n",
        "onde $N$ é o número de instâncias (linhas) do dataframe e $\\alpha$ é um número entre 2 e 10, sendo que alguns experimentos com $\\alpha= 2$ produzem bons modelos sem _overfitting_. Para saber mais sobre esta expressão e sobre $\\alpha$, sugiro a leitura do artigo mencionado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj6WfilbShX3"
      },
      "source": [
        "## Rede Neural Multicamada para o Operador Lógico XOR.\n",
        "\n",
        "Dataframe que representa o Operador Lógico XOR:\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$)|\n",
        "|---|---|---|---|\n",
        "| 0 | 0 | 0 | 0 |\n",
        "| 1 | 0 | 1 | 1 |\n",
        "| 2 | 1 | 0 | 1 |\n",
        "| 3 | 1 | 1 | 0 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uURlcU78LwbH"
      },
      "source": [
        "### Arquitetura da Rede Neural Multicamada que vamos desenvolver para o Operador Lógico XOR\n",
        "\n",
        "Os pesos $W_{H}= \\begin{bmatrix} W_{H}^{(1, 1)} & W_{H}^{(1, 2)} & W_{H}^{(1, 3)} \\\\ W_{H}^{(2, 1)} & W_{H}^{(2, 2)} & W_{H}^{(2, 3)} \\end{bmatrix}$ e $W_{O}= \\begin{bmatrix} W_{O}^{(1)} \\\\ W_{O}^{(2)} \\\\ W_{O}^{(3)} \\end{bmatrix}$ serão gerados aleatoriamente. A seguir, a arquitetura da Rede Neural com 1 _Hidden Layer_ contendo 3 neurônios:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XKMdlZr-e9l"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_Generic1.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV2eUQDuLCUL"
      },
      "source": [
        "[**Python**] - Importar as bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTWYP0V-LGHj"
      },
      "source": [
        "import math\n",
        "import numpy as np"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DG86PgxLDQA"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimais:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsvh5DOkXEVm"
      },
      "source": [
        "np.set_printoptions(precision = 3)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYIMcp8TLVuq"
      },
      "source": [
        "[**Python**] - Definir as entradas (_inputs_) $X$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Mt6zTnXEVq",
        "outputId": "39aa3fdf-81b1-43c8-a261-f254aa22d98d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "X"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [0, 1],\n",
              "       [1, 0],\n",
              "       [1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXLd1nZxLbXD"
      },
      "source": [
        "[**Python**] - Definir os _Outputs_ $Y$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oauq3veAXEVu",
        "outputId": "28df7cbc-2480-447f-db98-91dabafcc1b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Y = np.array([[0], [1], [1], [0]])\n",
        "Y"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC1y0tO1MAU9"
      },
      "source": [
        "### Gerar os pesos $W_{H}= \\begin{bmatrix} W_{H}^{(1, 1)} & W_{H}^{(1, 2)} & W_{H}^{(1, 3)} \\\\ W_{H}^{(2, 1)} & W_{H}^{(2, 2)} & W_{H}^{(2, 3)} \\end{bmatrix}$ e $W_{O}= \\begin{bmatrix} W_{O}^{(1)} \\\\ W_{O}^{(2)} \\\\ W_{O}^{(3)} \\end{bmatrix}$ aleatoriamente\n",
        "\n",
        "Por questões de reproducibilidade de resultados, vamos usar as sementes a seguir para gerar os pesos $W_{H}$ e $W_{O}$:\n",
        "\n",
        "* _seed_= 20111974 para gerar $W_{H}$;\n",
        "* _seed_= 19741120 para gerar $W_{O}$.\n",
        "\n",
        "Ao usarmos estas sementes, deveremos ter $W_{H}= \\begin{bmatrix} 0.531 & 0.570 & 0.543 \\\\ 0.655 & 0.857 & 0.602 \\end{bmatrix}$ e $W_{O}= \\begin{bmatrix} 0.240 \\\\ 0.318 \\\\ 0.142 \\end{bmatrix}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U3Id5XXG5tw"
      },
      "source": [
        "[**Python**] - Sementes para gerar $W_{H}$ (aleatoriamente)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVXiIpgIHId9"
      },
      "source": [
        "np.random.seed(20111974)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYj0NYofHKkk"
      },
      "source": [
        "[**Python**] - Gerar os pesos $W_{H}$ (aleatoriamente)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1eGsPNQXEVx",
        "outputId": "87231b73-8a82-4c24-f4b8-7a236502d63a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "W_H = np.array([np.random.random(3), np.random.random(3)])\n",
        "W_H"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.531, 0.57 , 0.543],\n",
              "       [0.655, 0.857, 0.602]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj6KJnP3Hbqf"
      },
      "source": [
        "[**Python**] - Sementes para gerar $W_{O}$ (aleatoriamente):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkVw-SWSHbqh"
      },
      "source": [
        "np.random.seed(19741120)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7ZjUT4oHbqk"
      },
      "source": [
        "[**Python**] - Gerar os pesos $W_{O}$ (aleatoriamente):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebs8p8mOXEV1",
        "outputId": "7a4fe3ca-e2ce-4944-9d45-13b994ea6fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "W_O = np.array([np.random.random(1), np.random.random(1), np.random.random(1)])\n",
        "W_O"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.24 ],\n",
              "       [0.318],\n",
              "       [0.142]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg1ByKjKsWcE"
      },
      "source": [
        "Confira os pesos dispostos na figura a seguir (antes x depois):\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_X1_X2.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiEc1DwPt7Hm"
      },
      "source": [
        "### Calcular $S = \\sum_{i=1}^{4}X_{i}W_{i}$ e passar o valor de $S$ para a função de ativação $f(S)$ (_Sigmoid_)\n",
        "\n",
        "Função _Sigmoid_:\n",
        "\n",
        "$$f(x)= y= \\frac{1}{1+e^{-x}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCZsXjIhHqId"
      },
      "source": [
        "[**Python**] - Definir a função de ativação $Sigmoid$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB4-UnOGXEV8"
      },
      "source": [
        "def FuncaoAtivacao_Sigmoid(x):\n",
        "    y = 1/(1+np.exp(-x))\n",
        "    return y"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkvMHw1KHrjT"
      },
      "source": [
        "[**Python**] - Função MostraCalculos, desenvolvida para validarmos os cálculos manuais de $S$ e $f(S)$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsxHrthYXEWA"
      },
      "source": [
        "def MostraCalculos(i):\n",
        "    print(f'Array W:\\n {W_H}')\n",
        "    print('\\n')\n",
        "    print(f'Array X:\\n {X[i]}')\n",
        "    S = X[i].dot(W_H)\n",
        "    f = FuncaoAtivacao_Sigmoid(S)\n",
        "    S2= f.dot(W_O)\n",
        "    f2= FuncaoAtivacao_Sigmoid(S2)\n",
        "    \n",
        "    print('\\n')\n",
        "    print(f'*** HIDDEN LAYER ***')\n",
        "    print(f'Função Soma S: {S}')\n",
        "    print(f'Função de Ativação Sigmoid: {f}')\n",
        "    \n",
        "    print('\\n')\n",
        "    print(f'*** OUTPUT LAYER ***')\n",
        "    print(f'Função Soma S: {S2}')\n",
        "    print(f'Função de Ativação Sigmoid: {f2}')\n",
        "    \n",
        "    print('\\n')\n",
        "    print(f'*** ERRO ***')\n",
        "    E= Y[i]-f2\n",
        "    print(f'Erro da linha i= {i}: {E}')\n",
        "          \n",
        "    return f   "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s80knPTzcIBy"
      },
      "source": [
        "___\n",
        "O Operador A.dot(B) faz o produto matricial entre os arrays A e B. Para saber mais sobre a função dot(), assista este [vídeo](https://youtu.be/Pb1VIe9657s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw0p2m8mbz3C"
      },
      "source": [
        "#### $\\Longrightarrow$ Para $i = 0$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CelKhuoHISyS"
      },
      "source": [
        "[**Python**] - Evocar a função f0= MostraCalculos(0):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar0zOLuUIio1",
        "outputId": "e902b1d1-116b-4e4b-8390-bc1791e50ccb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f0 = MostraCalculos(0)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array W:\n",
            " [[0.531 0.57  0.543]\n",
            " [0.655 0.857 0.602]]\n",
            "\n",
            "\n",
            "Array X:\n",
            " [0 0]\n",
            "\n",
            "\n",
            "*** HIDDEN LAYER ***\n",
            "Função Soma S: [0. 0. 0.]\n",
            "Função de Ativação Sigmoid: [0.5 0.5 0.5]\n",
            "\n",
            "\n",
            "*** OUTPUT LAYER ***\n",
            "Função Soma S: [0.35]\n",
            "Função de Ativação Sigmoid: [0.587]\n",
            "\n",
            "\n",
            "*** ERRO ***\n",
            "Erro da linha i= 0: [-0.587]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R1LdY9QvTqb"
      },
      "source": [
        "Observe na figura abaixo os cálculos manuais da Soma $S$, função de ativação $f(S)$ e Erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl_RBLiaa4xS"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_XOR_0.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOKtMLHoo_Yt"
      },
      "source": [
        "##### _HIDDEN LAYER_\n",
        "\\begin{align}\n",
        "S_{H}^{(0, 1)} &= (0)(0.531)+(0)(0.655)= 0 \\Longrightarrow f_{H}^{(0, 1)}(S_{H}^{(0, 1)})= f_{H}^{(0, 1)}(0)= 0.5 \\\\\n",
        "S_{H}^{(0, 2)} &= (0)(0.570)+(0)(0.857)= 0 \\Longrightarrow f_{H}^{(0, 2)}(S_{H}^{(0, 2)})= f_{H}^{(0, 2)}(0)= 0.5 \\\\\n",
        "S_{H}^{(0, 3)} &= (0)(0.543)+(0)(0.602)= 0 \\Longrightarrow f_{H}^{(0, 3)}(S_{H}^{(0, 3)})= f_{H}^{(0, 3)}(0)= 0.5\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kw-cakYsQGp"
      },
      "source": [
        "##### _OUTPUT LAYER_\n",
        "\n",
        "\\begin{align}\n",
        "S_{O}^{(0)}&= (0.5)(0.24)+(0.5)(0.318)+(0.5)(0.142)= 0.35 \\\\\n",
        "f_{O}^{(0)}(S_{O}^{(0)})&= f_{O}^{(0)}(0.35)= 0.587 \\\\\n",
        "E_{0}&= 0-0.587= -0.587\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFZ8w1dUdT7A"
      },
      "source": [
        "#### $\\Longrightarrow$ Para $i = 1$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTz3EfAUIoz-"
      },
      "source": [
        "[**Python**] - Evocar a função f1= MostraCalculos(1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INUDJ_aMXEWb",
        "outputId": "d6110e3b-c012-41c4-9fdc-44fd8ef48f58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f1 = MostraCalculos(1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array W:\n",
            " [[0.531 0.57  0.543]\n",
            " [0.655 0.857 0.602]]\n",
            "\n",
            "\n",
            "Array X:\n",
            " [0 1]\n",
            "\n",
            "\n",
            "*** HIDDEN LAYER ***\n",
            "Função Soma S: [0.655 0.857 0.602]\n",
            "Função de Ativação Sigmoid: [0.658 0.702 0.646]\n",
            "\n",
            "\n",
            "*** OUTPUT LAYER ***\n",
            "Função Soma S: [0.473]\n",
            "Função de Ativação Sigmoid: [0.616]\n",
            "\n",
            "\n",
            "*** ERRO ***\n",
            "Erro da linha i= 1: [0.384]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I91qgS1Uh2T1"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_XOR_1.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDuyxsKSvDds"
      },
      "source": [
        "##### _HIDDEN LAYER_\n",
        "\\begin{align}\n",
        "S_{H}^{(1, 1)} &= (0)(0.531)+(1)(0.655)= 0.655 \\Longrightarrow f_{H}^{(1, 1)}(S_{H}^{(1, 1)})= f_{H}^{(1, 1)}(0.655)= 0.658 \\\\\n",
        "S_{H}^{(1, 2)} &= (0)(0.570)+(1)(0.857)= 0.857 \\Longrightarrow f_{H}^{(1, 2)}(S_{H}^{(1, 2)})= f_{H}^{(1, 2)}(0.857)= 0.702 \\\\\n",
        "S_{H}^{(1, 3)} &= (0)(0.543)+(1)(0.602)= 0.602 \\Longrightarrow f_{H}^{(1, 3)}(S_{H}^{(1, 3)})= f_{H}^{(1, 3)}(0.602)= 0.646\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKPsQA9dvDdt"
      },
      "source": [
        "##### _OUTPUT LAYER_\n",
        "\n",
        "\\begin{align}\n",
        "S_{O}^{(1)}&= (0.658)(0.24)+(0.702)(0.318)+(0.646)(0.142)= 0.473 \\\\\n",
        "f_{O}^{(1)}(S_{O}^{(1)})&= f_{O}^{(1)}(0.473)= 0.616 \\\\\n",
        "E_{1}&= 1-0.616= 0.384\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBfztHLfeoTR"
      },
      "source": [
        "#### $\\Longrightarrow$ Para $i = 2$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjcpG53tIvHf"
      },
      "source": [
        "[**Python**] - Evocar a função f2= MostraCalculos(2):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbnG_WxdXEWg",
        "outputId": "adc1de00-c4e4-4497-9309-04a9daa3954b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f2 = MostraCalculos(2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array W:\n",
            " [[0.531 0.57  0.543]\n",
            " [0.655 0.857 0.602]]\n",
            "\n",
            "\n",
            "Array X:\n",
            " [1 0]\n",
            "\n",
            "\n",
            "*** HIDDEN LAYER ***\n",
            "Função Soma S: [0.531 0.57  0.543]\n",
            "Função de Ativação Sigmoid: [0.63  0.639 0.632]\n",
            "\n",
            "\n",
            "*** OUTPUT LAYER ***\n",
            "Função Soma S: [0.444]\n",
            "Função de Ativação Sigmoid: [0.609]\n",
            "\n",
            "\n",
            "*** ERRO ***\n",
            "Erro da linha i= 2: [0.391]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g9MegqIh-Vn"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_XOR_2.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gES50aaxszM"
      },
      "source": [
        "##### _HIDDEN LAYER_\n",
        "\\begin{align}\n",
        "S_{H}^{(2, 1)} &= (1)(0.531)+(0)(0.655)= 0.531 \\Longrightarrow f_{H}^{(2, 1)}(S_{H}^{(2, 1)})= f_{H}^{(2, 1)}(0.531)= 0.630 \\\\\n",
        "S_{H}^{(2, 2)} &= (1)(0.570)+(0)(0.857)= 0.570 \\Longrightarrow f_{H}^{(2, 2)}(S_{H}^{(2, 2)})= f_{H}^{(2, 2)}(0.570)= 0.639 \\\\\n",
        "S_{H}^{(2, 3)} &= (1)(0.543)+(0)(0.602)= 0.543 \\Longrightarrow f_{H}^{(2, 3)}(S_{H}^{(2, 3)})= f_{H}^{(2, 3)}(0.543)= 0.632\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7n4Eq-6xszP"
      },
      "source": [
        "##### _OUTPUT LAYER_\n",
        "\n",
        "\\begin{align}\n",
        "S_{O}^{(2)}&= (0.630)(0.24)+(0.639)(0.318)+(0.632)(0.142)= 0.444 \\\\\n",
        "f_{O}^{(2)}(S_{O}^{(2)})&= f_{O}^{(2)}(0.444)= 0.609 \\\\\n",
        "E_{2}&= 1-0.609= 0.391\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPJQKwBthCkh"
      },
      "source": [
        "#### $\\Longrightarrow$ Para $i = 3$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVhEsrqJI1T7"
      },
      "source": [
        "[**Python**] - Evocar a função f3= MostraCalculos(3):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU87GWKjXEWo",
        "outputId": "b7a0f521-6443-4e17-b6fc-a8032c9ff2dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f3 = MostraCalculos(3)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array W:\n",
            " [[0.531 0.57  0.543]\n",
            " [0.655 0.857 0.602]]\n",
            "\n",
            "\n",
            "Array X:\n",
            " [1 1]\n",
            "\n",
            "\n",
            "*** HIDDEN LAYER ***\n",
            "Função Soma S: [1.186 1.427 1.144]\n",
            "Função de Ativação Sigmoid: [0.766 0.806 0.758]\n",
            "\n",
            "\n",
            "*** OUTPUT LAYER ***\n",
            "Função Soma S: [0.548]\n",
            "Função de Ativação Sigmoid: [0.634]\n",
            "\n",
            "\n",
            "*** ERRO ***\n",
            "Erro da linha i= 3: [-0.634]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjUGJdaYiEH0"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_XOR_3.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKptTBkBzysP"
      },
      "source": [
        "##### _HIDDEN LAYER_\n",
        "\\begin{align}\n",
        "S_{H}^{(3, 1)} &= (1)(0.531)+(1)(0.655)= 1.186 \\Longrightarrow f_{H}^{(3, 1)}(S_{H}^{(3, 1)})= f_{H}^{(3, 1)}(1.186)= 0.766 \\\\\n",
        "S_{H}^{(3, 2)} &= (1)(0.570)+(1)(0.857)= 1.427 \\Longrightarrow f_{H}^{(3, 2)}(S_{H}^{(3, 2)})= f_{H}^{(3, 2)}(1.427)= 0.806 \\\\\n",
        "S_{H}^{(3, 3)} &= (1)(0.543)+(1)(0.602)= 1.144 \\Longrightarrow f_{H}^{(3, 3)}(S_{H}^{(3, 3)})= f_{H}^{(3, 3)}(1.144)= 0.758\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISxS131GzysR"
      },
      "source": [
        "##### _OUTPUT LAYER_\n",
        "\n",
        "\\begin{align}\n",
        "S_{O}^{(3)}&= (0.766)(0.24)+(0.806)(0.318)+(0.758)(0.142)= 0.548 \\\\\n",
        "f_{O}^{(3)}(S_{O}^{(3)})&= f_{O}^{(3)}(0.548)= 0.634 \\\\\n",
        "E_{3}&= 0-0.634= -0.634\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR3X25venRv5"
      },
      "source": [
        "### Resumo dos cálculos com _arrays_\n",
        "\n",
        "Os cálculos que foram realizados previamente com o NumPy _step by step_ aqui são feitos utilizando produto matricial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9D-5dE-I_IS"
      },
      "source": [
        "[**Python**] - Funções de ativação da _Hidden Layer_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efO2aSu8AzMp",
        "outputId": "107dedd9-956c-4979-c404-db7529665ec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f0"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5, 0.5, 0.5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoDRBC8oXEW0",
        "outputId": "6d8869c5-fab5-450b-e8e4-8e9c78054ab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X2 = np.array([f0, f1, f2, f3])\n",
        "X2"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5  , 0.5  , 0.5  ],\n",
              "       [0.658, 0.702, 0.646],\n",
              "       [0.63 , 0.639, 0.632],\n",
              "       [0.766, 0.806, 0.758]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIwTcF1JLIm"
      },
      "source": [
        "[**Python**] - Calcular a soma $S$ da _Output Layer_, dado pelo produto matricial de $X2$ por $W_{O}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddyC0sa6XEW5",
        "outputId": "0b8b8e4d-c2dc-4968-b2eb-57124444578e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "S = X2.dot(W_O)\n",
        "S"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35 ],\n",
              "       [0.473],\n",
              "       [0.444],\n",
              "       [0.548]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRkyUZN7Jooz"
      },
      "source": [
        "[**Python**] - Função de ativação da _Output Layer_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jadac2Q3XEW-",
        "outputId": "f12eef7b-2255-4283-eee7-be5cf0de252f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f = FuncaoAtivacao_Sigmoid(S)\n",
        "f"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.587],\n",
              "       [0.616],\n",
              "       [0.609],\n",
              "       [0.634]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuTe0mHg8Kzk"
      },
      "source": [
        "Os resultados das funções de ativação acima conferem com o resumo a seguir:\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Resumo.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2lHqqhmM6rd"
      },
      "source": [
        "[**Python**] - Calcular os Erros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCu8miA2XEXE",
        "outputId": "2a6061f5-8d2f-489b-c39c-e2c7a8432c26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "E = Y - f\n",
        "E"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.587],\n",
              "       [ 0.384],\n",
              "       [ 0.391],\n",
              "       [-0.634]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Q019cxotQM"
      },
      "source": [
        "Os cálculos estão resumidos na tabela a seguir:\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$) | ValorCalculado ($\\hat{Y}_{i}$) | $Erro$ | $Erro^{2}$ |\n",
        "|---|---|---|---|---|:----------------------:|------------------:|\n",
        "| 0 | 0 | 0 | 0 | 0.587 | -0.587 | 0.344 |\n",
        "| 1 | 0 | 1 | 1 | 0.616 | 0.384 | 0.147 |\n",
        "| 2 | 1 | 0 | 1 | 0.609 | 0.391 | 0.152 |\n",
        "| 3 | 1 | 1 | 0 | 0.634 | -0.634 | 0.401 |\n",
        "\n",
        "Onde:\n",
        "\n",
        "$Erro= y_{i}-\\hat{Y}_{i}$= ValorReal - ValorCalculado\n",
        "\n",
        "O cálculo do MSE será $MSE= \\frac{0.344+0.147+0.152+0.401}{4}= \\frac{1.044}{4}= 0.261$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHm_16jEz-kL"
      },
      "source": [
        "### Métrica para avaliação da performance da Rede Neural\n",
        "\n",
        "* O MSE é uma das principais métricas para medir a performance das Redes Neurais. A seguir, o cálculo do MSE:\n",
        "\n",
        "$$MSE= \\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{Y}_{i})^{2}}{n}= \\frac{(0-0.587)^{2}+(1-0.616)^{2}+(1-0.609)^{2}+(0-0.634)^{2}}{4}= \\frac{0.344+0.147+0.152+0.401}{4}=0.261$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Yo6TdpNIPW"
      },
      "source": [
        "[**Python**] - Desenvolver função MSE para calcular o MSE = Erro Quadrático Médio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EENpe-rbXEXL"
      },
      "source": [
        "def MSE(Y, f):\n",
        "    return np.square(Y - f).mean()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySpVD0-mNQ1s"
      },
      "source": [
        "[**Python**] - Evocar a função $MSE(Y, f)$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0L5ACZnXEXP",
        "outputId": "34d3078d-2e71-44b3-959f-3575ea3b050c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MSE(Y, f)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2614527354351902"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpzv12a48GhA"
      },
      "source": [
        "### _Backpropagation_ - Ajuste dos pesos $W_{O}= \\begin{bmatrix} W_{O}^{(1)} \\\\ W_{O}^{(2)} \\\\ W_{O}^{(3)} \\end{bmatrix}$\n",
        "\n",
        "> _Backpropagation_ (ou simplesmente _Backward_) é o processo que faz com que a Rede Neural aprenda a partir da atualização iterativa dos pesos $W$. A ideia do _Backpropagation_ é que podemos melhorar a performance da Rede Neural através da calibração dos pesos $W$ usando _Gradient Descent_, de forma que os _outputs_ ($\\hat{y}_{i}$) serão cada vez mais próximos do valor real ($y_{i}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjBeg2TTcd40"
      },
      "source": [
        "Como vimos anteriormente, a fórmula para atualização dos pesos $W$, dada pela expressão abaixo\n",
        "\n",
        "$$W_{n+1}= W_{n}*M+\\alpha \\frac{\\partial L}{\\partial W_{n}}= W_{n}*M+\\alpha*(X*\\Delta)$$\n",
        "\n",
        "necessita da derivada da _Loss Function_ $L$, que é a função _Sigmoid_, cuja expressão matemática é dada a seguir:\n",
        "\n",
        "$$y(x)= \\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "Portanto, a derivada da função _Sigmoid_ é dada pela expressão a seguir:\n",
        "\n",
        "$$\\frac{dy}{dx}= y^{'}= y(1-y)$$\n",
        "\n",
        "Caso você tenha dúvidas sobre a derivada da função de ativação _Sigmoid_, sugiro a leitura deste artigo: [Derivative of the Sigmoid function](https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDKJakFImuRp"
      },
      "source": [
        "* $D_{O}$ é a Derivada do neurônio da _Output Layer_;\n",
        "* $\\Delta_{H}= D_{O}* W_{O} * \\Delta_{O}$;\n",
        "* $\\Delta_{O}= E_{i}*D_{O}$.\n",
        "\n",
        "A seguir, a Derivada da função _Sigmoid_ usando o NumPy:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDnarWwwNZd0"
      },
      "source": [
        "[**Python**] - Função Derivada_Sigmoid, que calcula a Derivada da função _Sigmoid_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSxVsNeDXEXY"
      },
      "source": [
        "def Derivada_Sigmoid(y):\n",
        "    return y*(1-y)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY6O0qkWNhby"
      },
      "source": [
        "[**Python**] - Evocar a Derivada_Sigmoid(f), ou seja, calcular a derivada das funções de ativação da _Output Layer_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTpQfBTpXEXi",
        "outputId": "2bf6ea04-5be7-4b35-c00a-e94c69984ef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "D_O = Derivada_Sigmoid(f)\n",
        "D_O"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.242],\n",
              "       [0.237],\n",
              "       [0.238],\n",
              "       [0.232]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfVdgCFDf9-X"
      },
      "source": [
        "Os cálculos acima foram feitos no NumPy e são reproduzidos manualmente abaixo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOslp3YSN70r"
      },
      "source": [
        "[**Python**] - Função Backpropagation para calcular:\n",
        "* _Output Layer_:\n",
        "    * $D_{O}$ - Derivada dos valores da _Output Layer_;\n",
        "    * $\\Delta_{O}$ - Delta;\n",
        "* _Hidden Layer_:\n",
        "    * $D_{H}$ - Derivada dos valores da _Hidden Layer_;\n",
        "    * $\\Delta_{H}$ - Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PihyM2VXEXq"
      },
      "source": [
        "def Backpropagation(i):\n",
        "    print(f'***** OUTPUT LAYER *****')\n",
        "    print(f'*** Função de ativação ***')\n",
        "    print(f[i])\n",
        "    \n",
        "    #print('\\n')\n",
        "    print(f'*** Derivada ***')\n",
        "    D_O= Derivada_Sigmoid(f)\n",
        "    print(D_O[i])\n",
        "\n",
        "    #print('\\n')          \n",
        "    print(f'*** Erros ***')\n",
        "    print(E[i])\n",
        "\n",
        "    #print('\\n')\n",
        "    print(f'*** Delta ***')\n",
        "    Delta_O= D_O*E\n",
        "    print(Delta_O[i])\n",
        "    \n",
        "    print('\\n')\n",
        "    print(f'***** HIDDEN LAYER *****')\n",
        "    print(f'*** Função de ativação ***')\n",
        "    print(X2[i])\n",
        "\n",
        "    #print('\\n')\n",
        "    print(f'*** Derivada ***')\n",
        "    D_H= Derivada_Sigmoid(X2)\n",
        "    D_H\n",
        "    print(D_H[i])    \n",
        "          \n",
        "    #print('\\n')\n",
        "    print(f'*** Delta ***')\n",
        "    Delta_H= D_H*W_O.T*Delta_O\n",
        "    print(Delta_H[i])   "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyGWGHVaFxNG"
      },
      "source": [
        "#### $\\Longrightarrow$ Para $i = 0$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiOtWoNWOn24"
      },
      "source": [
        "[**Python**] - Evocar a função Backpropagation(0):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiNkv_DBXEXu",
        "outputId": "b35505da-0375-444a-d908-582550e8b864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Backpropagation(0)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** OUTPUT LAYER *****\n",
            "*** Função de ativação ***\n",
            "[0.587]\n",
            "*** Derivada ***\n",
            "[0.242]\n",
            "*** Erros ***\n",
            "[-0.587]\n",
            "*** Delta ***\n",
            "[-0.142]\n",
            "\n",
            "\n",
            "***** HIDDEN LAYER *****\n",
            "*** Função de ativação ***\n",
            "[0.5 0.5 0.5]\n",
            "*** Derivada ***\n",
            "[0.25 0.25 0.25]\n",
            "*** Delta ***\n",
            "[-0.009 -0.011 -0.005]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqZ_CvGI0ySD"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_XOR_0_2.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO4njWZb1V1w"
      },
      "source": [
        "##### _HIDDEN LAYER_\n",
        "\\begin{align}\n",
        "\\Delta_{H}^{(0, 1)} &= D_{H}^{(0, 1)}.W_{O}^{(1)}.\\Delta_{O}^{(0)}= (0.25)(0.24)(-0.142)= -0.009 \\\\\n",
        "\\Delta_{H}^{(0, 2)} &= D_{H}^{(0, 2)}.W_{O}^{(2)}.\\Delta_{O}^{(0)}= (0.25)(0.318)(-0.142)= -0.011 \\\\\n",
        "\\Delta_{H}^{(0, 3)} &= D_{H}^{(0, 3)}.W_{O}^{(3)}.\\Delta_{O}^{(0)}= (0.25)(0.142)(-0.142)= -0.005\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXpozezsYFCX"
      },
      "source": [
        "Na figura acima, temos que $\\Delta_{H}^{(0)}= [\\Delta_{H}^{(0, 1)}, \\Delta_{H}^{(0, 2)}, \\Delta_{H}^{(0, 3)}]= [-0.009, -0.011, -0.005]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDZYiujcHGzK"
      },
      "source": [
        "#### $\\Longrightarrow$ Para $i = 1$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROzKv5VtOuy5"
      },
      "source": [
        "[**Python**] - Evocar a função Backpropagation(1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6An6CyUXEX0",
        "outputId": "29516a2a-356c-42b0-dee2-cf8f9d49f10c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Backpropagation(1)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** OUTPUT LAYER *****\n",
            "*** Função de ativação ***\n",
            "[0.616]\n",
            "*** Derivada ***\n",
            "[0.237]\n",
            "*** Erros ***\n",
            "[0.384]\n",
            "*** Delta ***\n",
            "[0.091]\n",
            "\n",
            "\n",
            "***** HIDDEN LAYER *****\n",
            "*** Função de ativação ***\n",
            "[0.658 0.702 0.646]\n",
            "*** Derivada ***\n",
            "[0.225 0.209 0.229]\n",
            "*** Delta ***\n",
            "[0.005 0.006 0.003]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwblrxI20ygW"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_XOR_1_2.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18bsrlv_4B0Q"
      },
      "source": [
        "##### _HIDDEN LAYER_\n",
        "\\begin{align}\n",
        "\\Delta_{H}^{(1, 1)} &= D_{H}^{(1, 1)}.W_{O}^{(1)}.\\Delta_{O}^{(1)}= (0.225)(0.24)(0.091)= 0.005 \\\\\n",
        "\\Delta_{H}^{(1, 2)} &= D_{H}^{(1, 2)}.W_{O}^{(2)}.\\Delta_{O}^{(1)}= (0.209)(0.318)(0.091)= 0.006 \\\\\n",
        "\\Delta_{H}^{(1, 3)} &= D_{H}^{(1, 3)}.W_{O}^{(3)}.\\Delta_{O}^{(1)}= (0.229)(0.142)(0.091)= 0.003\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPfQUUUHYw4i"
      },
      "source": [
        "Na figura acima, temos que $\\Delta_{H}^{(1)}= [\\Delta_{H}^{(1, 1)}, \\Delta_{H}^{(1, 2)}, \\Delta_{H}^{(1, 3)}]= [0.005, 0.006, 0.003]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8qfA8CGHJo8"
      },
      "source": [
        "#### $\\Longrightarrow$ Para $i = 2$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWxqTLsKOyoK"
      },
      "source": [
        "[**Python**] - Evocar a função Backpropagation(2):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w39YvfOWXEX7",
        "outputId": "3659a71c-8120-440b-a53f-75bd2db12c3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Backpropagation(2)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** OUTPUT LAYER *****\n",
            "*** Função de ativação ***\n",
            "[0.609]\n",
            "*** Derivada ***\n",
            "[0.238]\n",
            "*** Erros ***\n",
            "[0.391]\n",
            "*** Delta ***\n",
            "[0.093]\n",
            "\n",
            "\n",
            "***** HIDDEN LAYER *****\n",
            "*** Função de ativação ***\n",
            "[0.63  0.639 0.632]\n",
            "*** Derivada ***\n",
            "[0.233 0.231 0.232]\n",
            "*** Delta ***\n",
            "[0.005 0.007 0.003]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBZuNcOC0yj9"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_XOR_2_2.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZam1meY48hW"
      },
      "source": [
        "##### _HIDDEN LAYER_\n",
        "\\begin{align}\n",
        "\\Delta_{H}^{(2, 1)} &= D_{H}^{(2, 1)}.W_{O}^{(1)}.\\Delta_{O}^{(2)}= (0.233)(0.24)(0.093)= 0.005 \\\\\n",
        "\\Delta_{H}^{(2, 2)} &= D_{H}^{(2, 2)}.W_{O}^{(2)}.\\Delta_{O}^{(2)}= (0.231)(0.318)(0.093)= 0.007 \\\\\n",
        "\\Delta_{H}^{(2, 3)} &= D_{H}^{(2, 3)}.W_{O}^{(3)}.\\Delta_{O}^{(2)}= (0.232)(0.142)(0.093)= 0.003\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAc8YrceY5kY"
      },
      "source": [
        "Na figura acima, temos que $\\Delta_{H}^{(2)}= [\\Delta_{H}^{(2, 1)}, \\Delta_{H}^{(2, 2)}, \\Delta_{H}^{(2, 3)}]= [0.005, 0.007, 0.003]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWrv-aRyHMPh"
      },
      "source": [
        "#### $\\Longrightarrow$ Para $i = 3$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKqR3izrO15N"
      },
      "source": [
        "[**Python**] - Evocar a função Backpropagation(3):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1APffWq2XEYA",
        "outputId": "77137518-68f0-4d89-94d2-5d9492ff2c86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Backpropagation(3)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** OUTPUT LAYER *****\n",
            "*** Função de ativação ***\n",
            "[0.634]\n",
            "*** Derivada ***\n",
            "[0.232]\n",
            "*** Erros ***\n",
            "[-0.634]\n",
            "*** Delta ***\n",
            "[-0.147]\n",
            "\n",
            "\n",
            "***** HIDDEN LAYER *****\n",
            "*** Função de ativação ***\n",
            "[0.766 0.806 0.758]\n",
            "*** Derivada ***\n",
            "[0.179 0.156 0.183]\n",
            "*** Delta ***\n",
            "[-0.006 -0.007 -0.004]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGAXyDhW0ynn"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_XOR_3_2.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdgIs_zP5i1y"
      },
      "source": [
        "##### _HIDDEN LAYER_\n",
        "\\begin{align}\n",
        "\\Delta_{H}^{(3, 1)} &= D_{H}^{(3, 1)}.W_{O}^{(1)}.\\Delta_{O}^{(3)}= (0.179)(0.24)(-0.147)= -0.006 \\\\\n",
        "\\Delta_{H}^{(3, 2)} &= D_{H}^{(3, 2)}.W_{O}^{(2)}.\\Delta_{O}^{(3)}= (0.156)(0.318)(-0.147)= -0.007 \\\\\n",
        "\\Delta_{H}^{(3, 3)} &= D_{H}^{(3, 3)}.W_{O}^{(3)}.\\Delta_{O}^{(3)}= (0.183)(0.142)(-0.147)= -0.004\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ie99-SqZA6z"
      },
      "source": [
        "Na figura acima, temos que $\\Delta_{H}^{(3)}= [\\Delta_{H}^{(3, 1)}, \\Delta_{H}^{(3, 2)}, \\Delta_{H}^{(3, 3)}]= [-0.006, -0.007, -0.004]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sndwYO-VbK1C"
      },
      "source": [
        "A seguir, cálculos usando o NumPy:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycvvhnWIO5s9"
      },
      "source": [
        "[**Python**] - $D_{O}$: Derivada da _Output Layer_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkdw8tUKw5vo",
        "outputId": "09f25489-8d94-42ae-e8f5-d4d171b137b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.587],\n",
              "       [0.616],\n",
              "       [0.609],\n",
              "       [0.634]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poTTrvYEXEYE",
        "outputId": "58ab8262-2b52-4d60-f277-426d0f4ee73f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "D_O = Derivada_Sigmoid(f)\n",
        "D_O"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.242],\n",
              "       [0.237],\n",
              "       [0.238],\n",
              "       [0.232]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkdyDN6BPNZT"
      },
      "source": [
        "[**Python**] - Mostrar os Erros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO9Qi9U0aWTx",
        "outputId": "4638b1e2-ad56-4295-ea55-644b89c26cc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "E"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.587],\n",
              "       [ 0.384],\n",
              "       [ 0.391],\n",
              "       [-0.634]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPPqqxpIPRsT"
      },
      "source": [
        "[**Python**] - $\\Delta_{O}$: Delta da _Output Layer_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fylksvtaT6h",
        "outputId": "84d0b1e5-8635-49f2-dda1-d007f293b524",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Delta_O = D_O*E\n",
        "Delta_O"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.142],\n",
              "       [ 0.091],\n",
              "       [ 0.093],\n",
              "       [-0.147]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9zsXwcWPXn1"
      },
      "source": [
        "[**Python**] - $D_{H}$: Derivada da _Hidden Layer_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCABYAGjaigm",
        "outputId": "e8707982-3906-4076-ada0-b563028326d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "D_H = Derivada_Sigmoid(X2)\n",
        "D_H"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25 , 0.25 , 0.25 ],\n",
              "       [0.225, 0.209, 0.229],\n",
              "       [0.233, 0.231, 0.232],\n",
              "       [0.179, 0.156, 0.183]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLa9L88VPdQu"
      },
      "source": [
        "[**Python**] - $D_{O}$ - Derivada da _Output Layer_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58r5kgNwa9xo",
        "outputId": "b61bd162-dad8-428c-caa6-f59c75bd07ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Delta_H = D_H*W_O.T*Delta_O\n",
        "Delta_H"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.009, -0.011, -0.005],\n",
              "       [ 0.005,  0.006,  0.003],\n",
              "       [ 0.005,  0.007,  0.003],\n",
              "       [-0.006, -0.007, -0.004]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roh5SVtkQrJE"
      },
      "source": [
        "### _Backpropagation_ - Atualizar os pesos da _Output Layer_ $W_{O}= \\begin{bmatrix} W_{O}^{(1)} \\\\ W_{O}^{(2)} \\\\ W_{O}^{(3)} \\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ69tO1IPsBQ"
      },
      "source": [
        "[**Python**] - $(X*\\Delta_{O})= (X2*\\Delta_{O})$ para atualizar $W_{O}^{(1)}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K991veZeXEYL",
        "outputId": "606d9fc5-9ce6-4f93-e79e-93dc5d27214d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X2.T.dot(Delta_O)[0]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.065])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz-0fQAGd7Aw"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Atualiza_W3.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovgjM8l6Np0e"
      },
      "source": [
        "$$(0.5)\\Delta_{O}^{(0)}+(0.658)\\Delta_{O}^{(1)}+(0.630)\\Delta_{O}^{(2)}+(0.766)\\Delta_{O}^{(3)}$$\n",
        "$$(0.5)(-0.142)+(0.658)(0.091)+(0.630)(0.093)+(0.766)(-0.147)= -0.065$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFaNh6NEXEYO"
      },
      "source": [
        "[**Python**] - $(X*\\Delta_{O})= (X2*\\Delta_{O})$ para atualizar $W_{O}^{(2)}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eomk5j12XEYT",
        "outputId": "dfdb5a38-e8f7-43b6-f0c5-9fd5ff0b6b7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X2.T.dot(Delta_O)[1]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.067])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-3gk0erRpSF"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Atualiza_W4.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVCFLfWGPE7W"
      },
      "source": [
        "$$(0.5)\\Delta_{O}^{(0)}+(0.702)\\Delta_{O}^{(1)}+(0.639)\\Delta_{O}^{(2)}+(0.866)\\Delta_{O}^{(3)}$$\n",
        "$$(0.5)(-0.142)+(0.702)(0.091)+(0.639)(0.093)+(0.806)(-0.147)= -0.067$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK92KMHYXEYV"
      },
      "source": [
        "[**Python**] - $(X*\\Delta_{O})= (X2*\\Delta_{O})$ para atualizar $W_{O}^{(3)}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D05BW8CgXEYc",
        "outputId": "9d3ef5e0-1f12-4aaa-f583-f0c9e30dd553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X2.T.dot(Delta_O)[2]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.065])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K754V1CSRtii"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Atualiza_W5.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q51biJ5TPkKX"
      },
      "source": [
        "$$(0.5)\\Delta_{O}^{(0)}+(0.646)\\Delta_{O}^{(1)}+(0.632)\\Delta_{O}^{(2)}+(0.758)\\Delta_{O}^{(3)}$$\n",
        "$$(0.5)(-0.142)+(0.646)(0.091)+(0.632)(0.093)+(0.758)(-0.147)= -0.067$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaEVJAXGV3Xd"
      },
      "source": [
        "###### Implementação com NumPy\n",
        "\n",
        "* Fórmula para atualização dos pesos $W_{O}$:\n",
        "\n",
        "$$W_{n+1}= W_{n}*M+\\alpha \\frac{\\partial L}{\\partial W_{n}}= W_{n}*M+\\alpha*(X*\\Delta)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7dGpwzfRN2M"
      },
      "source": [
        "[**Python**] - Calcular/atualizar os pesos $W_{O}$ através da expressão de $W_{n+1}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er3DprzjXEYg",
        "outputId": "1e923d78-05f3-40ca-b6a9-ebb60c4dd020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "M = 1\n",
        "alpha = 0.1\n",
        "\n",
        "W_O_New = W_O*M+alpha*(X2.T.dot(Delta_O))\n",
        "W_O_New"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.234],\n",
              "       [0.312],\n",
              "       [0.136]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-2weyIriNqN"
      },
      "source": [
        "Abaixo, os pesos atualizados de $W_{O}$ (antes e depois)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLkZfXbmi9c6"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/New_W3_W4_W5.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4fHsSY3AlFi"
      },
      "source": [
        "### _Backpropagation_ - Ajuste dos pesos $W_{H}= \\begin{bmatrix} W_{H}^{(1, 1)} & W_{H}^{(1, 2)} & W_{H}^{(1, 3)} \\\\ W_{H}^{(2, 1)} & W_{H}^{(2, 2)} & W_{H}^{(2, 3)} \\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCED4NKj1_FX"
      },
      "source": [
        "#### Ajuste dos pesos $W_{H}^{(1, 1)}, W_{H}^{(1, 2)}, W_{H}^{(1, 3)}$\n",
        "\n",
        "Para ajustar os pesos $W_{H}^{(1, 1)}, W_{H}^{(1, 2)}, W_{H}^{(1, 3)}$, precisamos dos valores de $\\Delta_{H}$, calculado anteriormente:\n",
        "\n",
        "* $\\Delta_{H}^{(0)}= [\\Delta_{H}^{(0, 1)}, \\Delta_{H}^{(0, 2)}, \\Delta_{H}^{(0, 3)}]= [-0.009, -0.011, -0.005]$;\n",
        "* $\\Delta_{H}^{(1)}= [\\Delta_{H}^{(((1, 1)}, \\Delta_{H}^{(1, 2)}, \\Delta_{H}^{(1, 3)}]= [0.005, 0.006, 0.003]$;\n",
        "* $\\Delta_{H}^{(2)}= [\\Delta_{H}^{(2, 1)}, \\Delta_{H}^{(2, 2)}, \\Delta_{H}^{(2, 3)}]= [0.005, 0.007, 0.003]$;\n",
        "* $\\Delta_{H}^{(3)}= [\\Delta_{H}^{(3, 1)}, \\Delta_{H}^{(3, 2)}, \\Delta_{H}^{(3, 3)}]= [-0.006, -0.007, -0.004]$.\n",
        "\n",
        "Veja abaixo no NumPy:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNHXkTXLRmzu"
      },
      "source": [
        "[**Python**] - Mostrar $\\Delta_{H}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYxrEVC7XEYn",
        "outputId": "df32dfbb-91fb-4554-ea87-ff1db440e2cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Delta_H"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.009, -0.011, -0.005],\n",
              "       [ 0.005,  0.006,  0.003],\n",
              "       [ 0.005,  0.007,  0.003],\n",
              "       [-0.006, -0.007, -0.004]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XihSawh-1iKI"
      },
      "source": [
        "##### Resumo dos valores de $(X*\\Delta_{H})$ calculados manualmente:\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Resumo_Delta_Hidden_Layer.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNXt5DAhBiVC"
      },
      "source": [
        "[**Python**] - $(X*\\Delta_{H})$ para atualizar $W_{H}^{(1, 1)}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06duXU28XEYy",
        "outputId": "e3546504-75ed-43ab-9648-f50f6dcc417a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.T.dot(Delta_H)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.001, -0.   , -0.001],\n",
              "       [-0.001, -0.001, -0.001]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTCJ_5O7SeU9"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/W11.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeJOgJd5P5BS"
      },
      "source": [
        "$$(0)\\Delta_{H}^{(0, 1)}+(0)\\Delta_{H}^{(1, 1)}+(1)\\Delta_{H}^{(2, 1)}+(1)\\Delta_{O}^{(3, 1)}$$\n",
        "$$(0)(-0.009)+(0)(0.005)+(1)(0.005)+(1)(-0.006)= -0.001$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mbs0ZNTCRKL"
      },
      "source": [
        "[**Python**] - $(X*\\Delta_{H})$ para atualizar $W_{H}^{(1, 2)}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF1iFyRWXEY9",
        "outputId": "db60329b-a53c-49f3-e6b3-777da1e45b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.T.dot(Delta_H)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.001, -0.   , -0.001],\n",
              "       [-0.001, -0.001, -0.001]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWkm7eyLSm6I"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/W12.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9LQgX05Qj8M"
      },
      "source": [
        "$$(0)\\Delta_{H}^{(0, 2)}+(0)\\Delta_{H}^{(1, 2)}+(1)\\Delta_{H}^{(2, 2)}+(1)\\Delta_{H}^{(3, 2)}$$\n",
        "$$(0)(-0.011)+(0)(0.006)+(1)(0.007)+(1)(-0.007)= 0$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaVbGCATCd7B"
      },
      "source": [
        "[**Python**] - $(X*\\Delta_{H})$ para atualizar $W_{H}^{(1, 3)}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UNZFSC5XEZE",
        "outputId": "3364213c-42cc-4362-bdbf-8425d1ddb087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.T.dot(Delta_H)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.001, -0.   , -0.001],\n",
              "       [-0.001, -0.001, -0.001]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JHAiH5GSqr0"
      },
      "source": [
        "\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/W13.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDrurArKQ5I_"
      },
      "source": [
        "$$(0)\\Delta_{H}^{(0, 3)}+(0)\\Delta_{H}^{(1, 3)}+(1)\\Delta_{H}^{(2, 3)}+(1)\\Delta_{H}^{(3, 3)}$$\n",
        "$$(0)(-0.005)+(0)(0.003)+(1)(0.003)+(1)(-0.004)= -0.001$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWwWUfiXXlom"
      },
      "source": [
        "#### Ajuste dos pesos $W_{H}^{(2, 1)}, W_{H}^{(2, 2)}$ e $W_{H}^{(2, 3)}$\n",
        "\n",
        "Para ajustar os pesos $W_{H}^{(1, 1)}, W_{H}^{(1, 2)}, W_{H}^{(1, 3)}$, precisamos dos valores de $\\Delta_{H}$, calculado anteriormente:\n",
        "\n",
        "* $\\Delta_{H}^{(0)}= [\\Delta_{H}^{(0, 1)}, \\Delta_{H}^{(0, 2)}, \\Delta_{H}^{(0, 3)}]= [-0.009, -0.011, -0.005]$;\n",
        "* $\\Delta_{H}^{(1)}= [\\Delta_{H}^{(1, 1)}, \\Delta_{H}^{(1, 2)}, \\Delta_{H}^{(1, 3)}]= [0.005, 0.006, 0.003]$;\n",
        "* $\\Delta_{H}^{(2)}= [\\Delta_{H}^{(2, 1)}, \\Delta_{H}^{(2, 2)}, \\Delta_{H}^{(2, 3)}]= [0.005, 0.007, 0.003]$;\n",
        "* $\\Delta_{H}^{(3)}= [\\Delta_{H}^{(3, 1)}, \\Delta_{H}^{(3, 2)}, \\Delta_{H}^{(3, 3)}]= [-0.006, -0.007, -0.004]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzeytNngSI08"
      },
      "source": [
        "[**Python**] - Mostra $\\Delta_{H}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfiS9bFqXEZH",
        "outputId": "a00abd85-cf78-4d07-bb94-0a9bbfe93c4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Delta_H"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.009, -0.011, -0.005],\n",
              "       [ 0.005,  0.006,  0.003],\n",
              "       [ 0.005,  0.007,  0.003],\n",
              "       [-0.006, -0.007, -0.004]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSDbe8o8k9yi"
      },
      "source": [
        "##### Resumo de $(X*\\Delta_{H})$ calculados manualmente:\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Resumo_Delta_Hidden_Layer.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6e2ZoMmDLFN"
      },
      "source": [
        "[**Python**] - $(X*\\Delta_{H})$ para atualizar $W_{H}^{(2, 1)}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH4yHqoYXEZP",
        "outputId": "77aaae6b-f681-42e3-92e1-a2981891e259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.T.dot(Delta_H)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.001, -0.   , -0.001],\n",
              "       [-0.001, -0.001, -0.001]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zORHdsEiXwSw"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/W21.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P11cTnsCRpwj"
      },
      "source": [
        "$$(0)\\Delta_{H}^{(0, 1)}+(1)\\Delta_{H}^{(1, 1)}+(0)\\Delta_{H}^{(2, 1)}+(1)\\Delta_{H}^{(3, 1)}$$\n",
        "$$(0)(-0.009)+(1)(0.005)+(0)(0.005)+(1)(-0.006)= -0.001$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_LMmSEVDXY7"
      },
      "source": [
        "[**Python**] - $(X*\\Delta_{H})$ para atualizar $W_{H}^{(2, 2)}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE4DH6P_XEZZ",
        "outputId": "73ba4375-f3f4-47cd-c356-c2a68d6cffcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.T.dot(Delta_H)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.001, -0.   , -0.001],\n",
              "       [-0.001, -0.001, -0.001]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz7bhUuDX6Me"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/W22.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLrwPoE7SGYu"
      },
      "source": [
        "$$(0)\\Delta_{H}^{(0, 2)}+(1)\\Delta_{H}^{(1, 2)}+(0)\\Delta_{H}^{(2, 2)}+(1)\\Delta_{H}^{(3, 2)}$$\n",
        "$$(0)(-0.011)+(1)(0.006)+(0)(0.007)+(1)(-0.007)= -0.001$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzbUzC8FDhuo"
      },
      "source": [
        "[**Python**] - $(X*\\Delta_{H})$ para atualizar $W_{H}^{(2, 3)}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-epl7I3XEZf",
        "outputId": "2392cf92-186c-4053-ba06-93c87639cd7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.T.dot(Delta_H)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.001, -0.   , -0.001],\n",
              "       [-0.001, -0.001, -0.001]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gT8_uDQX-NT"
      },
      "source": [
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/W23.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLz57OEPSWjl"
      },
      "source": [
        "$$(0)\\Delta_{H}^{(0, 3)}+(1)\\Delta_{H}^{(1, 3)}+(0)\\Delta_{H}^{(2, 3)}+(1)\\Delta_{H}^{(3, 3)}$$\n",
        "$$(0)(-0.005)+(1)(0.003)+(0)(0.003)+(1)(-0.004)= -0.001$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9gWkKWIIOL"
      },
      "source": [
        "##### Implementação com NumPy\n",
        "\n",
        "Usando:\n",
        "* M = 1;\n",
        "* $\\alpha = 0.1$;\n",
        "* Fórmula: $W_{n+1} = (W_{n}*M)+\\alpha*(X*\\Delta_{H})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C3NCWcuShqN"
      },
      "source": [
        "[**Python**] - Calcular/atualizar os pesos $W_{H}$ usando a expressão $W_{n+1}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys_y-R0BL7Iw",
        "outputId": "301d0529-3c87-4e55-90b8-9e1c6c5eba8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "M = 1\n",
        "alpha = 0.1\n",
        "\n",
        "W_H_New = W_H*M+alpha*(X.T.dot(Delta_H))\n",
        "W_H_New"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.531, 0.57 , 0.542],\n",
              "       [0.655, 0.857, 0.602]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvaIx_PKZEmd"
      },
      "source": [
        "##### Novos Pesos $W_{H}$ e $W_{O}$ da Rede Neural (Antes x Depois)\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Resumo_Antes_Depois.png?raw=true\" alt=\"Drawing\" width= \"800\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuO2t22CffE8"
      },
      "source": [
        "___\n",
        "# **Como as Redes Neurais aprendem?**\n",
        "\n",
        "> Vimos até agora grande parte dos cálculos matemáticos que envolvem o treinamento das Redes Neurais, que envolvem a repetição dos processos _Forward_ e _Backward_:\n",
        "\n",
        "1. _**Forward**_: Consiste na multiplicação de matrizes entre os _arrays_ da _input layer_, pesos $W$ e, na sequência, aplicar as funções de ativação.\n",
        "\n",
        "2. _**Backward**_: Consiste em atualizar os pesos $W_{O}$ e $W_{H}$ para minimizar a _Loss Function_ $L$ usando _Gradient Descent_.\n",
        "\n",
        "* Estes 2 processos foram vistos detalhadamente em aulas anteriores.\n",
        "    * Cálculos matemáticos passo a passo foram mostrados. Portanto, visite nossas aulas anteriores para aprender mais sobre os aspectos teóricos e matemáticos por trás das Redes Neurais.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MathMachado/Materials/master/Forward_Backward.png?raw=true\" alt=\"Drawing\" width= \"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpKyqSuDdbMr"
      },
      "source": [
        "___\n",
        "# **_GRADIENT DESCENT_**\n",
        "\n",
        "_Gradient Descent_ é um algoritmo interativo utilizado para otimizar (neste caso, minimizar) a _Loss Function_ $L$. \n",
        "\n",
        "* Minimizar a _Loss Function_ $L$ significa encontrar os pesos $W_{H}$ e $W_{O}$ que faz com que MSE seja o menor possível, pois quanto menor o MSE, melhor a performance da Rede Neural. \n",
        "\n",
        "* Para atualizar os pesos $W$, vamos usar a expressão a seguir:\n",
        "\n",
        "$$W_{n+1}= W_{n}*M+\\alpha \\frac{\\partial L}{\\partial W_{n}}= W_{n}*M+\\alpha*(X*\\Delta)$$\n",
        "\n",
        "onde:\n",
        "\n",
        "* $L$ é a _Loss Function_ a ser minimizada;\n",
        "* $W_{n}$ são os pesos atuais e que deverão ser atualizados para a próxima iteração;\n",
        "* $\\alpha$ é a taxa de aprendizado (_Learning Rate_ em inglês) e diz respeito à velocidade de aprendizagem da Rede Neural.\n",
        "    * Quanto MENOR o valor de $\\alpha$ $\\Longrightarrow$ mais devagar e demorada será a convergência para o mínimo global;\n",
        "    * Quanto MAIOR o valor de $\\alpha$ $\\Longrightarrow$ mais rápido será a convergência para o mínimo, mas sem a garantia de convergência para o mínimo global.\n",
        "* $M$ é o _Momentum_, que é o artifício para acelerar a otimização (ou minimização) da _Loss Function_ $L$.\n",
        "    * Valores altos $\\Longrightarrow$ Aumenta a velocidade da aprendizagem;\n",
        "    * Valores baixos $\\Longrightarrow$ Mais tempo para aprendizagem, mas com maiores chances de se encontrar a solução ótima, evitando os mínimos locais.\n",
        "* $\\frac{\\partial L}{\\partial W_{n}}$ é a derivada da _Loss Function_ $L$ em relação ao peso $W_{n}$. Como dito anteriormente, é a contribuição do peso $W$ no Erro. Calcular $(X*\\Delta)$ é a parte mais complicada da fórmula e fizemos estes cálculos passo a passo em aulas anteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzuFSOV4eboI"
      },
      "source": [
        "Observe a figura a seguir: O que o _Gradient Descent_ fará é encontrar o mínimo global da _loss function_, tentando ao máximo possível evitar os mínimos locais.\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/Local_and_Global_Minima.png?raw=true\" alt=\"Drawing\" width= \"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-UbiCxUgvHg"
      },
      "source": [
        "A seguir, alguns artigos sobre _Gradient Descent_ caso você queira saber um pouco mais sobre o assunto:\n",
        "\n",
        "* [An introduction to Gradient Descent Algorithm](https://medium.com/@montjoile/an-introduction-to-gradient-descent-algorithm-34cf3cee752b) - Abrir este artigo para mostrar os efeitos da _Learning Rate_ e os tipos de _Gradient Descent_ disponíveis para Machine Learning;\n",
        "* [Machine learning : Gradient Descent](https://medium.com/@arshren/gradient-descent-5a13f385d403);\n",
        "* [The Math and Intuition Behind Gradient Descent](https://medium.com/datadriveninvestor/the-math-and-intuition-behind-gradient-descent-13c45f367a11) - Mostra a matemática por trás do _Gradient Descent_;\n",
        "* [An Introduction to Gradient Descent](https://towardsdatascience.com/an-introduction-to-gradient-descent-c9cca5739307);\n",
        "* [Gradient Descent From Scratch](https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc);\n",
        "* [Gradient Descent Explanation & Implementation](https://towardsdatascience.com/gradient-descent-explanation-implementation-c74005ff7dd1) - Cálculos step-by-step;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "junvtVY4eePi"
      },
      "source": [
        "___\n",
        "# **_LOSS FUNCTION_ $L$**\n",
        "\n",
        "> Como vimos anteriormente, nosso objetivo é minimizar a _Loss function_ através do _Gradient Descent_. Em outras palavras, esse processo de otimização busca, à cada iteração (_epoch_), atualizar os pesos $W$ para reduzir a _Loss Function_. As _Loss Function_ mais comuns são:\n",
        "\n",
        "* **Regressão**: mse ou mae;\n",
        "* **Classificação**: _cross-entropy_ (quando queremos probabilidades de cada observação pertencer à uma determinada classe).\n",
        "    * **Classificação binária**: tf.keras.losses.BinaryCrossentropy()\n",
        ";\n",
        "    * **Classificação multi-classes**: tf.keras.losses.CategoricalCrossentropy()\n",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e4ULmJheePY"
      },
      "source": [
        "___\n",
        "# **MÉTRICAS PARA MEDIR A PERFORMANCE DAS REDES NEURAIS**\n",
        "\n",
        "* As métricas medem a qualidade/performance das Redes Neurais e as principais são:\n",
        "    * **Regressão**: Quanto mais próximo de 0 estiver MAE, MSE ou RMSE, melhor a performance da Rede Neural.\n",
        "        * MAE significa \"_Mean Absolute Error_\".\n",
        "\n",
        "        * MSE - significa \"_Mean Square Error_\", que é a diferença entre os valores reais $y_{i}$ e os valores previstos (ou calculados) $\\hat{Y}_{i}$.\n",
        "\n",
        "        * RMSE - significa \"_Root Mean Square Error_\".\n",
        "        \n",
        "    * **Classificação**: Quanto maior a accuracy, melhor a performance da Rede Neural.\n",
        "        * Accuracy\n",
        "\n",
        "* Expressões Matemáticas:\n",
        "\n",
        "\\begin{align}\n",
        "MSE &= \\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{Y}_{i})^{2}}{n} \\\\\n",
        "RMSE &= \\sqrt{MSE} \\\\\n",
        "MAE &= \\frac{\\sum_{i=1}^{n}|y_{i}-\\hat{Y}_{i}|}{n}\n",
        "\\end{align}\n",
        "\n",
        "Para os alunos que estão com dúvidas sobre qual métrica usar, sugiro a leitura do artigo [MAE and RMSE — Which Metric is Better?](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n4QjH1WeeO8"
      },
      "source": [
        "___\n",
        "# **_DROPOUT_**\n",
        "\n",
        "> _Dropout_ significa ignorar aleatoriamente e temporariamente um percentual $p$ de neurônios durante a fase de treinamento. Ao \"ignorar\", quero dizer que tais neurônios não serão considerados durante os processos _forward_ e _backpropagation_.\n",
        "\n",
        "* _Dropout_ força que a Rede Neural aprenda a partir dos dados, mas usando diferentes e aleatórios neurônios;\n",
        "* Recomenda-se $p = 0.20$. \n",
        "* Ao se usar _Dropout_, recomenda-se mais épocas para treinar a Redes Neurais;\n",
        "\n",
        "* **Vantagens**:\n",
        "    * Evita _overfitting_ - Num \"_fully connected layer_\", neurônios desenvolvem dependência durante a fase de treinamento levando ao _overfitting_. Com _dropout_ é possível reduzir um pouco desta dependência, reduzindo as chances de _overfitting_;\n",
        "\n",
        "![Dropout](https://github.com/MathMachado/Materials/blob/master/Dropout.png?raw=true)\n",
        "\n",
        "Fonte: [Dropout in (Deep) Machine learning](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-_dropout_-in-deep-machine-learning-74334da4bfc5).\n",
        "\n",
        "TEMPLATE: keras.layers.Dropout(rate, noise_shape=None, seed=None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-9Y9562kNNU"
      },
      "source": [
        "___\n",
        "# **Rede Neural multicamada (1 _Hidden Layer_) para o Operador Lógico XOR usando _Tensorflow_/_Keras_**\n",
        "\n",
        "* **Observações**:\n",
        "    * Há vários artigos (no _medium_, por exemplo) a discutir e desenvolver Redes Neurais para o Operador Lógico XOR. Então porque eu decidi produzir esta aula usando o dataframe do Operador Lógico XOR?\n",
        "        * Para explicar didaticamente e passo a passo todos os aspectos matemáticos por trás das Redes Neurais usando um dataframe pequeno e, apesar disso, complexo, pois é um problema linearmente NÃO-separável e, sendo assim, requer uma Rede Neural mais complexa (com pelo menos 1 _Hidden Layer_) para melhorar a acurácia e reduzir a _loss_;\n",
        "        * Para explicar como é fácil desenvolver Redes Neurais usando Tensorflow/Keras;\n",
        "        * Para explicar didaticamente e passo a passo os processos _Forward_ e _Backward_ para treinar Redes Neurais;\n",
        "    * Versão do Tensorflow usada: 2.x;\n",
        "    * Estou a utilizar o Google Colab;\n",
        "    * Nesta aula, não se preocupe demasiadamente com a sintaxe dos comandos. Porque?\n",
        "        * Vamos repetir tudo detalhadamente nas próximas aulas. Portanto, você terá a oportunidade de aprender e praticar muito em breve;\n",
        "        * O objetivo desta aula é simplesmente fazer uma introdução às Redes Neurais, Tensorflow/Keras e mostrar os passos/processos que vamos seguir aqui e no futuro para desenvolver Redes Neurais. Quando você assistir as aulas subsequentes, tudo ficará mais claro.\n",
        "* Todas as aulas do curso de Redes Neurais foram cuidadosamente planejadas e preparadas para trazer conteúdos relevantes para você aprender Redes Neurais no menor tempo possível. Portanto, nesta aula você vai encontrar várias linhas como a linha adiante:\n",
        "\n",
        "[**Python**] - Comando ou _code_ que deve ser executado.\n",
        "\n",
        "Estas linhas são uma espécie de _guide_ para não nos esquecermos de nenhum detalhe da aula. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOQbXbCgZZRL"
      },
      "source": [
        "A seguir, dataframe do Operador Lógico XOR:\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$)|\n",
        "|---|---|---|---|\n",
        "| 0 | 0 | 0 | 0 |\n",
        "| 1 | 0 | 1 | 1 |\n",
        "| 2 | 1 | 0 | 1 |\n",
        "| 3 | 1 | 1 | 0 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KusCpN1S4CtH"
      },
      "source": [
        "Vamos obedecer os _steps_ a seguir para construir nossa Rede Neural:\n",
        "\n",
        "1. Carregar as bibliotecas do Python e Tensorflow;\n",
        "2. Carregar os dados para treinar a Rede Neural;\n",
        "3. Definir a arquitetura da Rede Neural com Tensorflow/Keras;\n",
        "4. Compilar a Rede Neural;\n",
        "5. Ajustar a Rede Neural;\n",
        "6. Avaliar a performance da Rede Neural;\n",
        "7. _Fine tuning_ da Rede Neural;\n",
        "8. Fazer Predições com a Rede Neural;\n",
        "9. Conclusões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq7pF8854cf6"
      },
      "source": [
        "### 1. Carregar as bibliotecas do Python e Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br4REluttJXH"
      },
      "source": [
        "[**Python**] - Importar as bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-1jl_vnP7n3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA_bHIYOrNwy"
      },
      "source": [
        "[**Python**] - Verificar a versão do Tensorflow\n",
        "> Assegurar que está a utilizar a versão 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApSwaqVbQVGx",
        "outputId": "c9130af4-c1a8-4692-aec5-f3d243fb38cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH37RXFLtPpB"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimais = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzdu5btatTom"
      },
      "source": [
        "np.set_printoptions(precision = 3)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YbKhVkd4sm2"
      },
      "source": [
        "### 2. Carregar os dados para treinar a Rede Neural\n",
        "\n",
        "Segue abaixo o dataframe do Operador Lógico XOR:\n",
        "\n",
        "| i | $X_{1}$ | $X_{2}$ | ValorReal ($y_{i}$)|\n",
        "|---|---|---|---|\n",
        "| 0 | 0 | 0 | 0 |\n",
        "| 1 | 0 | 1 | 1 |\n",
        "| 2 | 1 | 0 | 1 |\n",
        "| 3 | 1 | 1 | 0 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uojCCMaWgtI"
      },
      "source": [
        "[**Python**] - Definir as entradas (_inputs_) $X$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTbtpKKdQh9M",
        "outputId": "7e3f9e8a-c58e-45f8-8018-dd30cc37034c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_XOR = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "X_XOR"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [0, 1],\n",
              "       [1, 0],\n",
              "       [1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hc1pYW-WsCp"
      },
      "source": [
        "[**Python**] - Definir os _Outputs_ $Y$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj4zl-JdQ0nR",
        "outputId": "644436e3-9782-46cd-dbff-4ade35017381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_XOR = np.array([[0], [1], [1], [0]])\n",
        "y_XOR"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WF73gTMZtN-"
      },
      "source": [
        "### 3. Conceito importante: _Fully connected layer_\n",
        "\n",
        "> A arquitetura da Rede Neural abaixo é dita _fully connected_, ou seja, os neurônios da camada anterior se conecta com todos os neurônios da camada subsequente. Observe a figura a seguir:\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_Multicamada.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X5eys3mxsl2"
      },
      "source": [
        "#### Arquitetura da Rede Neural\n",
        "\n",
        "> A seguir, a arquitetura da Rede Neural que vamos desenvolver neste exemplo:\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/RNA_Generic1.png?raw=true\" alt=\"Drawing\" width= \"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpmtGEFgWzhk"
      },
      "source": [
        "[**Python**] - Definir a arquitetura, ou seja:\n",
        "* $N_{I}$: Número de neurônios na camada de entrada (_Input Layer_);\n",
        "* $N_{O}$: Número de neurônios na camada de saída (_Output Layer_);\n",
        "* $N_{H}$: Número de neurônios na camada escondida (_Hidden Layer_);\n",
        "* FA: Função de ativação:\n",
        "    * _Hidden Layer_: Há várias opções que podem ser usadas, mas vou tentar resolver este exemplo com a função de ativação _Sigmoid_, que foi a função de ativação que foi a opção escolhida quando explicamos Redes Neurais passo a passo.\n",
        "    * _Output Layer_: Os valores de $y_{i}$ do dataframe são binários. Portanto, nossa opção para função de ativação para a _Output Layer_ é usar a função de ativação _Sigmoid_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id_P910LRRb4"
      },
      "source": [
        "# Número de Neurônios na Input Layer:\n",
        "N_I = 2 # Número de variáveis/colunas da matriz de preditoras\n",
        "\n",
        "# Número de neurônios na Output Layer:\n",
        "N_O = 1\n",
        "\n",
        "# Número de neurônios na Hidden Layer:\n",
        "N_H = 3\n",
        "\n",
        "# Função de Ativação da Hidden Layer:\n",
        "FA_H = tf.keras.activations.sigmoid\n",
        "\n",
        "# Função de Ativação da Output Layer\n",
        "FA_O = tf.keras.activations.sigmoid"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6s9RcjLXqQm"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DizOTqQR6-U"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "tf.random.set_seed(20111974)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdcdcNncYB15"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8KJ0f70HEwN"
      },
      "source": [
        "**Observação**:\n",
        "\n",
        "* A opção kernel_constraint = tf.keras.constraints.UnitNorm() será utilizada para reduzir _overfitting_, conforme sugere o artigo [How to Reduce Overfitting Using Weight Constraints in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/);."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LYbXfEZYNcC",
        "outputId": "9fc9f102-c61f-4041-cf8f-a29494977420",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "RN = Sequential() # nome da Rede Neural\n",
        "RN.add(Dense(units = N_H, \n",
        "             input_dim = N_I, \n",
        "             activation = FA_H, \n",
        "             kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dense(units = N_O, activation = FA_O))\n",
        "\n",
        "# Resumo da arquitetura da Rede Neural:\n",
        "print(RN.summary())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 9         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 13\n",
            "Trainable params: 13\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoA-_8A55jMW"
      },
      "source": [
        "### 4. Compilar a Rede Neural\n",
        "\n",
        "> Adam é um algoritmo de otimização.\n",
        "\n",
        "Para saber mais sobre o algoritmo de otimização 'adam', consulte o artigo [Gentle Introduction to the Adam Optimization Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifkjrCT6Yki6"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdIerBPAUGbY"
      },
      "source": [
        "algoritmo_otimizacao = tf.keras.optimizers.Adam() # Algoritmo de otimização\n",
        "loss_function = tf.keras.losses.MeanSquaredError() # A métrica para cálculo do erro\n",
        "metrica_performance = [tf.keras.metrics.binary_accuracy]\n",
        "\n",
        "RN.compile(optimizer = algoritmo_otimizacao, \n",
        "           loss = loss_function, \n",
        "           metrics = metrica_performance)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVx2w28c5urj"
      },
      "source": [
        "### 5. Ajustar/treinar a Rede Neural\n",
        "\n",
        "* 1 _Epoch_ = 1 iteração da Rede Neural, passando por todo o dataframe de treinamento, sendo que 1 iteração contempla 1 processo _Forward_ e 1 processo _Backward_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV3XwUJ8YvxE"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45inZ8X3U0Ew",
        "outputId": "75fd20b5-6653-4038-d5b4-be58280f74b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hist = RN.fit(X_XOR, y_XOR, epochs = 100)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2531 - binary_accuracy: 0.7500\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2527 - binary_accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2526 - binary_accuracy: 0.5000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2526 - binary_accuracy: 0.5000\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2526 - binary_accuracy: 0.5000\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2526 - binary_accuracy: 0.5000\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2525 - binary_accuracy: 0.5000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2525 - binary_accuracy: 0.5000\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2525 - binary_accuracy: 0.5000\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2525 - binary_accuracy: 0.5000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2525 - binary_accuracy: 0.5000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 957us/step - loss: 0.2524 - binary_accuracy: 0.5000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2524 - binary_accuracy: 0.5000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2524 - binary_accuracy: 0.5000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2524 - binary_accuracy: 0.5000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2524 - binary_accuracy: 0.5000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2523 - binary_accuracy: 0.5000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2523 - binary_accuracy: 0.5000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2523 - binary_accuracy: 0.5000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2523 - binary_accuracy: 0.5000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 990us/step - loss: 0.2523 - binary_accuracy: 0.5000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2523 - binary_accuracy: 0.5000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2522 - binary_accuracy: 0.5000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2522 - binary_accuracy: 0.5000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2522 - binary_accuracy: 0.5000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2522 - binary_accuracy: 0.5000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2522 - binary_accuracy: 0.5000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2522 - binary_accuracy: 0.5000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2522 - binary_accuracy: 0.5000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2522 - binary_accuracy: 0.5000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2521 - binary_accuracy: 0.5000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2521 - binary_accuracy: 0.5000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2521 - binary_accuracy: 0.5000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2521 - binary_accuracy: 0.5000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2521 - binary_accuracy: 0.5000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2521 - binary_accuracy: 0.5000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2521 - binary_accuracy: 0.5000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2521 - binary_accuracy: 0.5000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2521 - binary_accuracy: 0.5000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2521 - binary_accuracy: 0.5000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2520 - binary_accuracy: 0.5000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2520 - binary_accuracy: 0.5000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2520 - binary_accuracy: 0.5000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2520 - binary_accuracy: 0.5000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2520 - binary_accuracy: 0.5000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2520 - binary_accuracy: 0.5000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2520 - binary_accuracy: 0.5000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2520 - binary_accuracy: 0.5000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2520 - binary_accuracy: 0.5000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2520 - binary_accuracy: 0.5000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2519 - binary_accuracy: 0.5000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2519 - binary_accuracy: 0.5000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2519 - binary_accuracy: 0.5000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2519 - binary_accuracy: 0.5000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2519 - binary_accuracy: 0.5000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2519 - binary_accuracy: 0.5000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2519 - binary_accuracy: 0.5000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2519 - binary_accuracy: 0.5000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2519 - binary_accuracy: 0.5000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2519 - binary_accuracy: 0.5000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2519 - binary_accuracy: 0.5000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2518 - binary_accuracy: 0.5000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2518 - binary_accuracy: 0.5000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2518 - binary_accuracy: 0.5000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2518 - binary_accuracy: 0.5000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2518 - binary_accuracy: 0.5000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2518 - binary_accuracy: 0.5000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2518 - binary_accuracy: 0.5000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2518 - binary_accuracy: 0.5000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2518 - binary_accuracy: 0.5000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2518 - binary_accuracy: 0.5000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2517 - binary_accuracy: 0.5000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2517 - binary_accuracy: 0.5000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2517 - binary_accuracy: 0.5000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2517 - binary_accuracy: 0.5000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2517 - binary_accuracy: 0.5000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2517 - binary_accuracy: 0.5000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2517 - binary_accuracy: 0.5000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2517 - binary_accuracy: 0.5000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2517 - binary_accuracy: 0.5000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2517 - binary_accuracy: 0.5000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2517 - binary_accuracy: 0.5000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2516 - binary_accuracy: 0.5000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2516 - binary_accuracy: 0.5000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2516 - binary_accuracy: 0.5000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2516 - binary_accuracy: 0.5000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2516 - binary_accuracy: 0.5000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2516 - binary_accuracy: 0.5000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2516 - binary_accuracy: 0.5000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2516 - binary_accuracy: 0.5000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2516 - binary_accuracy: 0.5000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2516 - binary_accuracy: 0.5000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2515 - binary_accuracy: 0.5000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2515 - binary_accuracy: 0.5000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2515 - binary_accuracy: 0.5000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2515 - binary_accuracy: 0.5000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2515 - binary_accuracy: 0.5000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2515 - binary_accuracy: 0.5000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2515 - binary_accuracy: 0.5000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2515 - binary_accuracy: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1bUiekR5q1E"
      },
      "source": [
        "### 6. Avaliar a performance da Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBp4ctbKY8k7"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4HlrjjjVLjB",
        "outputId": "86254578-6bca-40bf-d8ee-75c8e5504f24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "RN.evaluate(X_XOR, y_XOR)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2515 - binary_accuracy: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.25147074460983276, 0.5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPwANO05VT5m"
      },
      "source": [
        "**Resultado**: O modelo _baseline_ (modelo inicial) apresenta os seguintes resultados:\n",
        "* loss = 0.2515;\n",
        "* accuracy= 50%.\n",
        "\n",
        "* **Comentário**: A Rede Neural apresenta resultados insatisfatórios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD2pw9H754ZZ"
      },
      "source": [
        "### 7. _Fine tuning_ da Rede Neural\n",
        "\n",
        "Antes de falarmos de _fine tuning_, vamos voltar a falar de CRISP-DM:\n",
        "\n",
        "CRISP-DM significa _Cross Industry Standard Process for Data Mining_ ou processos ou fases para desenvolvimento de projetos relacionados à _Data Mining_ e que tem sido muito utilizados pelos Cientistas de Dados para desenvolvimento de modelos predictivos.\n",
        "\n",
        "<img src=\"https://github.com/MathMachado/Materials/blob/master/CRISP-DM.png?raw=true\" alt=\"Drawing\" width= \"600\"/>\n",
        "\n",
        "Fonte: [The steps to a successful machine learning project](https://emba.epfl.ch/2018/04/10/steps-successful-machine-learning-project/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ssJuKF3FNA3"
      },
      "source": [
        "* CRISP-DM:\n",
        "    1. _Business Understanding_ (Entendimento do Negócio)\n",
        "        * Concentra-se no entendimento dos objetivos e requisitos do projeto sob uma perspectiva de negócios e, em seguida, na conversão desse conhecimento em uma definição de problema de mineração de dados e em um plano preliminar.\n",
        "\n",
        "    2. _Data Understanding_ (Entendimento/compreensão dos dados)\n",
        "        * Está relacionado com as atividades de extração de amostras para se familiarizar com os dados, identificar problemas de qualidade, descobrir as primeiras idéias ou detectar subconjuntos interessantes para formar hipóteses de informações ocultas.\n",
        "\n",
        "    3. _Data Preparation_ (Preparação de Dados)\n",
        "\n",
        "        * Abrange todas as atividades para construir o conjunto de dados final que será dividida entre amostra de treinamento e validação do modelo preditivo.\n",
        "\n",
        "    4. _Modeling_ (Modelagem)\n",
        "\n",
        "        * Nesta fase se avalia as possíveis técnicas que podem ser aplicadas.\n",
        "\n",
        "    5. _Evaluation_ (Avaliação do modelo)\n",
        "\n",
        "        * Após a construção do modelo _baseline_ (modelo inicial) e tendo _Loss Function_ pré-definidas, avalia-se ou testa-se a performance dos modelos preditivos (Redes Neurais, no nosso caso) para garantir que o modelo generaliza. De todos os modelos testados nesta fase, devemos selecionar o modelo campeão.\n",
        "\n",
        "    6. _Deployment_ (Implantação)\n",
        "\n",
        "        * Significa implementar o código do modelo em um sistema operacional para pontuar/escorar ou categorizar novos dados à medida que surgem e criar um mecanismo para o uso dessas novas informações na solução do problema comercial original. Importante, a representação de código também deve incluir todas as etapas de preparação de dados que antecederam a modelagem, para que o modelo trate novos dados brutos da mesma maneira que durante o desenvolvimento do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrgiPmD3jw_o"
      },
      "source": [
        "#### Estratégias para melhorar a acurácia da Rede Neural\n",
        "\n",
        "Nossas alternativas são:\n",
        "\n",
        "* a. Aumentar o número de neurônios na _Hidden Layer_;\n",
        "* b. Aumentar o número de _Hidden Layers_;\n",
        "* c. Aumentar o número de _Hidden Layers_ e o número de neurônios;\n",
        "* d. Alterar a função de ativação;\n",
        "* e. Aumentar o número de _epochs_;\n",
        "* f. Alterar o algoritmo de otimização (_optimizer_);\n",
        "\n",
        "Neste exemplo, depois de várias tentativas, obtive sucesso alterando os parâmetros a seguir: \n",
        "* Função de ativação: alterar para tf.keras.activations.relu;\n",
        "* Número de neurônios na camada escondida (_Hidden Layer_): aumentei para 64."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V81AQ9t8IA9D"
      },
      "source": [
        "#### 7.3. Definir a arquitetura da Rede Neural com Tensorflow/Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVhR55OoaWeX"
      },
      "source": [
        "[**Python**] - Definir a arquitetura, ou seja:\n",
        "* $N_{I}$: Número de neurônios na camada de entrada (_Input Layer_);\n",
        "* $N_{O}$: Número de neurônios na camada de saída (_Output Layer_);\n",
        "* $N_{H}$: Número de neurônios na camada escondida (_Hidden Layer_);\n",
        "* FA: Função de ativação:\n",
        "    * _Hidden Layer_: tf.keras.activations.relu;\n",
        "    * _Output Layer_: Os valores de $y_{i}$ do dataframe são binários. Portanto, nossa opção para função de ativação para a _Output Layer_ é usar a função de ativação _Sigmoid_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R26Rf7x_aWeZ"
      },
      "source": [
        "# Número de Neurônios na Input Layer:\n",
        "N_I = 2 # NÃO FOI ALTERADA!\n",
        "\n",
        "# Número de neurônios na Output Layer:\n",
        "N_O = 1 # NÃO FOI ALTERADA!\n",
        "\n",
        "# VARIÁVEIS ALTERADAS:\n",
        "# Número de neurônios na Hidden Layer:\n",
        "N_H = 64\n",
        "\n",
        "# Função de Ativação da Hidden Layer:\n",
        "FA_H = tf.keras.activations.relu # ALTERADA!\n",
        "\n",
        "# Função de Ativação da Output Layer\n",
        "FA_O = tf.keras.activations.sigmoid # NÃO FOI ALTERADA!"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtQXjYnvIdJR"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSCCZ6BcIdJZ"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "tf.random.set_seed(20111974)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwjdTXWNawSz"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWpJNRQjIRA4"
      },
      "source": [
        "**Observações**: \n",
        "\n",
        "Para evitar problemas relacionados ao _overfitting_ e _Vanishing or Exploding Gradients in Deep Neural Nets_, os artigos abaixo sugerem as seguintes opções para inicialização dos pesos $W$:\n",
        "\n",
        "* [How to Reduce Overfitting Using Weight Constraints in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/) sugere:\n",
        "    * kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Deep Learning Best Practices (1) — Weight Initialization](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.he_normal() para activation = 'tf.nn.relu' ou 'tf.nn.leaky_relu' e kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Vanishing/ Exploding Gradients in Deep Neural Nets and solving them](https://medium.com/swlh/vanishing-exploding-gradients-in-deep-neural-nets-and-solving-them-9d6070f28b29) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotUniform();\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotNormal()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khod_vL5awS2",
        "outputId": "7dc5b3f4-925e-4659-af89-403efbc94506",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "RN = Sequential()\n",
        "RN.add(Dense(units = N_H, input_dim = N_I, activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dense(units = N_O, activation = FA_O))\n",
        "\n",
        "print(RN.summary())"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 64)                192       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 257\n",
            "Trainable params: 257\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V5EygkgIRA8"
      },
      "source": [
        "#### 7.4. Compilar a Rede Neural\n",
        "\n",
        "> Adam é um algoritmo de otimização.\n",
        "\n",
        "Para saber mais sobre 'adam', consulte o artigo [Gentle Introduction to the Adam Optimization Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEgVJAInbFW0"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRhAlexLW29o"
      },
      "source": [
        "#algoritmo_otimizacao = tf.keras.optimizers.Adam()\n",
        "algoritmo_otimizacao = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name= 'Adam')\n",
        "\n",
        "loss_function = tf.keras.losses.MeanSquaredError()\n",
        "metrica_performance = [tf.keras.metrics.binary_accuracy]\n",
        "\n",
        "RN.compile(optimizer = algoritmo_otimizacao, loss = loss_function, metrics = metrica_performance)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxpOHzSKIRA-"
      },
      "source": [
        "#### 7.5. Ajustar a Rede Neural\n",
        "\n",
        "1 _Epoch_ = 1 iteração da Rede Neural, passando por todo o dataframe de treinamento, sendo que 1 iteração contempla 1 processo _Forward_ e 1 processo _Backward_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-1iqXLabR4O"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqyeqpq5XGAm",
        "outputId": "c874b32f-b745-4180-8d8c-f99c516748ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "RN.fit(X_XOR, y_XOR, epochs = 100)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2581 - binary_accuracy: 0.5000\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2419 - binary_accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2300 - binary_accuracy: 0.5000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2178 - binary_accuracy: 0.7500\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2065 - binary_accuracy: 0.7500\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1959 - binary_accuracy: 0.7500\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1855 - binary_accuracy: 0.7500\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1754 - binary_accuracy: 0.7500\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.1657 - binary_accuracy: 0.7500\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 982us/step - loss: 0.1564 - binary_accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 979us/step - loss: 0.1477 - binary_accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 917us/step - loss: 0.1399 - binary_accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1324 - binary_accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1254 - binary_accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1188 - binary_accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1126 - binary_accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1067 - binary_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1013 - binary_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0966 - binary_accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0923 - binary_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0881 - binary_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0843 - binary_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0806 - binary_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0771 - binary_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0739 - binary_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0709 - binary_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0681 - binary_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0653 - binary_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0628 - binary_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0605 - binary_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0583 - binary_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0560 - binary_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0539 - binary_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0519 - binary_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0500 - binary_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0481 - binary_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0463 - binary_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0446 - binary_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0428 - binary_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0412 - binary_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0396 - binary_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0380 - binary_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0364 - binary_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0349 - binary_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0335 - binary_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0320 - binary_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0306 - binary_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0292 - binary_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0279 - binary_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0266 - binary_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0254 - binary_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0242 - binary_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0230 - binary_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0219 - binary_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0208 - binary_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - binary_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0188 - binary_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0178 - binary_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0170 - binary_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0161 - binary_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0153 - binary_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0145 - binary_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0138 - binary_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0131 - binary_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0124 - binary_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0118 - binary_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0112 - binary_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0107 - binary_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0102 - binary_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0097 - binary_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0093 - binary_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0089 - binary_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0085 - binary_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0081 - binary_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0078 - binary_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0075 - binary_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0072 - binary_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0069 - binary_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0067 - binary_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0064 - binary_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0062 - binary_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0060 - binary_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0058 - binary_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0056 - binary_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0054 - binary_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0052 - binary_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0051 - binary_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0049 - binary_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0048 - binary_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0046 - binary_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0045 - binary_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0044 - binary_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0042 - binary_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0041 - binary_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0040 - binary_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0039 - binary_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0038 - binary_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0037 - binary_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0036 - binary_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0035 - binary_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff36153c9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C25ZV-x4IRBB"
      },
      "source": [
        "#### 7.6. Avaliar a performance da Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCd2S65ubg_M"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8-Vr9lXXav4",
        "outputId": "4f29e704-6544-4cee-afed-979655a37869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "RN.evaluate(X_XOR, y_XOR)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0034 - binary_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0034456099383533, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IqEhL-2Xj-t"
      },
      "source": [
        "**Resultado**: O modelo após o _fine tuning_ apresenta os seguintes resultados:\n",
        "* loss = 0.1502;\n",
        "* accuracy= 100%.\n",
        "\n",
        "* **Comentário**: A Rede Neural apresenta resultados satisfatórios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZjDavkO58Pu"
      },
      "source": [
        "### 8. Fazer Predições com a Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV4HkNDcbmJ2"
      },
      "source": [
        "[**Python**] - Comando RN.predict_classes(X_treinamento):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aum69OJENO6V",
        "outputId": "6d7bcd1f-146d-4fff-80b7-08e5f2615171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred = RN.predict_classes(X_XOR)\n",
        "y_pred"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-88-88953adbb8cc>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNogASabEhz8",
        "outputId": "83254f24-f308-4f00-e094-ccf0dd8987be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_XOR"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xaBkwD15-1d"
      },
      "source": [
        "### 9. Conclusões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UULwoI-9yPIs"
      },
      "source": [
        "A Rede Neural final, após a fase de _fine tuning_ apresenta os resultados mostrados na sessão 7.6. Diante destes resultados, sugerimos avançarmos para a fase de _deployment_ da Rede Neural, conforme sugere o CRISP-DM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFK4SeM5TLOb"
      },
      "source": [
        "### **Exercício**\n",
        "\n",
        "1. Experimente usar outras funções de ativação para a _Hidden Layer_, registre e reporte seus resultados. Para saber mais sobre quais funções de ativação podem ser usadas, consulte [Module: tf.keras.activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations);\n",
        "\n",
        "2. Experimente usar outros algoritmos de otimização para treinar a Rede Neural, registre e reporte seus resultados. Para saber quais algoritmos podem ser usados, consulte [Module: tf.keras.optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).\n",
        "\n",
        "3. Neste exemplo, usamos o algoritmo de otimização 'adam'. Consulte a documentação sobre o 'adam' no Tensorflow/Keras e você verá que a sintaxe do algoritmo é:\n",
        "\n",
        "```\n",
        "tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
        "    name= 'Adam', **kwargs\n",
        ")\n",
        "```\n",
        "\n",
        "Refaça o treinamento da Rede Neural alterando os valores da _Learning Rate_ e reporte seus resultados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyyiNwm6eeP4"
      },
      "source": [
        "___\n",
        "# **_ACTIVATION FUNCTION_**\n",
        "\n",
        "> As funções de ativação são uma importante parte das Redes Neurais, pois permitem às Redes Neurais a lidar com a não-linearidade existente na maioria dos problemas reais.\n",
        "\n",
        "As funções de ativação (_Activation Function_ em inglês) mais usadas são:\n",
        "* _Sigmoid_;\n",
        "* ReLU (_Rectified Linear Unit_);\n",
        "* Leaky ReLU;\n",
        "* _Generalized_ ReLU;\n",
        "* Tanh;\n",
        "* _Swish_.\n",
        "\n",
        "Os artigos a seguir discutem estas principais funções de ativação:\n",
        "* [Classical Neural Net: Why/Which Activations Functions?](https://towardsdatascience.com/classical-neural-net-why-which-activations-functions-401159ba01c4);\n",
        "* [Intermediate Topics in Neural Networks](https://towardsdatascience.com/comprehensive-introduction-to-neural-network-architecture-c08c6d8e5d98);\n",
        "* [Comparison of Activation Functions for Deep Neural Networks](https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7jF5SToOKYC"
      },
      "source": [
        "### Funções de ativação para _Hidden Layers_:\n",
        "\n",
        "Há várias funções de ativação que podem ser utilizadas na _Hidden Layer_. As principais são:\n",
        "\n",
        "* ReLU\n",
        "    * evita e corrige o problema conhecido como _vanishing gradient problem_, que é justamente o principal ponto fraco das funções de ativação _sigmoid_ e _tanh_. Este problema acontece porque algumas derivadas são zero para metade dos valores da entrada $X = [X_{1}, X_{2}, ..., X_{n}]$, o que pode levar ao que se chama de \"neurônios mortos\";\n",
        "    * Quase todos os modelos de _Deep Learning_ hoje usam ReLU que **deve ser usada somente para _Hidden Layers_ das Redes Neurais**. \n",
        "* Leaky ReLU\n",
        "    * Alternativa melhor que ReLU;\n",
        "* _Swish_\n",
        "    * esta é outra alternativa melhor que ReLU, proposta pelo Google em 2017;\n",
        "    * alguns artigos apontam melhoria dos resultados das Redes Neurais com _Swish_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dYvHeYbN_c4"
      },
      "source": [
        "### Funções de ativação para _Output Layers_:\n",
        "\n",
        "A função de ativação da _Output Layer_ depende do problema:\n",
        "\n",
        "* _Sigmoid_ para problemas de classificação binária (2 classes).\n",
        "    * Exemplo: Dataframe: Titanic, pois queremos estimar se o passageiro morreu ou sobreviveu;\n",
        "* _Softmax_ para problemas de classificação multi-classes (> 2 classes).\n",
        "    * Exemplo: Dataframe: Iris, pois queremos estimar a espécie das flores, que são versicolor, virginica e setosa;\n",
        "* _Linear_ para problemas de regressão. \n",
        "    * Exemplo: Dataframe: Boston Housing Prediction, pois queremos estimar o preço das casas em Boston, que é uma variável contínua.\n",
        "\n",
        "\n",
        "O artigo [Comparison of Activation Functions for Deep Neural Networks](https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a) compara e discute as principais funções de ativação de forma pormenorizada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clQq59rPIkvB"
      },
      "source": [
        "___\n",
        "# **EXEMPLO 1: Rede Neural para identificar o sexo a partir de peso e altura**\n",
        "\n",
        "> O dataframe a seguir contem 10.000 medidas de altura (_height_) e peso (_weight_), sendo 5.000 medidas para o sexo masculino (_males_) e 5.000 para o sexo feminino (_females_).\n",
        "\n",
        "**Objetivo**: Estimar gênero (sexo) (_Gender_, em inglês) em função das variáveis _Height_ e _Weight_.\n",
        "\n",
        "Fonte do dataframe: Kaggle (weight-height.csv).\n",
        "\n",
        "Nesta aplicação, vamos seguir os passos adiante:\n",
        "\n",
        "1. Carregar os dados;\n",
        "2. Pré-processamento e transformação dos dados;\n",
        "3. Definir as amostras de treinamento e validação;\n",
        "4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_;\n",
        "5. Compilar a Rede Neural;\n",
        "6. Ajustar a Rede Neural;\n",
        "7. Avaliar a performance da Rede Neural;\n",
        "8. _Fine tuning_ da Rede Neural;\n",
        "9. Fazer Predições com a Rede Neural;\n",
        "10. Conclusões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh5p2GcvLQQX"
      },
      "source": [
        "### 0. Carregar as principais bibliotecas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhjAdXgab99r"
      },
      "source": [
        "[**Python**] - Importar as bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kChuTlPddNZv"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZX00UN5cjvM"
      },
      "source": [
        "[**Python**] - Verificar a versão do Tensorflow\n",
        "> Assegurar que está a utilizar a versão 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THWNIk_FCe_g",
        "outputId": "400b7f1e-3b53-4b2e-c4ad-427981e4dd54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZgQAKqLcLX3"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzKor02BCe_d"
      },
      "source": [
        "np.set_printoptions(precision= 3)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5V4KopjLWOL"
      },
      "source": [
        "### 1. Carregar os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_cwAUW3tseE"
      },
      "source": [
        "[**Python**] - Carregar os dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bs87IWPtwtm"
      },
      "source": [
        "# Leitura do dataframe:\n",
        "df_sexo = pd.read_csv('https://raw.githubusercontent.com/MathMachado/DataFrames/master/weight-height.csv')"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBUeMtV7tzw6"
      },
      "source": [
        "[**Python**] - Mostrar as primeiras 5 linhas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcH-y4amt3gs",
        "outputId": "ab0dfebf-a441-47b7-c80d-a2e906fde226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "df_sexo.head()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>73.847017</td>\n",
              "      <td>241.893563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>68.781904</td>\n",
              "      <td>162.310473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>74.110105</td>\n",
              "      <td>212.740856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>71.730978</td>\n",
              "      <td>220.042470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>69.881796</td>\n",
              "      <td>206.349801</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Gender     Height      Weight\n",
              "0   Male  73.847017  241.893563\n",
              "1   Male  68.781904  162.310473\n",
              "2   Male  74.110105  212.740856\n",
              "3   Male  71.730978  220.042470\n",
              "4   Male  69.881796  206.349801"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSa161sPLcAw"
      },
      "source": [
        "### Pré-processamento e transformação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL2-6wpCuARF"
      },
      "source": [
        "[**Python**] - Construir coluna 'sexo' da seguinte forma:\n",
        "* Se Gender= 'Male' ==> sexo= 1;\n",
        "* Se Gender= 'Female' ==> sexo= 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccImSqCqDKre"
      },
      "source": [
        "def define_label(row):\n",
        "    if row['Gender'] == 'Male':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDYamauZCq77",
        "outputId": "c915e499-0d1b-4e63-8541-ccaf83fc4770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "df_sexo['sexo'] = df_sexo.apply(lambda row: define_label(row), axis = 1)\n",
        "df_sexo.head()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "      <th>sexo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>73.847017</td>\n",
              "      <td>241.893563</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>68.781904</td>\n",
              "      <td>162.310473</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>74.110105</td>\n",
              "      <td>212.740856</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>71.730978</td>\n",
              "      <td>220.042470</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>69.881796</td>\n",
              "      <td>206.349801</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Gender     Height      Weight  sexo\n",
              "0   Male  73.847017  241.893563     1\n",
              "1   Male  68.781904  162.310473     1\n",
              "2   Male  74.110105  212.740856     1\n",
              "3   Male  71.730978  220.042470     1\n",
              "4   Male  69.881796  206.349801     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqkOrJnNuZjg"
      },
      "source": [
        "[**Python**] - Renomear ou reescrever os nomes das colunas do dataframe em letras minúsculas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dahUMI6DsBz",
        "outputId": "b7bc9eb0-0b03-42e3-86a2-06bf18fe5d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "df_sexo = df_sexo.drop(columns= 'Gender', axis= 1)\n",
        "df_sexo = df_sexo.rename({'Height': 'altura', 'Weight': 'peso'}, axis= 1)\n",
        "df_sexo.head()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>altura</th>\n",
              "      <th>peso</th>\n",
              "      <th>sexo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>73.847017</td>\n",
              "      <td>241.893563</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>68.781904</td>\n",
              "      <td>162.310473</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>74.110105</td>\n",
              "      <td>212.740856</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>71.730978</td>\n",
              "      <td>220.042470</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69.881796</td>\n",
              "      <td>206.349801</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      altura        peso  sexo\n",
              "0  73.847017  241.893563     1\n",
              "1  68.781904  162.310473     1\n",
              "2  74.110105  212.740856     1\n",
              "3  71.730978  220.042470     1\n",
              "4  69.881796  206.349801     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTISVuZ4ukQO"
      },
      "source": [
        "[**Python**] - Definir os arrays X_sexo e y_sexo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMTIn6Zf5LlU"
      },
      "source": [
        "X_sexo = df_sexo.copy()\n",
        "X_sexo = X_sexo.drop(columns= ['sexo'])\n",
        "y_sexo = df_sexo['sexo'].values"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSThKwhj4LsC",
        "outputId": "cf3baf52-ea98-4f57-e4a8-4cf5fc2a5dba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_sexo"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiO_F95jc1_s"
      },
      "source": [
        "[**Python**] - Normalizar os dados - StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4myPAnSzE7-l",
        "outputId": "42409276-9aa8-4312-b743-60a7f27a3761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SS = StandardScaler()\n",
        "\n",
        "X_sexo= SS.fit_transform(X_sexo)\n",
        "X_sexo"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.944,  2.506],\n",
              "       [ 0.628,  0.027],\n",
              "       [ 2.012,  1.598],\n",
              "       ...,\n",
              "       [-0.65 , -1.027],\n",
              "       [ 0.693,  0.075],\n",
              "       [-1.15 , -1.489]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJaJWuUqJCha"
      },
      "source": [
        "### 3. Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoO2iEimu4SQ"
      },
      "source": [
        "[**Python**] - Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTCdm-F9JBGA",
        "outputId": "e7581d4d-62f0-48b6-c356-6192897f70a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_treinamento, X_teste, y_treinamento, y_teste= train_test_split(X_sexo, y_sexo, test_size = 0.1, random_state = 20111974)\n",
        "print(f'X: Treinamento=  {X_treinamento.shape}; X: Teste=  {X_teste.shape}')"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: Treinamento=  (9000, 2); X: Teste=  (1000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th9CsQpB8VDK",
        "outputId": "3247b34d-1fa6-4f85-bd21-8055d84ae10b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f'Y: Treinamento =  {y_treinamento.shape}; Y: Teste = {y_teste.shape}')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y: Treinamento =  (9000,); Y: Teste = (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bL-vXiULupD"
      },
      "source": [
        "### 4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxETX6dTfyU5"
      },
      "source": [
        "[**Python**] - Definir a arquitetura, ou seja:\n",
        "* $N_{I}$: Número de neurônios na camada de entrada (_Input Layer_);\n",
        "* $N_{O}$: Número de neurônios na camada de saída (_Output Layer_);\n",
        "* $N_{H}$: Número de neurônios na camada escondida (_Hidden Layer_);\n",
        "* FA: Função de ativação;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_MdsLicfyU6"
      },
      "source": [
        "# Número de Neurônios na Input Layer:\n",
        "N_I = 2\n",
        "\n",
        "# Número de neurônios na Output Layer:\n",
        "N_O = 1\n",
        "\n",
        "# Número de neurônios na Hidden Layer:\n",
        "N_H = 64\n",
        "\n",
        "# Função de Ativação da Hidden Layer:\n",
        "FA_H = tf.keras.activations.swish\n",
        "\n",
        "# Função de Ativação da Output Layer:\n",
        "FA_O = tf.keras.activations.swish"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUMmDuPCcYyB"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-echOBmceVy"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "tf.random.set_seed(20111974)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZceRRdinEM2"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXQsSYq2DBfI"
      },
      "source": [
        "* 1 camada _dropout_ com $p= 0.1$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRFR5Kr_nDtD",
        "outputId": "a80f3e1d-6db0-4881-d5d1-2d37bc5cba70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "RN = Sequential()\n",
        "RN.add(Dense(units = N_H, input_dim = N_I, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "#RN.add(Dense(units = N_H2, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "#RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_O, activation = FA_O))\n",
        "\n",
        "# Resumo da arquitetura da Rede Neural\n",
        "print(RN.summary())"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 64)                192       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 257\n",
            "Trainable params: 257\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JBZf4ypGO8o"
      },
      "source": [
        "### 5. Compilar a Rede Neural\n",
        "\n",
        "Este é um problema de classificação binária (_Male_ ou _Female_). Portanto, temos:\n",
        "* optimizer = tf.keras.optimizers.Adam();\n",
        "* loss =  tf.keras.losses.MeanSquaredError() ou loss = tf.keras.losses.BinaryCrossentropy(). Particularmente, eu gosto de usar loss =  tf.keras.losses.MeanSquaredError() porque o resultado é mais intuitivo;\n",
        "* metrics = tf.keras.metrics.binary_accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USmAuw6f00wL"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7KEi1_e6SSF"
      },
      "source": [
        "algoritmo_otimizacao = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.MeanSquaredError()\n",
        "metrica_performance = tf.keras.metrics.binary_accuracy\n",
        "\n",
        "RN.compile(optimizer = algoritmo_otimizacao, loss = loss_function, metrics = metrica_performance)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc90EeV_GojX"
      },
      "source": [
        "### 6. Ajustar a Rede Neural\n",
        "\n",
        "Obs.: A opção callbacks abaixo implementa o conceito de _early stopping_. Esta opção vai parar o processo de treinamento da Rede Neural antes de atingirmos o númerco de _epochs_ quando o modelo pára de melhorar, medido pela métrica val_loss. O parâmetro _patience_= k significa que o processo de otimização vai parar se tivermos k _epochs_ consecutivas sem observarmos melhoria da performance da Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCCTtUh_vEFP"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB91J6nrF0db",
        "outputId": "0cd3b342-b349-4b6f-f437-a919569e3ca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, min_delta = 0.001)]\n",
        "RNA_processo = RN.fit(X_treinamento, y_treinamento, epochs = 100, validation_data = (X_teste, y_teste), callbacks = callbacks)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.1804 - binary_accuracy: 0.8061 - val_loss: 0.0913 - val_binary_accuracy: 0.9040\n",
            "Epoch 2/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.1036 - binary_accuracy: 0.8977 - val_loss: 0.0868 - val_binary_accuracy: 0.9050\n",
            "Epoch 3/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0958 - binary_accuracy: 0.9028 - val_loss: 0.0863 - val_binary_accuracy: 0.9080\n",
            "Epoch 4/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0907 - binary_accuracy: 0.9062 - val_loss: 0.0849 - val_binary_accuracy: 0.9110\n",
            "Epoch 5/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0876 - binary_accuracy: 0.9081 - val_loss: 0.0844 - val_binary_accuracy: 0.9070\n",
            "Epoch 6/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0843 - binary_accuracy: 0.9148 - val_loss: 0.0840 - val_binary_accuracy: 0.9110\n",
            "Epoch 7/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0821 - binary_accuracy: 0.9137 - val_loss: 0.0841 - val_binary_accuracy: 0.9100\n",
            "Epoch 8/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0809 - binary_accuracy: 0.9127 - val_loss: 0.0842 - val_binary_accuracy: 0.9110\n",
            "Epoch 9/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0807 - binary_accuracy: 0.9136 - val_loss: 0.0836 - val_binary_accuracy: 0.9110\n",
            "Epoch 10/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0794 - binary_accuracy: 0.9139 - val_loss: 0.0833 - val_binary_accuracy: 0.9100\n",
            "Epoch 11/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0788 - binary_accuracy: 0.9144 - val_loss: 0.0833 - val_binary_accuracy: 0.9100\n",
            "Epoch 12/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0783 - binary_accuracy: 0.9151 - val_loss: 0.0838 - val_binary_accuracy: 0.9050\n",
            "Epoch 13/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0782 - binary_accuracy: 0.9141 - val_loss: 0.0828 - val_binary_accuracy: 0.9100\n",
            "Epoch 14/100\n",
            "282/282 [==============================] - 0s 1ms/step - loss: 0.0781 - binary_accuracy: 0.9151 - val_loss: 0.0828 - val_binary_accuracy: 0.9120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71mX1iwvHMc5",
        "outputId": "6137f10c-a480-4d9b-8628-6e9ffa1ef66a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "Model_Accuracy(RNA_processo)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-0cb3dc8627de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel_Accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNA_processo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Model_Accuracy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-zJ6GIjHbY8"
      },
      "source": [
        "Model_Loss(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1sL_DTrKmpq"
      },
      "source": [
        "### 7. Avaliar a performance da Rede Neural\n",
        "\n",
        "Para avaliar a a Rede Neural, simplesmente informamos as amostras de teste: X_teste e y_teste. A função evaluate() vai retornar uma lista contendo 2 valores: loss e accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VckQfEFPvMa7"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUhEiqxfKmpv"
      },
      "source": [
        "RN.evaluate(X_teste, y_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agO4cGTqKmpz"
      },
      "source": [
        "A seguir, a matriz de confusão:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLIAXu7SN7pV"
      },
      "source": [
        "Mostra_ConfusionMatrix()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5zYHcGuMPZe"
      },
      "source": [
        "### 8. _Fine tuning_ da Rede Neural\n",
        "\n",
        "Para aumentar a acurácia da Rede Neural, sugiro aumentarmos o número de neurônios na _Hidden Layer_ e/ou aumentar o número de _Hidden Layers_.\n",
        "\n",
        "No entanto, obtivemos uma acurácia razoável com a Rede Neural _baseline_. Portanto, deixo como exercício para os alunos o desafio de melhorar a acurácia desta Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ISodOu-Kmp3"
      },
      "source": [
        "### 9. Fazer Predições com a Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xgdL1W4vUrN"
      },
      "source": [
        "[**Python**] - Comando:\n",
        "* RN.predict_classes(X_treinamento);\n",
        "* RN.predict_classes(X_teste)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qun1-vOKmp4"
      },
      "source": [
        "y_pred = RN.predict_classes(X_teste)\n",
        "y_pred[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7sRwTWGKmp8"
      },
      "source": [
        "y_teste[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvywP0nZMtA-"
      },
      "source": [
        "### 10. Conclusões\n",
        "\n",
        "Desenvolvemos uma Rede Neural capaz de identificar Sexo (_Gender_) com acurácia= 0.9120."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5qOWxPczM1O"
      },
      "source": [
        "___\n",
        "# **EXEMPLO 2: Distinguir cédulas verdadeiras das falsas**\n",
        "\n",
        "* O exemplo a seguir foi extraído do site [OpenML](https://www.openml.org/home). Este é um problema interessante, que é o de distinguir cédulas verdadeiras de notas falsas. Os dados foram extraídos de imagens tiradas de cédulas verdadeiras e falsas. Para digitalização, foi usada uma câmera industrial normalmente usada para inspeção de impressão. As imagens finais têm 400x 400 pixels. Devido à lente do objeto e à distância do objeto investigado, foram obtidas imagens em escala de cinza com uma resolução de cerca de 660 dpi. Uma ferramenta Wavelet Transform foi usada para extrair recursos dessas imagens.\n",
        "\n",
        "* Este é o endereço do dataframe: https://www.openml.org/d/1462;\n",
        "* Descrição das variáveis - [banknote authentication Data Set](https://archive.ics.uci.edu/ml/datasets/banknote+authentication).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nup7tuLc5kYy"
      },
      "source": [
        "> A seguir, vamos desenvolver uma Rede Neural usando _Tensorflow_/_Keras_ para classificar notas falsas e verdadeiras. Nesta aplicação, vamos seguir os passos adiante:\n",
        "\n",
        "1. Carregar os dados;\n",
        "2. Pré-processamento e transformação dos dados;\n",
        "3. Definir as amostras de treinamento e validação;\n",
        "4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_;\n",
        "5. Compilar a Rede Neural;\n",
        "6. Ajustar a Rede Neural;\n",
        "7. Avaliar a performance da Rede Neural;\n",
        "8. _Fine tuning_ da Rede Neural;\n",
        "9. Fazer Predições com a Rede Neural;\n",
        "10. Conclusões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHi73Pbq5vvU"
      },
      "source": [
        "### 0. Carregar bibliotecas do Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsZW7_Ev5vvY"
      },
      "source": [
        "[**Python**] - Importar as bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U4OySJw5vvb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAaecKoj5vv5"
      },
      "source": [
        "[**Python**] - Verificar a versão do Tensorflow\n",
        "> Assegurar que está a utilizar a versão 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lPEsFy45vv6"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNvl-o5w5vvo"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqRIBc1J5vvp"
      },
      "source": [
        "np.set_printoptions(precision = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jo3Y9Hs5vwD"
      },
      "source": [
        "### 1. Carregar os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL7k4X--5vwE"
      },
      "source": [
        "[**Python**] - Carregar os dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTuRMwld5vwG"
      },
      "source": [
        "df_cedulas= pd.read_csv('https://raw.githubusercontent.com/MathMachado/DataFrames/master/Banknote-authentication-dataset.csv')\n",
        "df_cedulas.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "501b-Zv38ce7"
      },
      "source": [
        "[**Python**] - Corrigir ou renomear as colunas do dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBKafqZR8jFb"
      },
      "source": [
        "df_cedulas.columns= df_cedulas.columns.str.lower()\n",
        "df_cedulas.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBNIjNaT5vwM"
      },
      "source": [
        "[**Python**] - Mostrar quantas classes há na variável-target:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7mZgLDl5vwO"
      },
      "source": [
        "df_cedulas['class'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG4q-8nf2GS_"
      },
      "source": [
        "[**Python**] - Redefinindo a variável-target:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA7f1C4e1zOS"
      },
      "source": [
        "def Redefinir_label(row):\n",
        "    if row['class'] == 1:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DkBD1FU1zOo"
      },
      "source": [
        "df_cedulas['class'] = df_cedulas.apply(lambda row: Redefinir_label(row), axis = 1)\n",
        "df_cedulas.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0j5o4Iu5vwT"
      },
      "source": [
        "[**Python**] - Mostrar a distribuição da variável-target:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32EZZ8eP5vwV"
      },
      "source": [
        "j = sns.countplot(x = \"class\", data = df_cedulas)\n",
        "plt.show(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV8A71C55vwb"
      },
      "source": [
        "### 2. Pré-processamento e transformação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg-4TkYSXvuo"
      },
      "source": [
        "[**Python**] - Definir os arrays X_cedulas e y_cedulas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn2yMB80Xvux"
      },
      "source": [
        "X_cedulas = df_cedulas.copy()\n",
        "X_cedulas = X_cedulas.drop(columns = ['class'])\n",
        "y_cedulas = df_cedulas['class'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nckf3bieXvvC"
      },
      "source": [
        "[**Python**] - Normalizar os dados - StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFTlvOcRXvvE"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SS = StandardScaler()\n",
        "\n",
        "X_cedulas = SS.fit_transform(X_cedulas)\n",
        "X_cedulas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ouZ1it5vwz"
      },
      "source": [
        "### 3. Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6NrbTvd5vw1"
      },
      "source": [
        "[**Python**] - Definir as amostras de treinamento e validação da Rede Neural:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw3ZZ2fR5vw1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X_cedulas, y_cedulas, test_size = 0.1, random_state = 20111974)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trfqJbUg5vw8"
      },
      "source": [
        "print(f'X: Treinamento =  {X_treinamento.shape}; X: Teste =  {X_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duDt1c7i5vxB"
      },
      "source": [
        "print(f'Y: Treinamento =  {y_treinamento.shape}; Y: Teste = {y_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4TKmGtr5vxM"
      },
      "source": [
        "### 4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6EQymRK5vxO"
      },
      "source": [
        "[**Python**] - Definir a arquitetura, ou seja:\n",
        "* $N_{I}$: Número de neurônios na camada de entrada (_Input Layer_);\n",
        "* $N_{O}$: Número de neurônios na camada de saída (_Output Layer_);\n",
        "* $N_{H}$: Número de neurônios na camada escondida (_Hidden Layer_);\n",
        "* FA: Função de ativação;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlUflDN3YkG7"
      },
      "source": [
        "X_treinamento.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRsHYQp05vxO"
      },
      "source": [
        "# Número de Neurônios na Input Layer:\n",
        "N_I = X_treinamento.shape[1]\n",
        "\n",
        "# Número de neurônios na Output Layer:\n",
        "N_O = 1\n",
        "\n",
        "# Número de neurônios na Hidden Layer 1:\n",
        "N_H1 = 8\n",
        "\n",
        "# Número de neurônios na Hidden Layer 2:\n",
        "N_H2 = 8\n",
        "\n",
        "# Função de Ativação da Hidden Layer:\n",
        "FA_H = tf.keras.activations.swish\n",
        "\n",
        "# Função de Ativação da Output Layer:\n",
        "FA_O = tf.keras.activations.sigmoid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSOj8_9n5vxU"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFYGSoKH5vxU"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "tf.random.set_seed(20111974)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG1isER05vxZ"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6IPYp8l5vxa"
      },
      "source": [
        "**Observações**: \n",
        "\n",
        "Para evitar problemas relacionados ao _overfitting_ e _Vanishing or Exploding Gradients in Deep Neural Nets_, os artigos abaixo sugerem as seguintes opções para inicialização dos pesos $W$:\n",
        "\n",
        "* [How to Reduce Overfitting Using Weight Constraints in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/) sugere:\n",
        "    * kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Deep Learning Best Practices (1) — Weight Initialization](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.he_normal() para activation = 'tf.nn.relu' ou 'tf.nn.leaky_relu' e kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Vanishing/ Exploding Gradients in Deep Neural Nets and solving them](https://medium.com/swlh/vanishing-exploding-gradients-in-deep-neural-nets-and-solving-them-9d6070f28b29) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotUniform();\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotNormal()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCRp8O4V5vxa"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "RN = Sequential()\n",
        "RN.add(Dense(units = N_H1, input_dim = N_I, activation = FA_H, kernel_initializer = tf.keras.initializers.GlorotUniform(1)))#, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dense(units = N_O, activation = FA_O))\n",
        "\n",
        "# Resumo da arquitetura da Rede Neural\n",
        "print(RN.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Titw0r-d5vxh"
      },
      "source": [
        "### 5. Compilar a Rede Neural\n",
        "\n",
        "Este é um problema de classificação binária."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVQsayDq5vxi"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u686jTkd5vxj"
      },
      "source": [
        "algoritmo_otimizacao = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
        "metrica_performance = tf.keras.metrics.binary_accuracy\n",
        "\n",
        "RN.compile(optimizer = algoritmo_otimizacao, loss = loss_function, metrics = metrica_performance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKN3oCa65vxn"
      },
      "source": [
        "### 6. Ajustar a Rede Neural\n",
        "\n",
        "Nesta fase, precisamos informar:\n",
        "* **Epoch**: O número de épocas é um hiperparâmetro do _Gradient Descent_ que define o número de iterações para atualizar os pesos $W$ usando o dataframe de treinamento. Uma época significa que cada amostra no dataframe de treinamento atualizou os pesos $W$ 1 vez.\n",
        "* **Batch**: número de amostras consideradas pela Rede Neural em cada _epoch_ antes da atualização dos pesos $W$;\n",
        "\n",
        "#### Exemplo\n",
        "Suponha que temos um dataframe com 1.000 linhas (instâncias) e optamos por _epoch_= 1.000 e _batch_= 5. Isso significa que o dataframe será dividido em $\\frac{1000}{5}= 200$ _batches_. Desta forma, os pesos $W$ serão atualizados a cada processamento de 200 instâncias (linhas).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLHQdKsi5vxn"
      },
      "source": [
        "Obs.: A opção callbacks abaixo implementa o conceito de _early stopping_. Esta opção vai parar o processo de treinamento da Rede Neural antes de atingirmos o númerco de _epochs_ quando o modelo pára de melhorar, medido pela métrica val_loss. O parâmetro _patience_= k significa que o processo de otimização vai parar se tivermos k _epochs_ consecutivas sem observarmos melhoria da performance da Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6UMutI45vxp"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YhUEbTC5vxq"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3, min_delta = 0.001)]\n",
        "\n",
        "RNA_processo = RN.fit(X_treinamento, y_treinamento, epochs = 100, validation_data = (X_teste, y_teste), callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFmtvTwd5vxu"
      },
      "source": [
        "Model_Loss(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8Lu0jh55vxz"
      },
      "source": [
        "Model_Accuracy(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKh0f7Mc5vx4"
      },
      "source": [
        "### 7. Avaliar a performance da Rede Neural\n",
        "\n",
        "Para avaliar a Rede Neural, simplesmente informamos as amostras de teste: X_teste e y_teste.\n",
        "\n",
        "A função evaluate() vai retornar uma lista contendo 2 valores: loss e accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7nsNQoX5vx5"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1OvhTbf5vx6"
      },
      "source": [
        "RN.evaluate(X_teste, y_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8v2aody5vx-"
      },
      "source": [
        "### 8. Fazer Predições com a Rede Neural\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FQy0bZT5vx_"
      },
      "source": [
        "[**Python**] - Comando:\n",
        "* RN.predict_classes(X_treinamento);\n",
        "* RN.predict_classes(X_teste).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8e327A_5vx_"
      },
      "source": [
        "y_pred = RN.predict_classes(X_teste)\n",
        "y_pred[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiVyZ-CG5vyE"
      },
      "source": [
        "y_teste"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PawhHD_35vyI"
      },
      "source": [
        "### Conclusões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcIh4qua_eEU"
      },
      "source": [
        "___\n",
        "# **APLICAÇÃO 1 - Rede Neural para identificar espécies (Iris Dataframe)**\n",
        "\n",
        "> A seguir, vamos desenvolver uma Rede Neural usando _Tensorflow_/_Keras_ para classificar flores (Iris). Nesta aplicação, vamos seguir os passos adiante:\n",
        "\n",
        "1. Carregar os dados;\n",
        "2. Pré-processamento e transformação dos dados;\n",
        "3. Definir as amostras de treinamento e validação;\n",
        "4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_;\n",
        "5. Compilar a Rede Neural;\n",
        "6. Ajustar a Rede Neural;\n",
        "7. Avaliar a performance da Rede Neural;\n",
        "8. _Fine tuning_ da Rede Neural;\n",
        "9. Fazer Predições com a Rede Neural;\n",
        "10. Conclusões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXRYOpPR4XF4"
      },
      "source": [
        "### 0. Carregar bibliotecas do Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa0ir9C_dgOO"
      },
      "source": [
        "[**Python**] - Importar as bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNYF_qzydgOR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ird1VzZudgOU"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwj9CGzEdgOV"
      },
      "source": [
        "np.set_printoptions(precision = 3)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmN5HGLOdgOa"
      },
      "source": [
        "[**Python**] - Verificar a versão do Tensorflow\n",
        "> Assegurar que está a utilizar a versão 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI86wuv9dgOa",
        "outputId": "52a49fb8-b7cc-4684-925e-d7ccb47c4ad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoKZnsJpRA8o"
      },
      "source": [
        "Perfeito, estamos a usar o TensorFlow 2.x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkLZgdkjavO-"
      },
      "source": [
        "### 1. Carregar os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7LLQyA3vgBG"
      },
      "source": [
        "[**Python**] - Carregar os dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACzNibyKAkx_",
        "outputId": "9cf2915a-1b62-4195-c406-3ba4c50708bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "df_Iris = pd.read_csv('https://raw.githubusercontent.com/MathMachado/DataFrames/master/Iris.csv', index_col = 'Id')\n",
        "df_Iris.head()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
              "Id                                                                       \n",
              "1             5.1           3.5            1.4           0.2  Iris-setosa\n",
              "2             4.9           3.0            1.4           0.2  Iris-setosa\n",
              "3             4.7           3.2            1.3           0.2  Iris-setosa\n",
              "4             4.6           3.1            1.5           0.2  Iris-setosa\n",
              "5             5.0           3.6            1.4           0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Vy41RL-lAQ"
      },
      "source": [
        "[**Python**] - Corrigir ou renomear as colunas do dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN7nBQWg-lAX",
        "outputId": "ed124a6d-1fe5-4067-c890-08c315d05d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "# df_Iris.columns = [coluna.lower() for coluna in df_Iris.columns\n",
        "# df_Iris.head()"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepallengthcm</th>\n",
              "      <th>sepalwidthcm</th>\n",
              "      <th>petallengthcm</th>\n",
              "      <th>petalwidthcm</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sepallengthcm  sepalwidthcm  petallengthcm  petalwidthcm      species\n",
              "Id                                                                       \n",
              "1             5.1           3.5            1.4           0.2  Iris-setosa\n",
              "2             4.9           3.0            1.4           0.2  Iris-setosa\n",
              "3             4.7           3.2            1.3           0.2  Iris-setosa\n",
              "4             4.6           3.1            1.5           0.2  Iris-setosa\n",
              "5             5.0           3.6            1.4           0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFPGLipF7HZH"
      },
      "source": [
        "df_Iris.columns = df_Iris.columns.str.lower()\n",
        "df_Iris.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4E6rsO96xjy",
        "outputId": "7d9033a6-3f0b-4284-bfca-801c08449e77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_Iris.columns"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm',\n",
              "       'species'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em1wLwdzvkgh"
      },
      "source": [
        "[**Python**] - Mostrar quantas classes há na variável-target:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhuoPcRuA9Do",
        "outputId": "aca6b2ee-09e1-4cef-cc5f-806a250dcb4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_Iris['species'].value_counts()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Iris-virginica     50\n",
              "Iris-versicolor    50\n",
              "Iris-setosa        50\n",
              "Name: species, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvWcxUvru50G"
      },
      "source": [
        "[**Python**] - Mostrar a distribuição da variável-target 'Species':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMpJiMWJu50J",
        "outputId": "f8f29648-4b3d-4ed2-d664-b940886011e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "j = sns.countplot(x = \"species\", data = df_Iris)\n",
        "plt.show(j)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a55-G14aa_wG"
      },
      "source": [
        "### 2. Pré-processamento e transformação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uos9OyewvyMo"
      },
      "source": [
        "[**Python**] - Aplicar a transformação LabelEncoder() nos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0V0hWnBg0so"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "LE = LabelEncoder()\n",
        "\n",
        "species_encoded = LE.fit_transform(df_Iris['species'])  # espécies transformadas\n",
        "df_Iris = df_Iris.drop(columns= ['species'], axis = 1)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCL4uYVK7zwP",
        "outputId": "7830e67d-37a1-4eb9-b831-1a618644927c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "# df_Iris['species'].value_counts()\n",
        "df_Iris.head()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepallengthcm</th>\n",
              "      <th>sepalwidthcm</th>\n",
              "      <th>petallengthcm</th>\n",
              "      <th>petalwidthcm</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sepallengthcm  sepalwidthcm  petallengthcm  petalwidthcm\n",
              "Id                                                          \n",
              "1             5.1           3.5            1.4           0.2\n",
              "2             4.9           3.0            1.4           0.2\n",
              "3             4.7           3.2            1.3           0.2\n",
              "4             4.6           3.1            1.5           0.2\n",
              "5             5.0           3.6            1.4           0.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRa0gCeL7-Bm",
        "outputId": "808d7f7f-aab6-4106-dcb6-e444887fa5cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "species_encoded"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8L-b9gZwB4L"
      },
      "source": [
        "[**Python**] - Definir o array y_Iris:\n",
        "### Atenção com a forma do y, pois y já não possui mais uma única coluna, neste caso tem 3 colunas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMp2_hJ1wGm3",
        "outputId": "b976c811-da36-495a-d4ed-692f955bb0e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_Iris = tf.keras.utils.to_categorical(species_encoded)\n",
        "y_Iris[:5]"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53YhEa8h8iNi",
        "outputId": "4d99124b-d6a4-4dfc-9d85-fec5f551ec77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Observe onde começa a outra classe\n",
        "y_Iris[49:53]"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "294YPOZZ8wJ_",
        "outputId": "2a6fe22e-b7b0-43e3-98e4-5b5788403237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Observe onde começa a outra classe\n",
        "y_Iris[95:110]"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfFg6cWdv9El"
      },
      "source": [
        "[**Python**] - Definir o array X_Iris:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7coeWhVRjiQl",
        "outputId": "21d44695-9b8f-4d2c-a3c3-9f664bc9e97d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_Iris = df_Iris.values\n",
        "X_Iris[:10]"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTaMq2oS9ZUM"
      },
      "source": [
        "*X X1 X2 X3\n",
        "*1 1  0  0\n",
        "*1 1  0  0\n",
        "*1 1  0  0\n",
        "*2 0  1  0\n",
        "*2 0  1  0\n",
        "*3 0  0  1\n",
        "*3 0  0  1\n",
        "*3 0  0  1\n",
        "*3 0  0  1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUa2sJOSbUFO"
      },
      "source": [
        "### 3. Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDOw-RHux1nS"
      },
      "source": [
        "[**Python**] - Definir as amostras de treinamento e validação da Rede Neural:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJE_6w3KL_2O"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X_Iris, y_Iris, test_size = 0.2, random_state = 20111974)"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKHTG5IP9nVj",
        "outputId": "938a19a9-1795-4ed5-9e52-7a919fabed0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f'X: Treinamento =  {X_treinamento.shape}; X: Teste =  {X_teste.shape}')"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: Treinamento =  (120, 4); X: Teste =  (30, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHFI_bLXPPvl"
      },
      "source": [
        "### 4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRYoZ7hwgejR"
      },
      "source": [
        "[**Python**] - Definir a arquitetura, ou seja:\n",
        "* $N_{I}$: Número de neurônios na camada de entrada (_Input Layer_);\n",
        "* $N_{O}$: Número de neurônios na camada de saída (_Output Layer_);\n",
        "* $N_{H}$: Número de neurônios na camada escondida (_Hidden Layer_);\n",
        "* FA: Função de ativação;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb2LFg1C-BSP",
        "outputId": "1b1621a7-975b-45b1-99bb-3e62e9856754",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_treinamento.shape[1] # 4 COLUNAS"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb9F0xVu-LZ8",
        "outputId": "5a6b2f62-0c6d-4524-8e95-002cd66c567d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_Iris.shape[1] # 3 neurônios"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjF1haRmgejS"
      },
      "source": [
        "# Número de Neurônios na Input Layer:\n",
        "N_I = X_treinamento.shape[1]\n",
        "\n",
        "# Número de neurônios na Output Layer:\n",
        "N_O = y_Iris.shape[1]\n",
        "\n",
        "# Número de neurônios na Hidden Layer:\n",
        "N_H1 = 32\n",
        "\n",
        "# Função de Ativação da Hidden Layer:\n",
        "# FA_H = tf.keras.  --> para salvar o modelo depois --> não funcionou\n",
        "FA_H = tf.nn.leaky_relu\n",
        "\n",
        "# Função de Ativação da Output Layer:\n",
        "FA_O = tf.nn.softmax"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGeDaB3oo02k"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGS15afAo02n"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "tf.random.set_seed(20111974)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT9w2tUCo5-X"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nytcmC4BkSz1"
      },
      "source": [
        "**Observações**: \n",
        "\n",
        "Para evitar problemas relacionados ao _overfitting_ e _Vanishing or Exploding Gradients in Deep Neural Nets_, os artigos abaixo sugerem as seguintes opções para inicialização dos pesos $W$:\n",
        "\n",
        "* [How to Reduce Overfitting Using Weight Constraints in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/) sugere:\n",
        "    * kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Deep Learning Best Practices (1) — Weight Initialization](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.he_normal() para activation = 'tf.nn.relu' ou 'tf.nn.leaky_relu' e kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Vanishing/ Exploding Gradients in Deep Neural Nets and solving them](https://medium.com/swlh/vanishing-exploding-gradients-in-deep-neural-nets-and-solving-them-9d6070f28b29) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotUniform();\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotNormal()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnLeLMmZoUjU",
        "outputId": "219bfb89-adf6-4109-e3bc-94001eafc102",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "RN_Iris = Sequential()\n",
        "RN_Iris.add(Dense(units = N_H1, \n",
        "                  input_dim = N_I, \n",
        "                  kernel_initializer = tf.keras.initializers.GlorotNormal(), \n",
        "                  activation = FA_H, \n",
        "                  kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "# RN_Iris.add(Dropout(0.1))\n",
        "# RN_Iris.add(Dense(units = 32,\n",
        "#                  activation = tf.keras.activations.relu))\n",
        "RN_Iris.add(Dropout(0.1))\n",
        "RN_Iris.add(Dense(units = N_O, activation = FA_O))\n",
        "\n",
        "# Resumo da arquitetura da Rede Neural\n",
        "print(RN_Iris.summary())"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_17 (Dense)             (None, 32)                160       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 259\n",
            "Trainable params: 259\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eT-EHUecTj3"
      },
      "source": [
        "### 5. Compilar a Rede Neural\n",
        "\n",
        "Este é um problema de classificação multi-classes (> 2 classes). Portanto, temos:\n",
        "* loss = tf.keras.losses.CategoricalCrossentropy()\n",
        ";\n",
        "* metrics = tf.keras.metrics.binary_accuracy;\n",
        "* optimizer = tf.keras.optimizers.Adam()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsq0aEtwyAAM"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDStOKqhcRf4"
      },
      "source": [
        "algoritmo_otimizacao = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrica_performance = tf.keras.metrics.binary_accuracy\n",
        "\n",
        "RN_Iris.compile(optimizer = algoritmo_otimizacao, loss = loss_function, metrics = metrica_performance)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZFu65TecabN"
      },
      "source": [
        "### 6. Ajustar a Rede Neural\n",
        "\n",
        "Nesta fase, precisamos informar:\n",
        "* **Epoch**: O número de épocas é um hiperparâmetro do _Gradient Descent_ que define o número de iterações para atualizar os pesos $W$ usando o dataframe de treinamento. Uma época significa que cada amostra no dataframe de treinamento atualizou os pesos $W$ 1 vez.\n",
        "* **Batch**: número de amostras consideradas pela Rede Neural em cada _epoch_ antes da atualização dos pesos $W$;\n",
        "\n",
        "#### Exemplo\n",
        "Suponha que temos um dataframe com 1.000 linhas (instâncias) e optamos por _epoch_= 1.000 e _batch_= 5. Isso significa que o dataframe será dividido em $\\frac{1000}{5}= 200$ _batches_. Desta forma, os pesos $W$ serão atualizados a cada processamento de 200 instâncias (linhas).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boIs266gaZt1"
      },
      "source": [
        "*** Callbacks:*** \n",
        "\n",
        "## Obs.: A opção callbacks abaixo implementa o conceito de _early stopping_. Esta opção vai parar o processo de treinamento da Rede Neural antes de atingirmos o númerco de _epochs_ quando o modelo pára de melhorar, medido pela métrica val_loss. O parâmetro _patience_= k significa que o processo de otimização vai parar se tivermos k _epochs_ consecutivas sem observarmos melhoria da performance da Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpR3dXRZ-jom"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hDxEwHjca8V"
      },
      "source": [
        "# Definição da regra de callback:\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
        "                                              patience = 3, \n",
        "                                              min_delta = 0.001)]\n"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEfFehO-IC_l",
        "outputId": "3e568883-c3d6-4b67-e350-56bce211493f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "RNA_processo = RN_Iris.fit(X_treinamento, \n",
        "                           y_treinamento, \n",
        "                           epochs = 100, \n",
        "                           validation_data = (X_teste, y_teste), \n",
        "                           callbacks = callbacks)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 2.4396 - binary_accuracy: 0.4528 - val_loss: 1.6433 - val_binary_accuracy: 0.5667\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.8753 - binary_accuracy: 0.5667 - val_loss: 1.3238 - val_binary_accuracy: 0.4889\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.6104 - binary_accuracy: 0.5694 - val_loss: 1.1233 - val_binary_accuracy: 0.7111\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2293 - binary_accuracy: 0.6944 - val_loss: 0.9995 - val_binary_accuracy: 0.7222\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.9207 - binary_accuracy: 0.7472 - val_loss: 0.7088 - val_binary_accuracy: 0.7667\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.8391 - binary_accuracy: 0.7389 - val_loss: 0.6819 - val_binary_accuracy: 0.7778\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7697 - binary_accuracy: 0.7861 - val_loss: 0.5614 - val_binary_accuracy: 0.8444\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7214 - binary_accuracy: 0.7778 - val_loss: 0.6528 - val_binary_accuracy: 0.7000\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6024 - binary_accuracy: 0.8194 - val_loss: 0.5274 - val_binary_accuracy: 0.8778\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5675 - binary_accuracy: 0.8250 - val_loss: 0.4862 - val_binary_accuracy: 0.8444\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5961 - binary_accuracy: 0.7917 - val_loss: 0.4662 - val_binary_accuracy: 0.8444\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4925 - binary_accuracy: 0.8500 - val_loss: 0.5245 - val_binary_accuracy: 0.7889\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5460 - binary_accuracy: 0.8194 - val_loss: 0.4856 - val_binary_accuracy: 0.8444\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4836 - binary_accuracy: 0.8639 - val_loss: 0.4264 - val_binary_accuracy: 0.9222\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4771 - binary_accuracy: 0.8500 - val_loss: 0.4246 - val_binary_accuracy: 0.9111\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4018 - binary_accuracy: 0.8806 - val_loss: 0.4407 - val_binary_accuracy: 0.8667\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3902 - binary_accuracy: 0.8944 - val_loss: 0.4030 - val_binary_accuracy: 0.9222\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3881 - binary_accuracy: 0.8889 - val_loss: 0.3973 - val_binary_accuracy: 0.9333\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.3312 - binary_accuracy: 0.9167 - val_loss: 0.3714 - val_binary_accuracy: 0.9556\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.3418 - binary_accuracy: 0.9222 - val_loss: 0.3671 - val_binary_accuracy: 0.9333\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2842 - binary_accuracy: 0.9444 - val_loss: 0.3693 - val_binary_accuracy: 0.9333\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.3232 - binary_accuracy: 0.9167 - val_loss: 0.3765 - val_binary_accuracy: 0.9000\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2826 - binary_accuracy: 0.9250 - val_loss: 0.3387 - val_binary_accuracy: 0.9333\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.2750 - binary_accuracy: 0.9528 - val_loss: 0.3314 - val_binary_accuracy: 0.9333\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2610 - binary_accuracy: 0.9639 - val_loss: 0.3172 - val_binary_accuracy: 0.9333\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.2522 - binary_accuracy: 0.9639 - val_loss: 0.3044 - val_binary_accuracy: 0.9333\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2184 - binary_accuracy: 0.9806 - val_loss: 0.3117 - val_binary_accuracy: 0.9333\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2363 - binary_accuracy: 0.9694 - val_loss: 0.2733 - val_binary_accuracy: 0.9778\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2105 - binary_accuracy: 0.9722 - val_loss: 0.2648 - val_binary_accuracy: 0.9778\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2055 - binary_accuracy: 0.9722 - val_loss: 0.2746 - val_binary_accuracy: 0.9333\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2141 - binary_accuracy: 0.9694 - val_loss: 0.3037 - val_binary_accuracy: 0.8889\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2164 - binary_accuracy: 0.9667 - val_loss: 0.2411 - val_binary_accuracy: 0.9778\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1955 - binary_accuracy: 0.9694 - val_loss: 0.2697 - val_binary_accuracy: 0.9333\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1765 - binary_accuracy: 0.9722 - val_loss: 0.2884 - val_binary_accuracy: 0.9000\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.1878 - binary_accuracy: 0.9611 - val_loss: 0.2408 - val_binary_accuracy: 0.9556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF7EC-g82Hho",
        "outputId": "9cc3442b-3067-4f03-c13b-53c586fb20c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "Model_Loss(RNA_processo)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-169-1777fcbedf9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNA_processo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Model_Loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea0HHBsY2NZ5",
        "outputId": "919c15d3-9906-41c6-a505-c0f478aa25b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "Model_Accuracy(RNA_processo)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-170-0cb3dc8627de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel_Accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNA_processo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Model_Accuracy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqVJMX3xchLF"
      },
      "source": [
        "### 7. Avaliar a performance da Rede Neural\n",
        "\n",
        "Para avaliar a Rede Neural, simplesmente informamos as amostras de teste: X_teste e y_teste.\n",
        "\n",
        "A função evaluate() vai retornar uma lista contendo 2 valores: loss e accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKbO1nT0yQM1"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYqDY9V9chcZ",
        "outputId": "95320d24-31d7-4a09-8dcb-ca32f5bb782d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "RN_Iris.evaluate(X_teste, y_teste)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2408 - binary_accuracy: 0.9556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2407999038696289, 0.9555555582046509]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qchEpyipcnbE"
      },
      "source": [
        "### 8. Fazer Predições com a Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X2SZ5fx2_s5"
      },
      "source": [
        "[**Python**] - Comando:\n",
        "* RN.predict_classes(X_treinamento);\n",
        "* RN.predict_classes(X_teste)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR6ySksLhRvR",
        "outputId": "04d5e8c7-fda3-4eb5-b8f8-f2f3ab910ba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred = RN_Iris.predict_classes(X_teste)\n",
        "y_pred"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 2, 2, 1, 2, 1, 1, 2, 0, 2, 2, 2, 1, 1, 0, 0, 2, 2, 1, 1, 2,\n",
              "       0, 0, 2, 2, 0, 1, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWnRgBbmmlA2",
        "outputId": "9e9dddc8-572c-48db-9b0b-c4ce3fd1d526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_teste"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjCtiKieE_TT"
      },
      "source": [
        "### Conclusões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxMzVIGzzGHH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpufntZjyH8T"
      },
      "source": [
        "### Salvar a Rede Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKqn0leTyLOy",
        "outputId": "01d0426b-93a6-4a01-d021-b532bd47c460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "# Save the weights\n",
        "RN_Iris.save(\"./content/RN_Iris_versao_final.h5\")"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-177-2b5b6d1425d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRN_Iris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./content/RN_Iris_versao_final.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \"\"\"\n\u001b[1;32m   1978\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1979\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m   def save_weights(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    129\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    130\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 131\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    132\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = './content/RN_Iris_versao_final.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCP0jKpjzUIe",
        "outputId": "2bd91d3d-fc65-4c2f-8793-f40b9cb85c13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# Restore the weights\n",
        "RN_Iris2 = tf.keras.models.load_model('./content/RN_versao_final.h5')"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-178-33be73da10e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Restore the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRN_Iris2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./content/RN_versao_final.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m                   (export_dir,\n\u001b[1;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ./content/RN_versao_final.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qjSoM5quog1"
      },
      "source": [
        "___\n",
        "# **APLICAÇÃO 2 - Rede Neural para identificar o tipo do vinho (_tinto or White_)**\n",
        "\n",
        "> Nesta aplicação, vamos usar o dataframe [wine quality](https://archive.ics.uci.edu/ml/datasets/wine+quality) extraído do repositório da UCI Machine Learning Repository. Nosso objetivo é prever o tipo do vinho (red ou white) baseado nas suas propriedades químicas.\n",
        "\n",
        "Novamente, vamos desenvolver uma Rede Neural usando _Tensorflow_/_Keras_ e seguir os passos adiante:\n",
        "\n",
        "1. Carregar os dados;\n",
        "2. Pré-processamento e transformação dos dados;\n",
        "3. Definir as amostras de treinamento e validação;\n",
        "4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_;\n",
        "5. Compilar a Rede Neural;\n",
        "6. Ajustar a Rede Neural;\n",
        "7. Avaliar a performance da Rede Neural;\n",
        "8. _Fine tuning_ da Rede Neural;\n",
        "9. Fazer Predições com a Rede Neural;\n",
        "10. Conclusões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuG8zbYS4jyx"
      },
      "source": [
        "### 0. Carregar bibliotecas do Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3XD4otFd9Ht"
      },
      "source": [
        "[**Python**] - Importar as bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5Ok5fhid9Hv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBXRTgnCd9Hz"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFrOyNUgd9Hz"
      },
      "source": [
        "np.set_printoptions(precision = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGxW2zn6q8xY"
      },
      "source": [
        "[**Python**] - Verificar a versão do Tensorflow\n",
        "> Assegurar que está a utilizar a versão 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C36Z6vGD4jy8"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWJXP5diof5G"
      },
      "source": [
        "### 1. Carregar os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJJe4r_ITDzv"
      },
      "source": [
        "[**Python**] - Carregar os dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIjqYXlH4tyG"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "Wine = load_wine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iU33wKQGrFb"
      },
      "source": [
        "df_tinto = pd.read_table(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep = ';')\n",
        "df_tinto.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liy2KSr6HJUX"
      },
      "source": [
        "df_branco= pd.read_table('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', sep= ';')\n",
        "df_branco.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TzUnuM4TMmJ"
      },
      "source": [
        "[**Python**] - Mostrar o número de linhas e colunas de cada dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHNwVRQyIwdv"
      },
      "source": [
        "print(f'Dimensão de df_tinto: {df_tinto.shape}; Dimensão de df_branco: {df_branco.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBWzMsUHJjTU"
      },
      "source": [
        "[**Python**] - Construir a variável-target 'type_wine' (tipo do vinho):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0WmX3FYJqMR"
      },
      "source": [
        "df_tinto['type_wine']= 0\n",
        "df_branco['type_wine']= 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVUc1vbqJ0D9"
      },
      "source": [
        "[**Python**] - Empilhar os dois dataframes: df_tinto e df_branco:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS9SlpvaJ2Ex"
      },
      "source": [
        "df_vinhos= pd.concat([df_tinto, df_branco], ignore_index=True)\n",
        "df_vinhos.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM19HHENKMtq"
      },
      "source": [
        "df_vinhos.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpoNmcqpeQxO"
      },
      "source": [
        "[**Python**] - Mostrar o número de linhas e colunas do dataframe df_vinhos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJDazoMMeMet"
      },
      "source": [
        "df_vinhos.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNyjXmFzo7oe"
      },
      "source": [
        "### 2. Pré-processamento e transformação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcV_dlHsTpiP"
      },
      "source": [
        "[**Python**] - Renomear o nome das colunas usando letras minúsculas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jau2OybudGc0"
      },
      "source": [
        "df_vinhos.columns = df_vinhos.columns.str.strip().str.lower().str.replace(' ', '_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzOWiaCFdq_C"
      },
      "source": [
        "df_vinhos.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1wgkLcvete8"
      },
      "source": [
        "[**Python**] - Estatísticas descritivas do dataframe df_vinhos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-EbHitYepeQ"
      },
      "source": [
        "df_vinhos.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRo_7tOjex3Z"
      },
      "source": [
        "####  _Missing Values Handling_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ3K_FmCUCsk"
      },
      "source": [
        "[**Python**] - Mostrar o número de _missing values_ no dataframe df_vinhos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PODgu5B6e5qQ"
      },
      "source": [
        "df_vinhos.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szH_TGJZecXp"
      },
      "source": [
        "Como podem ver, o dataframe df_vinhos não contem _missing values_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64znO089vF_2"
      },
      "source": [
        "[**Python**] - Mostrar a distribuição da variável-target 'type_wine':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztQ17lhWfdAu"
      },
      "source": [
        "df_vinhos['type_wine'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDRVQy5avF_4"
      },
      "source": [
        "j = sns.countplot(x=\"type_wine\", data= df_vinhos)\n",
        "plt.show(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxexO1Uyeqnj"
      },
      "source": [
        "Como podem ver, temos 6.497 instâncias, das quais 4.898 (75%) instâncias são type_wine= 1 (_branco_) e 1.599 (25%) instâncias são type_wine= 0 (_tinto_). Logo, trata-se de um dataframe desbalanceado. No entanto, não vou me preocupar com este assunto neste Notebook e deixo para os alunos como exercício verificar os efeitos das amostras desbalanceadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr_HZnF0UUh3"
      },
      "source": [
        "[**Python**] - Definir os arrays X_vinhos e y_vinhos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNvImi4djc-3"
      },
      "source": [
        "X_vinhos= df_vinhos.copy()\n",
        "X_vinhos= X_vinhos.drop(columns= ['type_wine'])\n",
        "y_vinhos= df_vinhos['type_wine'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViMFMd_SlE3x"
      },
      "source": [
        "[**Python**] - Normalizar os dados - StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OurlnbX3hzSb"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define o scaler \n",
        "SS = StandardScaler()\n",
        "\n",
        "# Scale o dataframe\n",
        "X_vinhos = SS.fit_transform(X_vinhos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFkJ7b4U1x3D"
      },
      "source": [
        "### 3. Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBqzBcDL2JH4"
      },
      "source": [
        "[**Python**] - Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCQUxbbxhVTM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X_vinhos, y_vinhos, test_size = 0.2, random_state = 20111974)\n",
        "print(f'X: Treinamento=  {X_treinamento.shape}; X: Teste= {X_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHmTtQ3F9A9c"
      },
      "source": [
        "print(f'Y: Treinamento=  {y_treinamento.shape}; Y: Teste= {y_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPW0aWd4jw0o"
      },
      "source": [
        "### 4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea4WTsP-hCqS"
      },
      "source": [
        "[**Python**] - Definir a arquitetura, ou seja:\n",
        "* $N_{I}$: Número de neurônios na camada de entrada (_Input Layer_);\n",
        "* $N_{O}$: Número de neurônios na camada de saída (_Output Layer_);\n",
        "* $N_{H}$: Número de neurônios na camada escondida (_Hidden Layer_);\n",
        "* FA: Função de ativação;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1pVFjFThCqU"
      },
      "source": [
        "# Número de Neurônios na Input Layer:\n",
        "N_I = X_vinhos.shape[1]\n",
        "\n",
        "# Número de neurônios na Output Layer:\n",
        "N_O = 1\n",
        "\n",
        "# Número de neurônios na Hidden Layer:\n",
        "N_H1 = 7\n",
        "\n",
        "# Função de Ativação da Hidden Layer:\n",
        "FA_H = tf.nn.leaky_relu\n",
        "\n",
        "# Função de Ativação da Output Layer:\n",
        "FA_O = tf.nn.leaky_relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "url8178EpUNO"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU-8uWn-pUNQ"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "tf.random.set_seed(20111974)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TLUirr6oXv1"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HOQIkQ3beJm"
      },
      "source": [
        "**Observações**: \n",
        "\n",
        "Para evitar problemas relacionados ao _overfitting_ e _Vanishing or Exploding Gradients in Deep Neural Nets_, os artigos abaixo sugerem as seguintes opções para inicialização dos pesos $W$:\n",
        "\n",
        "* [How to Reduce Overfitting Using Weight Constraints in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/) sugere:\n",
        "    * kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Deep Learning Best Practices (1) — Weight Initialization](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.he_normal() para activation = 'tf.nn.relu' ou 'tf.nn.leaky_relu' e kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Vanishing/ Exploding Gradients in Deep Neural Nets and solving them](https://medium.com/swlh/vanishing-exploding-gradients-in-deep-neural-nets-and-solving-them-9d6070f28b29) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotUniform();\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotNormal()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj5uJvgaj43u"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "RN = Sequential()\n",
        "RN.add(Dense(units = N_H, input_dim = N_I, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_O, activation = FA_O))\n",
        "\n",
        "# Resumo da arquitetura da Rede Neural\n",
        "print(RN.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTs8xbKx2O3Z"
      },
      "source": [
        "### 5. Compilar a Rede Neural\n",
        "\n",
        "Este é um problema de classificação binária (_tinto_ ou _branco_). Portanto, temos:\n",
        "* loss =  tf.keras.losses.BinaryCrossentropy();\n",
        "* metrics = tf.keras.metrics.binary_accuracy;\n",
        "* optimizer = tf.keras.optimizers.Adam().\n",
        "\n",
        "> **Lembre-se**: se o problema fosse de classificação de multi-classes (> 2 classes), então devemos usar loss = tf.keras.losses.CategoricalCrossentropy()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUpBKJkl07KH"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGZ1bKCo2L7A"
      },
      "source": [
        "algoritmo_otimizacao = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
        "metrica_performance = tf.keras.metrics.binary_accuracy\n",
        "\n",
        "RN.compile(optimizer = algoritmo_otimizacao, loss = loss_function, metrics = metrica_performance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBhkSc582ROC"
      },
      "source": [
        "### 6. Ajustar a Rede Neural\n",
        "\n",
        "Obs.: A opção callbacks abaixo implementa o conceito de _early stopping_. Esta opção vai parar o processo de treinamento da Rede Neural antes de atingirmos o númerco de _epochs_ quando o modelo pára de melhorar, medido pela métrica val_loss. O parâmetro _patience_= k significa que o processo de otimização vai parar se tivermos k _epochs_ consecutivas sem observarmos melhoria da performance da Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls8FfHz0z2lX"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mpbdiuJ2d1W"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, min_delta = 0.001)]\n",
        "RNA_processo = RN.fit(X_treinamento, y_treinamento, epochs = 10, validation_data = (X_teste, y_teste), callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sALJp9FQ2WGC"
      },
      "source": [
        "Model_Accuracy(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWR2rMLg2ZlQ"
      },
      "source": [
        "Model_Loss(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciXdkbVr2VDG"
      },
      "source": [
        "### 7. Avaliar a performance da Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS4_cV5TyamK"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkkxiqPe2gZK"
      },
      "source": [
        "RN.evaluate(X_teste, y_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Vs47VV4RsV"
      },
      "source": [
        "A Rede Neural tem acurácia= 0.9946."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHQoDk533TRX"
      },
      "source": [
        "[**Python**] - Comando:\n",
        "* RN.predict_classes(X_treinamento);\n",
        "* RN.predict_classes(X_teste)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlWfMR-k7Qc1"
      },
      "source": [
        "y_pred = RN.predict_classes(X_teste)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCpp-MO0nXmj"
      },
      "source": [
        "A seguir, outras medidas para avaliarmos a performance da Rede Neural:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWF-wWfTnjQh"
      },
      "source": [
        "# Import the modules from sklearn.metrics\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHD0CvGQnm0v"
      },
      "source": [
        "# Precision \n",
        "precision_score(y_teste, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERoX4BeQnosC"
      },
      "source": [
        "# Recall\n",
        "recall_score(y_teste, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_2bS23Inq1p"
      },
      "source": [
        "# F1 score\n",
        "f1_score(y_teste,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLM1BSK2QfZm"
      },
      "source": [
        "A seguir, a matriz de confusão:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGAigaBMQfZn"
      },
      "source": [
        "Mostra_ConfusionMatrix()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3yUb9tP2YfE"
      },
      "source": [
        "### 8. Fazer Predições com a Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CupPMVRo3a5q"
      },
      "source": [
        "[**Python**] - Comando:\n",
        "* RN.predict_classes(X_treinamento);\n",
        "* RN.predict_classes(X_teste)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GePOr4EJ3mfn"
      },
      "source": [
        "y_pred = RN.predict_classes(X_teste)\n",
        "y_pred[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO79VSBS3mfv"
      },
      "source": [
        "y_teste[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHlDHVp_KcLn"
      },
      "source": [
        "### 10. Conclusão\n",
        "\n",
        "Desenvolvemos uma Rede Neural com 1 _Hidden Layer_ que atingiu acurácia de 0.998."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr2VzmwPFRdr"
      },
      "source": [
        "___\n",
        "# **EXERCÍCIO 1**: Prever a qualidade do vinho\n",
        "\n",
        "Neste exercício, vamos considerar a variável quality como múltiplas classes.\n",
        "\n",
        "Novamente, vamos desenvolver uma Rede Neural usando _Tensorflow_/_Keras_ e seguir os passos adiante:\n",
        "\n",
        "1. Carregar os dados;\n",
        "2. Pré-processamento e transformação dos dados;\n",
        "3. Definir as amostras de treinamento e validação;\n",
        "4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_;\n",
        "5. Compilar a Rede Neural;\n",
        "6. Ajustar a Rede Neural;\n",
        "7. Avaliar a performance da Rede Neural;\n",
        "8. _Fine tuning_ da Rede Neural;\n",
        "9. Fazer Predições com a Rede Neural;\n",
        "10. Conclusões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYpF4TtpvXp4"
      },
      "source": [
        "[**Python**] - Mostrar a distribuição da variável-target 'quality':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmeFj14PvXp6"
      },
      "source": [
        "j = sns.countplot(x = \"quality\", data = df_vinhos)\n",
        "plt.show(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smWNCLsMQWBI"
      },
      "source": [
        "Muitas classes... Abaixo, KMeans para definirmos a quantidade de clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co4GNVexVUoP"
      },
      "source": [
        "# Função adaptada de: https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/\n",
        "\n",
        "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, manhattan_distances\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def Numero_Clusters_Elbow(X):\n",
        "    distortions = [] \n",
        "    inertias = [] \n",
        "    mapping1 = {} \n",
        "    mapping2 = {} \n",
        "    K = range(1,10)    \n",
        "    for k in K:\n",
        "        #Building and fitting the model \n",
        "        kmeanModel = KMeans(n_clusters=k).fit(X) \n",
        "        kmeanModel.fit(X)\n",
        "        distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0]) \n",
        "        inertias.append(kmeanModel.inertia_)\n",
        "        mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0] \n",
        "        mapping2[k] = kmeanModel.inertia_ \n",
        "\n",
        "    # Using the different values of Distortion\n",
        "    print('Cálculo da Distorção:')\n",
        "    for key,val in mapping1.items():\n",
        "        print(str(key)+' : '+str(val))\n",
        "\n",
        "    plt.plot(K, distortions, 'bx-')\n",
        "    plt.xlabel('Values of K')\n",
        "    plt.ylabel('Distortion')\n",
        "    plt.title('The Elbow Method using Distortion')\n",
        "    plt.show() \n",
        "\n",
        "    # Using the different values of Inertia\n",
        "    print('Cálculo da Inertia:')\n",
        "    for key,val in mapping2.items():\n",
        "        print(str(key)+' : '+str(val))\n",
        "\n",
        "    plt.plot(K, inertias, 'bx-')\n",
        "    plt.xlabel('Values of K')\n",
        "    plt.ylabel('Inertia') \n",
        "    plt.title('The Elbow Method using Inertia')\n",
        "    plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTA4O3yII-am"
      },
      "source": [
        "X_vinhosQ = df_vinhos.copy()\n",
        "X_vinhosQ = X_vinhosQ.drop(columns = ['quality'])\n",
        "X_vinhosQ = X_vinhosQ.values\n",
        "#y_vinhosQ = df_vinhos['quality2'].values\n",
        "X_vinhosQ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJDCLdsalKvS"
      },
      "source": [
        "[**Python**] - Normalizar os dados - StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D9qCW6SYKwA"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define o scaler \n",
        "SS = StandardScaler().fit(X_vinhosQ)\n",
        "\n",
        "# Scale o dataframe\n",
        "X_vinhosQ = SS.transform(X_vinhosQ)\n",
        "X_vinhosQ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naLIn_ufIZtL"
      },
      "source": [
        "Numero_Clusters_Elbow(X_vinhosQ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At5KtD-DV0f6"
      },
      "source": [
        "Os gráficos acima mostram que o número de _clusters_ ótimos para o dataframe df_vinhos é 3. Portanto, vamos trabalhar com n_cluster= 3 daqui para frente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oz9kAvh5yPk"
      },
      "source": [
        "Sugiro fazermos o seguinte agrupamento:\n",
        "* Se quality= 1, 2, 3 $\\Longrightarrow$ quality2= 1 (qualidade ruim);\n",
        "* Se quality= 4, 5, 6, 7 $\\Longrightarrow$ quality2= 2 (qualidade média);\n",
        "* Se quality= 8, 9, 10 $\\Longrightarrow$ quality2= 3 (qualidade excelente)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZc2OQR87E9n"
      },
      "source": [
        "def define_quality2(row):\n",
        "    if row['quality'] <= 3:\n",
        "        return 1\n",
        "    elif row['quality'] > 3 and row['quality'] <= 7:\n",
        "        return 2\n",
        "    elif row['quality'] > 7:\n",
        "        return 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8zbLsN27E9t"
      },
      "source": [
        "df_vinhos['quality2']=df_vinhos.apply(lambda row: define_quality2(row), axis= 1)\n",
        "df_vinhos.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ugFbdVm77wo"
      },
      "source": [
        "df_vinhos['quality2'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf3RYZTwEtAD"
      },
      "source": [
        "[**Python**] - Análise de Correlação:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dimuJxsPEsAt"
      },
      "source": [
        "corr = df_vinhos.corr()['quality2'].drop('quality2')\n",
        "print(corr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9KALiQsEsAx"
      },
      "source": [
        "plt.figure(figsize = (12, 10))\n",
        "cor = df_vinhos.corr()\n",
        "sns.heatmap(cor, annot = True, linewidths = 0, vmin = -1, cmap = \"RdBu_r\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At1XAt6AU2oh"
      },
      "source": [
        "[**Python**] - Aplicar a transformação LabelEncoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kld7QvCaH1YQ"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "LE = LabelEncoder()\n",
        "LE.fit(df_vinhos['quality2'])quality_Encoded = LE.transform(df_vinhos['quality2'])\n",
        "\n",
        "y_vinhosQ = tf.keras.utils.to_categorical(quality_Encoded)\n",
        "y_vinhosQ[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qqY5vxeGEMC"
      },
      "source": [
        "X_vinhosQ = df_vinhos.copy()\n",
        "X_vinhosQ = X.drop(columns = 'quality2', axis= 1)\n",
        "X_vinhosQ.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKnfuTQFlQLB"
      },
      "source": [
        "[**Python**] - Normalizar os dados - StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VSNAOQzGEMG"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define o scaler \n",
        "SS = StandardScaler().fit(X_vinhosQ)\n",
        "\n",
        "# Scale o dataframe\n",
        "X_vinhosQ = SS.transform(X_vinhosQ)\n",
        "X_vinhosQ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE5SV0ET-Zwv"
      },
      "source": [
        "[**Python**] - Aplicar PCA para selecionar somente os atributos mais importantes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3IRWaIj-feA"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_vinhosQ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbJ6NBcAVPO1"
      },
      "source": [
        "[**Python**] - Proporção de variância explicada por cada componente principal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOqLh5BgVVN0"
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j_cYnbSVGaT"
      },
      "source": [
        "[**Python**] - Proporção acumulada de cada fator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux0lAfy2Ar6g"
      },
      "source": [
        "pca.explained_variance_ratio_.cumsum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UzYqn7-A5He"
      },
      "source": [
        "Como podemos ver acima, 8 componentes principais acumulam mais que 93% da variância. Portanto, vamos selecionar 8 componentes para treinar a Rede Neural."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzekEmIMBqe1"
      },
      "source": [
        "pca8 = PCA(n_components = 8)\n",
        "X_pca8 = pca8.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYDhJYdV-pSb"
      },
      "source": [
        "A seguir, o gráfico com as componentes principais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zUN_3wR-nOX"
      },
      "source": [
        "plt.figure(figsize = (7, 7))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO3Umy7sGEMJ"
      },
      "source": [
        "### 3. Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2gXueuI2Nb4"
      },
      "source": [
        "[**Python**] - Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giFgdIK7GEMJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X_pca8, y_vinhosQ, test_size = 0.2, random_state = 20111974)\n",
        "print(f'X: Treinamento =  {X_treinamento.shape}; X: Teste =  {X_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr4sryEr-eOy"
      },
      "source": [
        "print(f'Y: Treinamento =  {y_treinamento.shape}; Y: Teste = {y_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2KSZ8TQGEMM"
      },
      "source": [
        "### 4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQyKOA8Qhgtd"
      },
      "source": [
        "[**Python**] - Definir a arquitetura, ou seja:\n",
        "* $N_{I}$: Número de neurônios na camada de entrada (_Input Layer_);\n",
        "* $N_{O}$: Número de neurônios na camada de saída (_Output Layer_);\n",
        "* $N_{H}$: Número de neurônios na camada escondida (_Hidden Layer_);\n",
        "* FA: Função de ativação;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62E0wT8hhgtd"
      },
      "source": [
        "# Número de Neurônios na Input Layer:\n",
        "N_I = X_pca8.shape[1]\n",
        "\n",
        "# Número de neurônios na Output Layer:\n",
        "N_O = y_vinhosQ.shape[1]\n",
        "\n",
        "# Número de neurônios na Hidden Layer:\n",
        "N_H1 = 32\n",
        "N_H2 = 32\n",
        "N_H3 = 32\n",
        "N_H4 = 32\n",
        "N_H5 = 32\n",
        "N_H6 = 32\n",
        "\n",
        "# Função de Ativação da Hidden Layer:\n",
        "FA_H = tf.nn.leaky_relu\n",
        "\n",
        "# Função de Ativação da Output Layer:\n",
        "FA_O = tf.nn.softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT0kLyuGpZ4C"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3Wdq7XvpZ4F"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "tf.random.set_seed(20111974)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ZBeC4EnZyQ"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjUyPpoJbg20"
      },
      "source": [
        "**Observações**: \n",
        "\n",
        "Para evitar problemas relacionados ao _overfitting_ e _Vanishing or Exploding Gradients in Deep Neural Nets_, os artigos abaixo sugerem as seguintes opções para inicialização dos pesos $W$:\n",
        "\n",
        "* [How to Reduce Overfitting Using Weight Constraints in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/) sugere:\n",
        "    * kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Deep Learning Best Practices (1) — Weight Initialization](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.he_normal() para activation = 'tf.nn.relu' ou 'tf.nn.leaky_relu' e kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Vanishing/ Exploding Gradients in Deep Neural Nets and solving them](https://medium.com/swlh/vanishing-exploding-gradients-in-deep-neural-nets-and-solving-them-9d6070f28b29) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotUniform();\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotNormal()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry43SCP4nbvp"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "RN = Sequential()\n",
        "RN.add(Dense(units = N_H1, input_dim = N_I, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_H2, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_H3, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_H4, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_H5, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_H6, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dense(units = N_O, activation = FA_O)) # Atenção à Função de Ativação!\n",
        "\n",
        "# Resumo da arquitetura da Rede Neural\n",
        "print(RN.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQwY5qQLGEMR"
      },
      "source": [
        "### 5. Compilar a Rede Neural\n",
        "\n",
        "Este é um problema de classificação multi-classes (> 2 classes). Portanto, temos:\n",
        "* loss = tf.keras.losses.CategoricalCrossentropy();\n",
        "* optimizer = tf.keras.optimizers.Adam();\n",
        "* metrics = tf.keras.metrics.binary_accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVp9pT8n1AQC"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el86VgZ-GEMS"
      },
      "source": [
        "algoritmo_otimizacao = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrica_performance = tf.keras.metrics.binary_accuracy\n",
        "\n",
        "RN.compile(optimizer = algoritmo_otimizacao, loss = loss_function, metrics = metrica_performance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNyxbvjsGEMV"
      },
      "source": [
        "### 6. Ajustar a Rede Neural\n",
        "\n",
        "Obs.: A opção callbacks abaixo implementa o conceito de _early stopping_. Esta opção vai parar o processo de treinamento da Rede Neural antes de atingirmos o númerco de _epochs_ quando o modelo pára de melhorar, medido pela métrica val_loss. O parâmetro _patience_= k significa que o processo de otimização vai parar se tivermos k _epochs_ consecutivas sem observarmos melhoria da performance da Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZtHza72z_q9"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tikYg2CrGEMV"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, min_delta = 0.001)]\n",
        "RNA_processo = RN.fit(X_treinamento, y_treinamento, epochs = 100, validation_data = (X_teste, y_teste), batch_size= 12, callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6aoEBw92p86"
      },
      "source": [
        "Model_Loss(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei7XWVel2qNW"
      },
      "source": [
        "Model_Accuracy(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tovtcp98GEMa"
      },
      "source": [
        "### 7. Avaliar a performance da Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAC9nKCvyiKu"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmhkMbmpGEMb"
      },
      "source": [
        "RN.evaluate(X_teste, y_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjON99B1B9F0"
      },
      "source": [
        "* Rede Neural com Accuracy= 0.9692 SEM PCA.\n",
        "* Rede Neural com Accuracy= 0.9677 COM PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VzIx2_EGEMp"
      },
      "source": [
        "### 8. Fazer Predições com a Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpLRlBTI3gvF"
      },
      "source": [
        "[**Python**] - Comando:\n",
        "* RN.predict_classes(X_treinamento);\n",
        "* RN.predict_classes(X_teste);"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c1K5vGjGEMq"
      },
      "source": [
        "y_pred = RN.predict_classes(X_teste)\n",
        "y_pred[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b-hDZ04GEMs"
      },
      "source": [
        "y_teste[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3mgEOcXu20F"
      },
      "source": [
        "___\n",
        "# **APLICAÇÃO 3 - Rede Neural para identificar Câncer de Mama (_Breast Cancer Dataframe_)**\n",
        "\n",
        "Fonte: [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)\n",
        "\n",
        "Vamos desenvolver uma Rede Neural usando _Tensorflow_/_Keras_ e seguir os passos adiante:\n",
        "\n",
        "1. Carregar os dados;\n",
        "2. Pré-processamento e transformação dos dados;\n",
        "3. Definir as amostras de treinamento e validação;\n",
        "4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_;\n",
        "5. Compilar a Rede Neural;\n",
        "6. Ajustar a Rede Neural;\n",
        "7. Avaliar a performance da Rede Neural;\n",
        "8. _Fine tuning_ da Rede Neural;\n",
        "9. Fazer Predições com a Rede Neural;\n",
        "10. Conclusões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVZQ1meF9l_-"
      },
      "source": [
        "### 0. Carregar bibliotecas do Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd4zQSkseKzv"
      },
      "source": [
        "[**Python**] - Importar as bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzMsBroueKzx"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJSbJx9rrBG3"
      },
      "source": [
        "[**Python**] - Verificar a versão do Tensorflow\n",
        "> Assegurar que está a utilizar a versão 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alCVjy_JCWUo"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ra_PjiFeKz0"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4I2Eh2YeKz1"
      },
      "source": [
        "np.set_printoptions(precision = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nXjn5KM94lI"
      },
      "source": [
        "### 1. Carregar os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovtqPCcWBgfI"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "Cancer = load_breast_cancer()\n",
        "X_cancer = Cancer.data\n",
        "y_Cancer = Cancer.target\n",
        "nomes_colunas = Cancer.feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qbOErH6Dkcu"
      },
      "source": [
        "nomes_colunas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weKuFIHqDWRX"
      },
      "source": [
        "X_cancer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kix-c9TDY_R"
      },
      "source": [
        "y_Cancer[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg_IuwHp946c"
      },
      "source": [
        "### 2. Pré-processamento e transformação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wnkuDnMlVe8"
      },
      "source": [
        "[**Python**] - Normalizar os dados - StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVEHT9sqG9WR"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SS = StandardScaler()\n",
        "X_cancer = SS.fit_transform(X_cancer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNeVqHH295FA"
      },
      "source": [
        "### 3. Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rbc3MwX2RDU"
      },
      "source": [
        "[**Python**] - Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR6XSrbsFX-P"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X_cancer, y_Cancer, test_size = 0.1, random_state = 20111974)\n",
        "print(f'X: Treinamento =  {X_treinamento.shape}; X: Teste =  {X_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRQ8zW1m-g6l"
      },
      "source": [
        "print(f'Y: Treinamento =  {y_treinamento.shape}; Y: Teste = {y_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "775KoeQz95O6"
      },
      "source": [
        "### 4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFaKVel2ilZs"
      },
      "source": [
        "[**Python**] - Definir a arquitetura, ou seja:\n",
        "* $N_{I}$: Número de neurônios na camada de entrada (_Input Layer_);\n",
        "* $N_{O}$: Número de neurônios na camada de saída (_Output Layer_);\n",
        "* $N_{H}$: Número de neurônios na camada escondida (_Hidden Layer_);\n",
        "* FA: Função de ativação;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB7xcQFWilZs"
      },
      "source": [
        "# Número de Neurônios na Input Layer:\n",
        "N_I = X_cancer.shape[1]\n",
        "\n",
        "# Número de neurônios na Output Layer:\n",
        "N_O = 1\n",
        "\n",
        "# Número de neurônios na Hidden Layer:\n",
        "N_H1 = 16\n",
        "N_H2 = 16\n",
        "N_H3 = 16\n",
        "\n",
        "# Função de Ativação da Hidden Layer:\n",
        "FA_H = tf.nn.leaky_relu\n",
        "\n",
        "# Função de Ativação da Output Layer:\n",
        "FA_O = tf.nn.softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm1PeihTRlgz"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3oTC94GRlg3"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "tf.random.set_seed(20111974)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5GBW-Ucnh9T"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my2xjCniIb4J"
      },
      "source": [
        "Vamos adicionar duas camadas _Dropout_ com $p= 0.1$ para evitarmos _overfitting_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kne06wmjnTYD"
      },
      "source": [
        "**Observações**: \n",
        "\n",
        "Para evitar problemas relacionados ao _overfitting_ e _Vanishing or Exploding Gradients in Deep Neural Nets_, os artigos abaixo sugerem as seguintes opções para inicialização dos pesos $W$:\n",
        "\n",
        "* [How to Reduce Overfitting Using Weight Constraints in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/) sugere:\n",
        "    * kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Deep Learning Best Practices (1) — Weight Initialization](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.he_normal() para activation = 'tf.nn.relu' ou 'tf.nn.leaky_relu' e kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Vanishing/ Exploding Gradients in Deep Neural Nets and solving them](https://medium.com/swlh/vanishing-exploding-gradients-in-deep-neural-nets-and-solving-them-9d6070f28b29) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotUniform();\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotNormal()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6x7mo4dnjlE"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "RN = Sequential()\n",
        "RN.add(Dense(units = N_H1, input_dim = N_I, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_H2, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_H3, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dense(units = N_O, activation = FA_O))\n",
        "\n",
        "# Resumo da arquitetura da Rede Neural\n",
        "print(RN.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04cKraWZ9mMb"
      },
      "source": [
        "### 5. Compilar a Rede Neural\n",
        "\n",
        "Este é um problema de classificação binária (maligno ou benigno). Portanto, temos:\n",
        "* loss = tf.keras.losses.BinaryCrossentropy();\n",
        "* metrics = tf.keras.metrics.binary_accuracy;\n",
        "* optimizer = tf.keras.optimizers.Adam()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRjWcsm1FM9"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLIoA8FrJJCx"
      },
      "source": [
        "algoritmo_otimizacao = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
        "metrica_performance = tf.keras.metrics.binary_accuracy\n",
        "\n",
        "RN.compile(optimizer = algoritmo_otimizacao, loss = loss_function, metrics = metrica_performance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnL12eaF9mU6"
      },
      "source": [
        "### 6. Ajustar a Rede Neural\n",
        "\n",
        "Obs.: A opção callbacks abaixo implementa o conceito de _early stopping_. Esta opção vai parar o processo de treinamento da Rede Neural antes de atingirmos o númerco de _epochs_ quando o modelo pára de melhorar, medido pela métrica val_loss. O parâmetro _patience_= k significa que o processo de otimização vai parar se tivermos k _epochs_ consecutivas sem observarmos melhoria da performance da Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVsziJfk0FIv"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apQY6cQjJb-z"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, min_delta = 0.001)]\n",
        "RNA_processo = RN.fit(X_treinamento, y_treinamento, epochs = 100, validation_data = (X_teste, y_teste), callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avd2cXpO20cY"
      },
      "source": [
        "Model_Accuracy(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCd8xFxA25Lc"
      },
      "source": [
        "Model_Loss(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zToEvUs-pCt"
      },
      "source": [
        "### 7. Avaliar a performance da Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqzKH7jsymwL"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmjuk6OqJ7zD"
      },
      "source": [
        "RN.evaluate(X_teste, y_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ZGPI6DKcnz"
      },
      "source": [
        "A seguir, a matriz de confusão:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MZyagwaKfkM"
      },
      "source": [
        "Mostra_ConfusionMatrix()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KasqSFWG-pTG"
      },
      "source": [
        "### 8. _Fine tuning_ da Rede Neural\n",
        "\n",
        "Não é necessário, pois obtivemos 0.9825 de acurácia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLxgJP3L-pdZ"
      },
      "source": [
        "### 9. Fazer Predições com a Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iXGBnNZYb4V"
      },
      "source": [
        "[**Python**] - Comando:\n",
        "* RN.predict_classes(X_treinamento);\n",
        "* RN.predict_classes(X_teste)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqBFwxg5Yb4b"
      },
      "source": [
        "y_pred = RN.predict_classes(X_teste)\n",
        "y_pred[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CHdWhgD-plr"
      },
      "source": [
        "### 10. Conclusões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2AQ4uDShdgE"
      },
      "source": [
        "___\n",
        "# **APLICAÇÃO 4 - Rede Neural para identificar Diabetes (Diabetes Dataframe)**\n",
        "\n",
        "> Vamos desenvolver uma Rede Neural usando _Tensorflow_/_Keras_ e seguir os passos adiante:\n",
        "\n",
        "1. Carregar os dados;\n",
        "2. Pré-processamento e transformação dos dados;\n",
        "3. Definir as amostras de treinamento e validação;\n",
        "4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_;\n",
        "5. Compilar a Rede Neural;\n",
        "6. Ajustar a Rede Neural;\n",
        "7. Avaliar a performance da Rede Neural;\n",
        "8. _Fine tuning_ da Rede Neural;\n",
        "9. Fazer Predições com a Rede Neural;\n",
        "10. Conclusões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOEJGtAzQfX3"
      },
      "source": [
        "### 0. Carregar bibliotecas do Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxa5UaIXeRgN"
      },
      "source": [
        "[**Python**] - Importar as bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylfhuYeveRgO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG9B3WTkeRgR"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimais:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZkm0YVoeRgR"
      },
      "source": [
        "np.set_printoptions(precision= 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDhEsSJ1rFpy"
      },
      "source": [
        "[**Python**] - Verificar a versão do Tensorflow\n",
        "> Assegurar que está a utilizar a versão 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfKcNLZ3QfYJ"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIT9N7jSQfYO"
      },
      "source": [
        "### 1. Carregar os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9QUJZgbSWDG"
      },
      "source": [
        "[**Python**] - Carregar os dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofSJNoyfQfYR"
      },
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "Diabetes = load_diabetes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo7q0BnyShVG"
      },
      "source": [
        "[**Python**] - Definir os arrays X_diabetes e y_diabetes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTnrDMPLQfYW"
      },
      "source": [
        "X_diabetes = Diabetes.data\n",
        "y_diabetes = Diabetes.target\n",
        "nomes_colunas = Diabetes.feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjrdZUwp_l40"
      },
      "source": [
        "[**Python**] - Corrigir ou renomear as colunas do dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skgDY4Lu_l46"
      },
      "source": [
        "X_diabetes.columns = X_diabetes.columns.str.lower()\n",
        "X_diabetes.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5NQO8b-QfYb"
      },
      "source": [
        "X_diabetes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adpBpNDeQfYj"
      },
      "source": [
        "y_diabetes[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQA4fN4HQfYo"
      },
      "source": [
        "### 2. Pré-processamento e transformação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTnBpjwalbVG"
      },
      "source": [
        "[**Python**] - Normalizar os dados - StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS_unh4wQfYp"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SS = StandardScaler()\n",
        "X_diabetes = SS.fit_transform(X_diabetes)\n",
        "y_diabetes= SS.fit_transform(y_diabetes.reshape(-(1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSviMMrISt96"
      },
      "source": [
        "Y[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWmVyMF0QfYu"
      },
      "source": [
        "### 3. Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEMSNMJu2VqI"
      },
      "source": [
        "[**Python**] - Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WUQMh2HQfYx"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X_diabetes, y_diabetes, test_size = 0.1, random_state = 20111974)\n",
        "print(f'X: Treinamento =  {X_treinamento.shape}; X: Teste =  {X_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvx80NPT-j0S"
      },
      "source": [
        "print(f'Y: Treinamento =  {y_treinamento.shape}; Y: Teste = {y_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk_CG4H5QfY2"
      },
      "source": [
        "### 4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1bDqK5vi49C"
      },
      "source": [
        "[**Python**] - Definir a arquitetura, ou seja:\n",
        "* $N_{I}$: Número de neurônios na camada de entrada (_Input Layer_);\n",
        "* $N_{O}$: Número de neurônios na camada de saída (_Output Layer_);\n",
        "* $N_{H}$: Número de neurônios na camada escondida (_Hidden Layer_);\n",
        "* FA: Função de ativação;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "por467-ci49D"
      },
      "source": [
        "# Número de Neurônios na Input Layer:\n",
        "N_I = X_diabetes.shape[1]\n",
        "\n",
        "# Número de neurônios na Output Layer:\n",
        "N_O = 1\n",
        "\n",
        "# Número de neurônios na Hidden Layer:\n",
        "N_H1 = 6\n",
        "\n",
        "# Função de Ativação da Hidden Layer:\n",
        "FA_H = tf.nn.leaky_relu\n",
        "\n",
        "# Função de Ativação da Output Layer:\n",
        "FA_O = tf.nn.linear"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r7VC-7lpkkC"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43f-ZPW7pkkD"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "tf.random.set_seed(20111974)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qv8lJmHnqi3"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Veeqccdbnks"
      },
      "source": [
        "**Observações**: \n",
        "\n",
        "Para evitar problemas relacionados ao _overfitting_ e _Vanishing or Exploding Gradients in Deep Neural Nets_, os artigos abaixo sugerem as seguintes opções para inicialização dos pesos $W$:\n",
        "\n",
        "* [How to Reduce Overfitting Using Weight Constraints in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/) sugere:\n",
        "    * kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Deep Learning Best Practices (1) — Weight Initialization](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.he_normal() para activation = 'tf.nn.relu' ou 'tf.nn.leaky_relu' e kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Vanishing/ Exploding Gradients in Deep Neural Nets and solving them](https://medium.com/swlh/vanishing-exploding-gradients-in-deep-neural-nets-and-solving-them-9d6070f28b29) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotUniform();\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotNormal()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMOoG_0bnsSD"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "RN = Sequential()\n",
        "RN.add(Dense(units = N_H1, input_dim = N_I, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dense(units = N_O, activation = FA_O)) # Se não definirmos o parâmetro activation, então por default será 'linear'.\n",
        "\n",
        "# Resumo da arquitetura da Rede Neural\n",
        "print(RN.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W8VtONlQfZP"
      },
      "source": [
        "### 5. Compilar a Rede Neural\n",
        "\n",
        "Este é um problema de regressão. Portanto, temos:\n",
        "* loss = tf.keras.losses.MeanSquaredError;\n",
        "* metrics = 'mse';\n",
        "* optimizer = tf.keras.optimizers.Adam()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g97mJCSr1Kat"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXJFtlcEQfZQ"
      },
      "source": [
        "algoritmo_otimizacao = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.MeanSquaredError()\n",
        "metrica_performance = tf.keras.metrics.MeanSquaredError()\n",
        "\n",
        "RN.compile(optimizer = algoritmo_otimizacao, loss = loss_function, metrics = metrica_performance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s1S7Fn_QfZW"
      },
      "source": [
        "### 6. Ajustar a Rede Neural\n",
        "\n",
        "Obs.: A opção callbacks abaixo implementa o conceito de _early stopping_. Esta opção vai parar o processo de treinamento da Rede Neural antes de atingirmos o númerco de _epochs_ quando o modelo pára de melhorar, medido pela métrica val_loss. O parâmetro _patience_= k significa que o processo de otimização vai parar se tivermos k _epochs_ consecutivas sem observarmos melhoria da performance da Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNgR4ihA0JMy"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUaqK4j-QfZY"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, min_delta = 0.001)]\n",
        "RNA_processo = RN.fit(X_treinamento, y_treinamento, epochs = 200, validation_data = (X_teste, y_teste), callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bFg6kut1jkb"
      },
      "source": [
        "A seguir, funções para plotarmos os gráficos das métricas MSE, _Loss_ e _Accuracy_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPASVWaR1sWN"
      },
      "source": [
        "def Model_Loss(RNA_processo):\n",
        "    print(RNA_processo.history.keys())\n",
        "    plt.plot(RNA_processo.history['loss'])\n",
        "    plt.plot(RNA_processo.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training', 'Validation'], loc= 'upper right')\n",
        "    plt.show()\n",
        "\n",
        "def Model_Accuracy(RNA_processo):\n",
        "    print(RNA_processo.history.keys())\n",
        "    plt.plot(RNA_processo.history['accuracy'])\n",
        "    plt.plot(RNA_processo.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training', 'Validation'], loc= 'upper right')\n",
        "    plt.show()\n",
        "\n",
        "def Model_MSE(RNA_processo):\n",
        "    print(RNA_processo.history.keys())\n",
        "    plt.plot(RNA_processo.history['mse'])\n",
        "    plt.plot(RNA_processo.history['val_mse'])\n",
        "    plt.title('Model MSE')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training', 'Validation'], loc= 'upper right')\n",
        "    plt.show()\n",
        "\n",
        "def Mostra_ConfusionMatrix():\n",
        "    y_pred = RN.predict_classes(X_teste)\n",
        "    mc = confusion_matrix(y_teste, y_pred)\n",
        "    #sns.heatmap(mc,annot=True, annot_kws={\"size\": 10},fmt=\"d\")\n",
        "    sns.heatmap(mc/np.sum(mc), annot = True, annot_kws = {\"size\": 10}, fmt= '.2%', cmap = 'Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWhJUP0v2_fm"
      },
      "source": [
        "Model_Loss(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8IZFKGyCvqO"
      },
      "source": [
        "Model_MSE(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37_0RhXLQfZc"
      },
      "source": [
        "### 7. Avaliar a performance da Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mMrIS9JyriW"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRjEkvWzQfZe"
      },
      "source": [
        "RN.evaluate(X_teste, y_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA6_RkjgQfZs"
      },
      "source": [
        "### 8. _Fine tuning_ da Rede Neural\n",
        "\n",
        "Vou deixar esta fase como exercício para o aluno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPJtuCzXQfZu"
      },
      "source": [
        "### 9. Fazer Predições com a Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1EptFS1Yi-D"
      },
      "source": [
        "[**Python**] - Comando:\n",
        "* RN.predict_classes(X_treinamento);\n",
        "* RN.predict_classes(X_teste)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbrvwgyvYi-I"
      },
      "source": [
        "y_pred = RN.predict_classes(X_teste)\n",
        "y_pred[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOsTSHwoQfZ0"
      },
      "source": [
        "### 10. Conclusões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoQ5nySZmLDP"
      },
      "source": [
        "___\n",
        "# **APLICAÇÃO 5 - Rede Neural para prever os preços das casas (_Boston House Price Prediction_)**\n",
        "\n",
        "Vamos desenvolver uma Rede Neural usando _Tensorflow_/_Keras_ e seguir os passos adiante:\n",
        "\n",
        "1. Carregar os dados;\n",
        "2. Pré-processamento e transformação dos dados;\n",
        "3. Definir as amostras de treinamento e validação;\n",
        "4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_;\n",
        "5. Compilar a Rede Neural;\n",
        "6. Ajustar a Rede Neural;\n",
        "7. Avaliar a performance da Rede Neural;\n",
        "8. _Fine tuning_ da Rede Neural;\n",
        "9. Fazer Predições com a Rede Neural;\n",
        "10. Conclusões."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vdRpBS8VTw_"
      },
      "source": [
        "### 0. Carregar bibliotecas do Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v629ZppSeY5T"
      },
      "source": [
        "[**Python**] - Importar as bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVXroVLTeY5U"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYCcNW9qeY5W"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNn-kwlGeY5X"
      },
      "source": [
        "np.set_printoptions(precision = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnlZU1rLrJwt"
      },
      "source": [
        "[**Python**] - Verificar a versão do Tensorflow\n",
        "> Assegurar que está a utilizar a versão 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "445U8OKgVTxW"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Ckhzf0VTxc"
      },
      "source": [
        "### 1. Carregar os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAz0_L0e1mxX"
      },
      "source": [
        "[] Carregar os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOdpdceAVTxd"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "\n",
        "Boston = load_boston()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0P23sJs1raX"
      },
      "source": [
        "[**Python**] - Definir as matrizes X_Boston e y_Boston:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPpJOsgJ1y7J"
      },
      "source": [
        "X_Boston = Boston.data\n",
        "y_Boston = Boston.target\n",
        "nomes_colunas= Boston.feature_names\n",
        "nomes_colunas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XBRc6og_ySA"
      },
      "source": [
        "[**Python**] - Corrigir ou renomear as colunas do dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPGDwXSF_ySE"
      },
      "source": [
        "X_Boston.columns = X_Boston.columns.str.lower()\n",
        "X_Boston.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKiKT-fkVTxq"
      },
      "source": [
        "y_Boston[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9uYgjz-VTxu"
      },
      "source": [
        "### 2. Pré-processamento e transformação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rbpIU5jlgv6"
      },
      "source": [
        "[**Python**] - Normalizar os dados - StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbs-x9a2VTxw"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SS = StandardScaler()\n",
        "X_Boston = SS.fit_transform(X_Boston)\n",
        "y_Boston =  SS.fit_transform(y_Boston.reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2w2H9BOXK9u"
      },
      "source": [
        "X_Boston"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXNIHeS2XM_k"
      },
      "source": [
        "y_Boston[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcbomDeKVTx1"
      },
      "source": [
        "### 3. Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEkX579Q2D2q"
      },
      "source": [
        "[**Python**] - Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZyRBsfYVTx2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X_Boston, y_Boston, test_size = 0.1, random_state = 20111974)\n",
        "print(f'X: Treinamento =  {X_treinamento.shape}; X: Teste =  {X_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g89c6edL-mBW"
      },
      "source": [
        "print(f'Y: Treinamento =  {y_treinamento.shape}; Y: Teste = {y_teste.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU-ebO-3VTx7"
      },
      "source": [
        "### 4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMVzohGHjS_p"
      },
      "source": [
        "[**Python**] - Definir a arquitetura, ou seja:\n",
        "* $N_{I}$: Número de neurônios na camada de entrada (_Input Layer_);\n",
        "* $N_{O}$: Número de neurônios na camada de saída (_Output Layer_);\n",
        "* $N_{H}$: Número de neurônios na camada escondida (_Hidden Layer_);\n",
        "* FA: Função de ativação;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf32pQtWjS_u"
      },
      "source": [
        "# Número de Neurônios na Input Layer:\n",
        "N_I = X_Boston.shape[1]\n",
        "\n",
        "# Número de neurônios na Output Layer:\n",
        "N_O = 1\n",
        "\n",
        "# Número de neurônios na Hidden Layer:\n",
        "N_H1 = 7\n",
        "\n",
        "# Função de Ativação da Hidden Layer:\n",
        "FA_H = tf.nn.leaky_relu\n",
        "\n",
        "# Função de Ativação da Output Layer:\n",
        "FA_O = FA_O"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOI4_BPYVTyE"
      },
      "source": [
        "Vamos adicionar uma camada _Dropout_ com $p= 0.1$ para evitarmos _overfitting_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN9lxrXspp-m"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWcQ3OS5pp-n"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "tf.random.set_seed(20111974)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73PnOLbon3Jh"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjKR3qgEneJr"
      },
      "source": [
        "**Observações**: \n",
        "\n",
        "Para evitar problemas relacionados ao _overfitting_ e _Vanishing or Exploding Gradients in Deep Neural Nets_, os artigos abaixo sugerem as seguintes opções para inicialização dos pesos $W$:\n",
        "\n",
        "* [How to Reduce Overfitting Using Weight Constraints in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/) sugere:\n",
        "    * kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Deep Learning Best Practices (1) — Weight Initialization](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.he_normal() para activation = 'tf.nn.relu' ou 'tf.nn.leaky_relu' e kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Vanishing/ Exploding Gradients in Deep Neural Nets and solving them](https://medium.com/swlh/vanishing-exploding-gradients-in-deep-neural-nets-and-solving-them-9d6070f28b29) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotUniform();\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotNormal()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnZuQZZTn4_W"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "RN = Sequential()\n",
        "RN.add(Dense(units = N_H1, input_dim = N_I, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_O, activation = FA_O)) # # Se não definirmos o parâmetro activation, então por default será 'linear'.\n",
        "\n",
        "# Resumo da arquitetura da Rede Neural\n",
        "print(RN.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-hyQiokVTyM"
      },
      "source": [
        "### 5. Compilar a Rede Neural\n",
        "\n",
        "Este é um problema de regressão. Portanto, temos:\n",
        "* loss = tf.keras.losses.MeanSquaredError ou tf.keras.losses.MeanAbsoluteError();\n",
        "* metrics = 'mse'.\n",
        "* optimizer = tf.keras.optimizers.Adam() ou 'rmsprop'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwQOPOhr1Oh0"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY2aKnL_VTyN"
      },
      "source": [
        "algoritmo_otimizacao = tf.keras.optimizers.RMSprop()\n",
        "loss_function = tf.keras.losses.MeanSquaredError()\n",
        "metrica_performance = tf.keras.metrics.MeanSquaredError()\n",
        "\n",
        "RN.compile(optimizer = algoritmo_otimizacao, loss = loss_function, metrics = metrica_performance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygJi0ux5VTyT"
      },
      "source": [
        "### 6. Ajustar a Rede Neural\n",
        "\n",
        "Obs.: A opção callbacks abaixo implementa o conceito de _early stopping_. Esta opção vai parar o processo de treinamento da Rede Neural antes de atingirmos o númerco de _epochs_ quando o modelo pára de melhorar, medido pela métrica val_loss. O parâmetro _patience_= k significa que o processo de otimização vai parar se tivermos k _epochs_ consecutivas sem observarmos melhoria da performance da Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz0urLrq0NPG"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HoQZUl8VTyU"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, min_delta = 0.001)]\n",
        "RNA_processo = RN.fit(X_treinamento, y_treinamento, epochs = 200, validation_data = (X_teste, y_teste), callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_StfUsUzbto"
      },
      "source": [
        "Model_Loss(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1FIaMx_zzVW"
      },
      "source": [
        "Model_MAE(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t3k3oqg0pXW"
      },
      "source": [
        "Model_MSE(RNA_processo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH0llgTsVTyY"
      },
      "source": [
        "### 7. Avaliar a performance da Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZZPMFXvyvtG"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZGhNF5vVTyZ"
      },
      "source": [
        "RN.evaluate(X_teste, y_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkosiHm6lmww"
      },
      "source": [
        "Observe que a Rede Neural _baseline_ (modelo inicial) apresenta MSE= 0.0795. Ainda assim, vamos tentar reduzir a _Loss Function_ e tentar chegar à um MSE ainda menor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcLONQpPVTyi"
      },
      "source": [
        "### 8. _Fine tuning_ da Rede Neural\n",
        "\n",
        "O que pode ser feito para melhorar a performance da Rede Neural?\n",
        "* aumentar o número de _Hidden Layers_ e o número de neurônios em cada uma delas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Uxk3j_ndFX"
      },
      "source": [
        "N_I = X_Boston.shape[1]\n",
        "N_H1 = 32\n",
        "N_H2 = 32\n",
        "N_H3 = 32\n",
        "N_O = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3Eb_5dp2PZ"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X48MWaa_p2Pb"
      },
      "source": [
        "np.random.seed(20111974)\n",
        "tf.random.set_seed(20111974)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHbjgPT7nP-q"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8s-XRqdbuXP"
      },
      "source": [
        "**Observações**: \n",
        "\n",
        "Para evitar problemas relacionados ao _overfitting_ e _Vanishing or Exploding Gradients in Deep Neural Nets_, os artigos abaixo sugerem as seguintes opções para inicialização dos pesos $W$:\n",
        "\n",
        "* [How to Reduce Overfitting Using Weight Constraints in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/) sugere:\n",
        "    * kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Deep Learning Best Practices (1) — Weight Initialization](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.he_normal() para activation = 'tf.nn.relu' ou 'tf.nn.leaky_relu' e kernel_constraint = tf.keras.constraints.UnitNorm();\n",
        "* [Vanishing/ Exploding Gradients in Deep Neural Nets and solving them](https://medium.com/swlh/vanishing-exploding-gradients-in-deep-neural-nets-and-solving-them-9d6070f28b29) sugere:\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotUniform();\n",
        "    * kernel_initializer = tf.keras.initializers.GlorotNormal()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HcNOQoFnR3W"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "RN = Sequential()\n",
        "RN.add(Dense(units = N_H1, input_dim = N_I, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_H2, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm())\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_H3, kernel_initializer = tf.keras.initializers.GlorotNormal(), activation = FA_H, kernel_constraint = tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units = N_O, activation = FA_O)) # Se não definirmos o parâmetro activation, então por default será 'linear'.\n",
        "\n",
        "# Resumo da arquitetura da Rede Neural\n",
        "print(RN.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBolPrXZnZ5i"
      },
      "source": [
        "#### 8.5. Compilar a Rede Neural (_Fine tuning_ da Rede Neural)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMBCiUTC1W2H"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlBccXXmnZ5k"
      },
      "source": [
        "algoritmo_otimizacao = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.MeanSquaredError()\n",
        "metrica_performance = tf.keras.metrics.MeanSquaredError()\n",
        "\n",
        "RN.compile(optimizer = algoritmo_otimizacao, loss = loss_function, metrics = metrica_performance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIOA5UFfnZ5p"
      },
      "source": [
        "#### 8.6. Ajustar a Rede Neural (_Fine tuning_ da Rede Neural)\n",
        "\n",
        "Obs.: A opção callbacks abaixo implementa o conceito de _early stopping_. Esta opção vai parar o processo de treinamento da Rede Neural antes de atingirmos o númerco de _epochs_ quando o modelo pára de melhorar, medido pela métrica val_loss. O parâmetro _patience_= k significa que o processo de otimização vai parar se tivermos k _epochs_ consecutivas sem observarmos melhoria da performance da Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktlrSmGQ0Qrq"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM5x90ArnZ5r"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, min_delta = 0.001)]\n",
        "RNA_processo = RN.fit(X_treinamento, y_treinamento, epochs = 500, validation_data = (X_teste, y_teste), callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfxvOccmnZ5z"
      },
      "source": [
        "#### 8.7. Avaliar a performance da Rede Neural (_Fine tuning_ da Rede Neural)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XZmb9zIy1Xf"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "belFKJQSnZ51"
      },
      "source": [
        "RN.evaluate(X_teste, y_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7whymUw5VTyq"
      },
      "source": [
        "### 10. Conclusões\n",
        "\n",
        "A performance da Rede Neural melhorou um pouco com a redução do MSE."
      ]
    }
  ]
}