
Vou ajudá-lo a criar uma base de dados sintética com 10 colunas para ensinar XGBoost, além de um notebook estruturado para introduzir o modelo de aprendizagem supervisionada. Vamos gerar uma base de dados-modelo com 10 features, onde algumas variáveis são numéricas e outras categóricas, com uma coluna-alvo para classificação binária.

### Passo 1: Gerar a Base de Dados Sintética

Para gerar a base de dados, vamos criar variáveis com diferentes tipos de dados para simular um caso realista:

1. `age`: Idade (numérica, inteira)
2. `income`: Renda anual (numérica, contínua)
3. `employment_years`: Anos de experiência no trabalho (numérica, inteira)
4. `credit_score`: Pontuação de crédito (numérica, contínua)
5. `has_debt`: Indicador se possui dívida (binária, categórica)
6. `home_ownership`: Tipo de moradia (`own`, `rent`, `mortgage`) (categórica)
7. `education_level`: Nível de escolaridade (`high_school`, `bachelor`, `master`, `phd`) (categórica)
8. `marital_status`: Estado civil (`single`, `married`, `divorced`) (categórica)
9. `num_children`: Número de filhos (numérica, inteira)
10. `monthly_spending`: Gastos mensais médios (numérica, contínua)

A coluna `target` será nossa variável de resposta, que indica a probabilidade de aprovação de crédito (0 = não aprovado, 1 = aprovado).

### Passo 2: Estrutura do Notebook

O notebook pode ser dividido nas seguintes seções:

1. **Introdução ao XGBoost**: Explicação breve do modelo, incluindo características principais e motivos para sua popularidade.
  
2. **Importação das Bibliotecas Necessárias**:
   ```python
   import pandas as pd
   import numpy as np
   from sklearn.model_selection import train_test_split
   from xgboost import XGBClassifier
   from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
   ```

3. **Geração da Base de Dados Sintética**:
   ```python
   np.random.seed(0)

   data = pd.DataFrame({
       'age': np.random.randint(18, 70, 1000),
       'income': np.random.normal(50000, 15000, 1000),
       'employment_years': np.random.randint(1, 40, 1000),
       'credit_score': np.random.normal(600, 100, 1000),
       'has_debt': np.random.choice([0, 1], 1000),
       'home_ownership': np.random.choice(['own', 'rent', 'mortgage'], 1000),
       'education_level': np.random.choice(['high_school', 'bachelor', 'master', 'phd'], 1000),
       'marital_status': np.random.choice(['single', 'married', 'divorced'], 1000),
       'num_children': np.random.randint(0, 5, 1000),
       'monthly_spending': np.random.normal(2000, 500, 1000),
       'target': np.random.choice([0, 1], 1000)
   })
   ```

4. **Pré-processamento dos Dados**: Conversão de variáveis categóricas em dummies e divisão dos dados.
   ```python
   data = pd.get_dummies(data, columns=['home_ownership', 'education_level', 'marital_status'], drop_first=True)
   X = data.drop('target', axis=1)
   y = data['target']

   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
   ```

5. **Treinamento do Modelo XGBoost**:
   ```python
   model = XGBClassifier()
   model.fit(X_train, y_train)
   ```

6. **Avaliação do Modelo**:
   ```python
   y_pred = model.predict(X_test)
   print("Accuracy:", accuracy_score(y_test, y_pred))
   print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
   print("Classification Report:\n", classification_report(y_test, y_pred))
   ```

Essa estrutura é eficiente para ensinar XGBoost, cobrindo desde a geração de dados até a avaliação do modelo.
