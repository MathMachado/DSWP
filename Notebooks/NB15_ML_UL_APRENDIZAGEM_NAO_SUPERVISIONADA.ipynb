{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "NB15_ML_UL_APRENDIZAGEM_NAO_SUPERVISIONADA.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathMachado/DSWP/blob/master/Notebooks/NB15_ML_UL_APRENDIZAGEM_NAO_SUPERVISIONADA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP5pknjXihMW"
      },
      "source": [
        "# MACHINE LEARNING | APRENDIZAGEM NÃO SUPERVISIONADA | CLUSTERING & PCA (Principal Components Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJx2WWNdRIEP"
      },
      "source": [
        "## Leitura Complementar\n",
        "https://scikit-learn.org/stable/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBkvZrI9ihMZ"
      },
      "source": [
        "# Introdução aos Algoritmos de Aprendizagem não-supervisionada\n",
        "* Unsupervised Learning são um tipo de Machine Learning que trabalha com dataframes não-rotulados;\n",
        "* Intuitivamente, os modelos desta classe tentam estabelecer relacionamento entre os dados;\n",
        "* Algoritmo mais comum: clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEDglEnoihMa"
      },
      "source": [
        "# Clustering\n",
        "* Agrupa objetos similares.\n",
        "* Aplicações de Clustering:\n",
        "  - Rotular os dados;\n",
        "  - Entender padrões escondindos nos dados;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FTf-drRSiTT"
      },
      "source": [
        "## Exemplo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM-T8AxGSk5M"
      },
      "source": [
        "### Carrega as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9fU9we1ihMb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-N9e3jlSx3I"
      },
      "source": [
        "### Carrega os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MvFRbiVihMi",
        "outputId": "0cd7c16d-a7c0-4042-97da-fa929d63fdb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "X, y = make_blobs(n_features = 2, n_samples = 1000, centers = 3, cluster_std = 1, random_state = 20111974)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn.datasets.samples_generator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b75eb5cb0b8d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_blobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_blobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20111974\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.datasets.samples_generator'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI4a6SD0mY8b"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HNqoJy__G0-"
      },
      "source": [
        "y[:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v955A9iMlwHa"
      },
      "source": [
        "### Relação entre as variáveis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI0Q8JPWlvVU"
      },
      "source": [
        "f, ax = plt.subplots(figsize=(15, 7))\n",
        "sns.scatterplot(x = X[:, 0], y = X[:, 1], hue = y, data = X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs1igKzHihMn"
      },
      "source": [
        "### Distância ou Similaridade\n",
        "* Dados do mesmo grupo/cluster são similares ao passo que dados pertencentes a diferentes grupos/clusters são diferentes;\n",
        "* Precisamos medir a similaridade e diferenças entre os dados;\n",
        "* Considere as seguintes medidas:\n",
        "\n",
        " - Distância de Minkowiski:\n",
        "\n",
        " <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/4060cc840aeab9e41b5e47356088889e2e7a6f0f\">\n",
        "\n",
        " - Se p = 1 --> Distância de Manhattan;\n",
        " - Se p = 2 --> Euclidiana.\n",
        "\n",
        " - Cosseno: Adequado para dados de texto\n",
        "\n",
        " <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1d94e5903f7936d3c131e040ef2c51b473dd071d\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP8-_fyBihMo"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, manhattan_distances\n",
        "from scipy.spatial.distance import cdist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9YINVqtihM9"
      },
      "source": [
        "## Tipos de Clustering\n",
        "* Métodos de particionamento:\n",
        "  - Particionar N pontos em k partições.\n",
        "  - Inicialmente, partições aleatórias são criadas e gradualmente os dados são movidos para outras partições;\n",
        "  - Usa-se as distâncias entre os pontos para otimizar os clusters;\n",
        "  - Exemplo: KMeans\n",
        "* Métodos Hierárquicos\n",
        "  - Decomposição do dataframe;\n",
        "  - Approach 1: assume cada dado individual como cluster e na sequência os dados vão sendo agrupados conforme a similaridade;\n",
        "  - Approach 2: Começa com 1 cluster para todos os dados e, na sequência, particiona-se em clusters menores;\n",
        "* Métodos Density-based\n",
        "  - Vai acrescentando dados ao cluster até que a densidade exceda um certo threashold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFgu4ZFIihNA"
      },
      "source": [
        "## Métodos de Particionamento\n",
        "### KMeans\n",
        "> K-means é um algoritmo simples de Machine Learning que agrupa um conjunto de dados ou pontos em k clusters especificado pelo usuário.\n",
        ">> O algoritmo é um tanto ingênuo, pois agrupa os dados em k clusters, mesmo que k não seja o número certo de agrupamentos a serem usados. Portanto, ao usar k-means, os usuários precisam de alguma maneira de determinar se estão usando o número certo de clusters.\n",
        ">>> Formas de se determinar o número ideal de clusters:\n",
        "* **Método de Elbow** - um dos métodos mais populares para determinar o valor ótimo de k;\n",
        "* **Silhoute Score**;\n",
        "* **Calinski Harabaz Score**;\n",
        "* **Dendograma**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P979NBTAcfgJ"
      },
      "source": [
        "#### Como funciona o k-Means\n",
        "Animação para entendermos K-Means: http://tech.nitoyon.com/en/blog/2013/11/07/k-means/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPQhWhq4b6ff"
      },
      "source": [
        "#### Elbow Method\n",
        "* O método de Elbow calcula:\n",
        "    * **Distorção**: é a média das distâncias dos centros dos respectivos clusters. Normalmente, a métrica de distância euclidiana é usada.\n",
        "    * **Inércia**: É a soma das distâncias quadradas das amostras ao centro de aglomerado mais próximo.\n",
        "* Para determinar o número ideal de clusters, selecione o valor de k no gráfico de Elbow a partir do qual a distorção/inércia começa a diminuir de maneira linear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ2cS33MbAEO"
      },
      "source": [
        "### Algoritmo K-Means\n",
        "1. Inicializa k centroides de forma aleatória;\n",
        "2. Atribui cada dado/ponto ao centroide mais próximo, criando clusters;\n",
        "3. Recalcula centroide, que é a média de todos os dados/pontos que pertencem a cada cluster;\n",
        "4. Repete os passos 2 e 3 até que não se tenha dados/pontos para atribuir aos centroides;\n",
        "\n",
        "* Os centróides são escolhidos de forma a minimizar a soma dos quadrados do cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XftpzbEihNA"
      },
      "source": [
        "from sklearn.datasets import make_blobs, make_moons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iefqZbJ-d_3j"
      },
      "source": [
        "# Função adaptada de: https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/\n",
        "def Numero_Clusters_Elbow(X):\n",
        "    distortions = []\n",
        "    inertias = []\n",
        "    mapping1 = {}\n",
        "    mapping2 = {}\n",
        "    K = range(1,10)\n",
        "    for k in K:\n",
        "        #Building and fitting the model\n",
        "        kmeanModel = KMeans(n_clusters=k).fit(X)\n",
        "        kmeanModel.fit(X)\n",
        "        distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'),axis=1)) / X.shape[0])\n",
        "        inertias.append(kmeanModel.inertia_)\n",
        "        mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'),axis=1)) / X.shape[0]\n",
        "        mapping2[k] = kmeanModel.inertia_\n",
        "\n",
        "    # Using the different values of Distortion\n",
        "    print('Cálculo da Distorção:')\n",
        "    for key,val in mapping1.items():\n",
        "        print(str(key)+' : '+str(val))\n",
        "\n",
        "    plt.plot(K, distortions, 'bx-')\n",
        "    plt.xlabel('Values of K')\n",
        "    plt.ylabel('Distortion')\n",
        "    plt.title('The Elbow Method using Distortion')\n",
        "    plt.show()\n",
        "\n",
        "    # Using the different values of Inertia\n",
        "    print('Cálculo da Inertia:')\n",
        "    for key,val in mapping2.items():\n",
        "        print(str(key)+' : '+str(val))\n",
        "\n",
        "    plt.plot(K, inertias, 'bx-')\n",
        "    plt.xlabel('Values of K')\n",
        "    plt.ylabel('Inertia')\n",
        "    plt.title('The Elbow Method using Inertia')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ9ZWkPrp74J"
      },
      "source": [
        "### Exemplo 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aCqUXAiihNF"
      },
      "source": [
        "X_ex1, y_ex1 = make_blobs(n_features = 2 , n_samples = 1000, cluster_std = .5, random_state = 20111974)\n",
        "X_ex1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mouoxJ0bpmVk"
      },
      "source": [
        "f, ax = plt.subplots(figsize = (15, 7))\n",
        "plt.scatter(X_ex1[:, 0], X_ex1[:, 1], s = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4xmM9nnYE4R"
      },
      "source": [
        "Quantos clusters tem a figura acima?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFb2rGTiaVX2"
      },
      "source": [
        "Numero_Clusters_Elbow(X_ex1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILsz8vUgI0YJ"
      },
      "source": [
        "Os gráficos de Elbon/Inércia apontam que o número ideal de clusters são 2. Entretanto, vamos pedir ao KMeans para construir 3 clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGuyd7XuHJRh"
      },
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ2t9TnzihNN"
      },
      "source": [
        "# Instancia:\n",
        "kmeans = KMeans(n_clusters = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b75p2eXbHQto"
      },
      "source": [
        "# fit():\n",
        "kmeans.fit(X_ex1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFccFMlihNW"
      },
      "source": [
        "f, ax = plt.subplots(figsize = (15, 7))\n",
        "plt.scatter(X_ex1[:, 0], X_ex1[:, 1], s = 10, c = kmeans.predict(X_ex1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUKjJ-VCJFHU"
      },
      "source": [
        "E agora, com 2 clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zUYs9mxJKel"
      },
      "source": [
        "# Instancia:\n",
        "kmeans = KMeans(n_clusters = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBDBbYOJJKeo"
      },
      "source": [
        "# fit():\n",
        "kmeans.fit(X_ex1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WASJA7EtJKeq"
      },
      "source": [
        "f, ax = plt.subplots(figsize = (15, 7))\n",
        "plt.scatter(X_ex1[:, 0], X_ex1[:, 1], s = 10, c = kmeans.predict(X_ex1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_fQP5sHq8am"
      },
      "source": [
        "### Dendograma\n",
        "* O dendograma corrobora 2 clusters conforme sugerido por Elbow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEISLL7gq8au"
      },
      "source": [
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "plt.figure()\n",
        "dendrogram(ward(X_ex1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXIWrRUlp4eK"
      },
      "source": [
        "### Exemplo 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAc4C5IPihNZ"
      },
      "source": [
        "X_ex2, y_ex2 = make_moons(n_samples = 1000, noise = .09, random_state = 20111974)\n",
        "X_ex2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQIfnf7uihNb"
      },
      "source": [
        "f, ax = plt.subplots(figsize = (15, 7))\n",
        "plt.scatter(X_ex2[:, 0], X_ex2[:, 1],s = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT2gxTLgjb1K"
      },
      "source": [
        "# Quantos clusters tem os dados acima\n",
        "Numero_Clusters_Elbow(X_ex2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDlsKNbtjrqc"
      },
      "source": [
        "Número ideal de clusters são 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7muMLKyihNg"
      },
      "source": [
        "kmeans = KMeans(n_clusters = 2)\n",
        "kmeans.fit(X_ex2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN3uJi4HihNs"
      },
      "source": [
        "f, ax = plt.subplots(figsize = (15, 7))\n",
        "plt.scatter(X_ex2[:, 0], X_ex2[:, 1], s = 10, c = kmeans.predict(X_ex2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5YvOAuIkvWY"
      },
      "source": [
        "### Dendograma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ve7mXeLkvsE"
      },
      "source": [
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "plt.figure()\n",
        "dendrogram(ward(X_ex2))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUwx51ZMihNy"
      },
      "source": [
        "### Limitações do K-Means\n",
        "* Chance/possibilidade de um dado/ponto pertencer à múltiplos clusters;\n",
        "* K-Means tenta encontrar os mínimos locais e isso depende dos valores iniciais que são gerados aleatoriamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9jCRmvHihOh"
      },
      "source": [
        "### Hierarchical Clustering\n",
        "* Combina múltiplos clusters similares para criar um cluster ou OU particionar um cluster para criar clusters menores de forma a agrupar dados/pontos similares;\n",
        "* Tipos de hierarchaial Clustering:\n",
        "  - Agglomerative Method - botton-up approach.\n",
        "  - Divisive Method - top-down approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-EbOvffihOi"
      },
      "source": [
        "#### Agglomerative Method\n",
        "* Inicia atribuindo um cluster para cada dado/ponto;\n",
        "* Combina clusters que possuem alta medida de similaridade;\n",
        "* As diferenças entre os métodos surgem devido a diferentes maneiras de definir a distância (ou similaridade) entre os clusters. As seções a seguir descrevem várias técnicas aglomerativas em detalhes.\n",
        "  - Single Linkage Clustering\n",
        "  - Complete linkage clustering\n",
        "  - Average linkage clustering\n",
        "  - Average group linkage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGz2r7C-rRbU"
      },
      "source": [
        "### Exemplo 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9gF2_vihOk"
      },
      "source": [
        "X_ex3, y_ex3 = make_moons(n_samples = 1000, noise = .05, random_state = 20111974)\n",
        "X_ex3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dlub92DrVcB"
      },
      "source": [
        "f, ax = plt.subplots(figsize = (15, 7))\n",
        "plt.scatter(X_ex3[:, 0], X_ex3[:, 1], s = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IyYHzBNihOm"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "agc = AgglomerativeClustering(linkage = 'single', n_clusters = 2)\n",
        "agc.fit(X_ex3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-s9i6e_ihOs"
      },
      "source": [
        "f, ax = plt.subplots(figsize = (15, 7))\n",
        "plt.scatter(X_ex3[:, 0], X_ex3[:, 1], s = 10, c = agc.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKm1-cDllr2a"
      },
      "source": [
        "### Dendograma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf6l98Zflr2c"
      },
      "source": [
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "plt.figure()\n",
        "dendrogram(ward(X_ex3))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8arpf1UihOu"
      },
      "source": [
        "## Density Based Clustering - DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_9Hub5ZsWlh"
      },
      "source": [
        "### Exemplo 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzMsUK4rihOv"
      },
      "source": [
        "centers = [[1, 1], [-1, -1], [1, -1]]\n",
        "X_ex4, labels_true = make_blobs(n_samples=750, centers = centers, cluster_std = 0.4, random_state = 20111974)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq9uU5bqihOw"
      },
      "source": [
        "f, ax = plt.subplots(figsize = (15, 7))\n",
        "plt.scatter(X_ex4[:, 0], X_ex4[:, 1], s = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMpXy7SihOy"
      },
      "source": [
        "from sklearn.cluster import DBSCAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWrG35e2ihO0"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X_ex4 = StandardScaler().fit_transform(X_ex4)\n",
        "\n",
        "db = DBSCAN(eps = 0.3, min_samples = 10).fit(X_ex4)\n",
        "core_samples_mask = np.zeros_like(db.labels_, dtype = bool)\n",
        "core_samples_mask[db.core_sample_indices_] = True\n",
        "labels = db.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5SbRf22ihO2"
      },
      "source": [
        "f, ax = plt.subplots(figsize = (15, 7))\n",
        "plt.scatter(X_ex4[:, 0], X_ex4[:, 1], s = 10, c = labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mITopD3AihO4"
      },
      "source": [
        "# Medir a Performance dos Clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHZj39OQihO6"
      },
      "source": [
        "## completeness_score\n",
        "- 'Completeness' significa que todos os pontos/dados que são membros de uma determinada classe são elementos do mesmo cluster.\n",
        "- Accuracy é 1.0 se o dado/ponto pertencente à mesma classe também pertence ao mesmo cluster, mesmo que múltiplas classes pertençam ao mesmo cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIGJMfEuihO_"
      },
      "source": [
        "from sklearn.metrics.cluster import completeness_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMCiwctFihPD"
      },
      "source": [
        "completeness_score(labels_true = [10, 10, 11, 11], labels_pred = [1, 1, 0, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGbXVL2AihPG"
      },
      "source": [
        "* Acurácia= 1 porque todos os dados/pontos pertencentes à mesma classe também pertence ao mesmo cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVYHsgJPihPG"
      },
      "source": [
        "completeness_score(labels_true = [11, 22, 22, 11], labels_pred = [1, 0, 1, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIqpTk98ihPK"
      },
      "source": [
        "Porque a Acurácia = 0.3?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aIUD1k5ihPN"
      },
      "source": [
        "print(completeness_score([10, 10, 11, 11], [0, 0, 0, 0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2bkuNClfIfs"
      },
      "source": [
        "Porque a Acurácia= 1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m_g6F9KihPQ"
      },
      "source": [
        "## Homogeneity_score\n",
        "- Uma clusterização satisfaz a homogeneidade se todos os seus clusters contiverem apenas pontos/dados que são membros de uma única classe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqnyZofuihPR"
      },
      "source": [
        "from sklearn.metrics.cluster import homogeneity_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLGBAY7CihPT"
      },
      "source": [
        "homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXblz2lRihPV"
      },
      "source": [
        "homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDZjiUhAihPX"
      },
      "source": [
        "homogeneity_score([0, 0, 0, 0], [1, 1, 0, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04U6xi5UihPZ"
      },
      "source": [
        "* Mesma classe subdividida em 2 clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAUqOTB8ihPa"
      },
      "source": [
        "## silhoutte_score\n",
        "* Calculado usando a distância intra-cluster média (a) e a distância média do cluster mais próximo (b) para cada amostra.\n",
        "* **Decisão**: Quanto Maior --> Melhor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyyz0LJNihPb"
      },
      "source": [
        "### Exemplo 5\n",
        "* Selecionar o número de clusters usando silhoutte_score no KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxJeWuiKihPc"
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X_ex5, y_ex5 = make_blobs(n_samples = 1000,\n",
        "                          n_features = 2,\n",
        "                          centers = 4,\n",
        "                          cluster_std = 1,\n",
        "                          center_box = (-10.0, 10.0),\n",
        "                          shuffle = True,\n",
        "                          random_state = 20111974)\n",
        "\n",
        "X_ex5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb_sZGoiihPg"
      },
      "source": [
        "f, ax = plt.subplots(figsize = (15, 7))\n",
        "plt.scatter(X_ex5[:, 0], X_ex5[:, 1], s = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0vVAa6WihPl"
      },
      "source": [
        "range_n_clusters = [2, 3, 4, 5, 6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWEXVYuQihPq"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtPAcysIihPt"
      },
      "source": [
        "for n_cluster in range_n_clusters:\n",
        "    kmeans = KMeans(n_clusters=n_cluster)\n",
        "    kmeans.fit(X_ex5)\n",
        "    labels = kmeans.predict(X_ex5)\n",
        "    print (n_cluster, silhouette_score(X_ex5, labels))\n",
        "    print('------------------------------------------')\n",
        "    #print (n_cluster, completeness_score(X_ex5, labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs15UzKEyQTG"
      },
      "source": [
        "f, ax = plt.subplots(figsize = (15, 7))\n",
        "plt.scatter(X_ex5[:, 0], X_ex5[:, 1], s = 10, c = labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SOy7IQEihPw"
      },
      "source": [
        "* O número ótimo/recomendado de cluster é 2. Porque?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2bltW_Agrmf"
      },
      "source": [
        "## calinski_harabaz_score\n",
        "* Este score é calculado como razão entre a dispersão dentro do cluster e a dispersão entre cluster.\n",
        "* **Decisão**: Quanto menor --> Melhor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGf_A5vFihPx"
      },
      "source": [
        "from sklearn.metrics import calinski_harabaz_score\n",
        "\n",
        "for n_cluster in range_n_clusters:\n",
        "    kmeans = KMeans(n_clusters= n_cluster)\n",
        "    kmeans.fit(X_ex5)\n",
        "    labels = kmeans.predict(X_ex5)\n",
        "    print (n_cluster, calinski_harabaz_score(X_ex5, labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWq6Mz85j-Td"
      },
      "source": [
        "Numero_Clusters_Elbow(X_ex5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Components Analysis\n",
        "- O PCA é uma técnica de redução de dimensionalidade (número de colunas/features de uma tabela) que transforma as features de um dataset em componentes principais. Ele ajuda a identificar as dimensões mais relevantes para a análise, preservando a maior parte da variabilidade dos dados. O PCA ajuda a reduzir o número de colunas correlacionadas, removendo a redundância nos dados. Em outras palavras, o PCA mantém (ou mostra) quais são as colunas mais importantes para os modelos de Machine Learning.\n",
        "\n",
        "    - Etapas principais para usar o PCA:\n",
        "        * Entender o Dataset: Entender as correlações entre as colunas é muito importante para Machine Learning de forma geral;\n",
        "\n",
        "        * Pré-Processamento: Escalar os dados para garantir que todas as variáveis estejam na mesma escala (usando StandardScaler ou MinMaxScaler).\n",
        "\n",
        "        * Aplicar o PCA: Transformar os dados originais em componentes principais.\n",
        "\n",
        "        * Interpretar os Resultados: Identificar quais componentes principais retêm mais variabilidade e decidir quantos componentes usar."
      ],
      "metadata": {
        "id": "Ioci7EOD6Px4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seguir, criamos um dataframe fake usando a library Faker para aprendermos os principais conceitos por trás do PCA."
      ],
      "metadata": {
        "id": "VxErpN7d6TAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "TayV9H_K_P5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-importar as bibliotecas após o reset do ambiente\n",
        "from faker import Faker\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Inicializar o Faker e definir a semente aleatória\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "\n",
        "# Gerar um conjunto de dados sintético com 5000 linhas e 10 colunas\n",
        "n_rows = 5000\n",
        "n_columns = 10\n",
        "\n",
        "# Criar dados aleatórios para as colunas\n",
        "np.random.seed(42)\n",
        "data = {\n",
        "    f\"Feature_{i+1}\": np.random.uniform(10, 100, n_rows) + np.random.normal(0, 10, n_rows)\n",
        "    for i in range(n_columns)\n",
        "}\n",
        "\n",
        "# Adicionar correlações leves entre algumas features\n",
        "data[\"Feature_2\"] = data[\"Feature_1\"] + np.random.normal(0, 5, n_rows)\n",
        "data[\"Feature_3\"] = data[\"Feature_2\"] * 0.5 + data[\"Feature_4\"] * 0.3\n",
        "\n",
        "# Converter para um DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Exibir o conjunto de dados gerado para o usuário\n",
        "df.head()"
      ],
      "metadata": {
        "id": "FcliiTNx7DDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Escalar os Dados"
      ],
      "metadata": {
        "id": "N9JWGURg_fyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Escalar o dataset\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "scaled_data"
      ],
      "metadata": {
        "id": "tYUX8VhT_c9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Aplicar PCA"
      ],
      "metadata": {
        "id": "NYfbiPii_s_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular a variância explicada acumulada usando PCA\n",
        "pca = PCA(n_components=10)\n",
        "pca.fit(df)\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Gerar um gráfico para visualizar a variância explicada acumulada\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, 11), cumulative_variance, marker='o', linestyle='--', linewidth=2)\n",
        "plt.xticks(range(1, 11))\n",
        "plt.title(\"Variância Explicada Acumulada por Número de Componentes\", fontsize=16)\n",
        "plt.xlabel(\"Número de Componentes Principais\", fontsize=14)\n",
        "plt.ylabel(\"Variância Explicada Acumulada\", fontsize=14)\n",
        "plt.grid(alpha=0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qly2YRuV_1Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mfuzjsSBFKkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_variance"
      ],
      "metadata": {
        "id": "f0RZh-PRBywc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Selecionar o Número de Componentes"
      ],
      "metadata": {
        "id": "Xbyuayz4_8Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Gráfico da variância explicada acumulada\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, 11), cumulative_variance, marker='o', linestyle='--')\n",
        "plt.xlabel(\"Número de Componentes\")\n",
        "plt.ylabel(\"Variância Explicada Acumulada\")\n",
        "plt.title(\"Escolha do Número de Componentes\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sH-j-uKjAHCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EXPLICAÇÃO:\n",
        "- Olhando o gráfico, observamos que a variância acumulada atinge 100% no 8º componente.\n",
        "- Nesse caso, escolhemos 8 componentes principais, porque eles explicam a maior parte da variância.\n",
        "- Se, por outro lado, quisermos reduzir ainda mais o número de componentes principais, podemos por exemplo escolher 7 componentes principais, pois a variância explicada é aproximadamente igual a 90%."
      ],
      "metadata": {
        "id": "4FfmHks4FRjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Reduzir Dimensionalidade\n",
        "- Abaixo eu escolhi 2 componentes principais somente para mostrar o gráfico. Para número de componentes principais maior que 3, não conseguimos visualizar o gráfico."
      ],
      "metadata": {
        "id": "RUrA_d7lALE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionar os 2 principais componentes\n",
        "pca_2 = PCA(n_components=2)\n",
        "pca_2_result = pca_2.fit_transform(scaled_data)\n",
        "\n",
        "# Transformar os dados e criar um novo DataFrame com os componentes principais\n",
        "df_pca_2d = pd.DataFrame(data=pca_2_result, columns=[\"PC1\", \"PC2\"])\n",
        "df_pca_2d[\"Label\"] = \"Cluster\"  # Pode adicionar clusters ou categorias para análise\n",
        "df_pca_2d.head()"
      ],
      "metadata": {
        "id": "0cYtIW0ZAQJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Visualizar os Clusters em 2D\n"
      ],
      "metadata": {
        "id": "DD2RLnqBAV6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfico dos dois principais componentes\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df_pca_2d[\"PC1\"], df_pca_2d[\"PC2\"], alpha=0.5)\n",
        "plt.xlabel(\"Componente Principal 1\")\n",
        "plt.ylabel(\"Componente Principal 2\")\n",
        "plt.title(\"Visualização com os Dois Principais Componentes\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3A2znwgaAb0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusão sobre o PCA:\n",
        "O PCA simplifica os dados reduzindo sua dimensionalidade, o que facilita a análise e visualização sem perder muita variabilidade. Isso é especialmente útil em clustering, classificação e visualizações de dados multidimensionais."
      ],
      "metadata": {
        "id": "ddJnpUOY_j0Q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF7ltoH7QHT5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercícios\n",
        "Considere o dataframe a seguir, criado através da library faker."
      ],
      "metadata": {
        "id": "_4wRBx_TMPWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Inicializa a criação dos dados fakes\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define o número de linhas e colunas do dataframe\n",
        "n_rows = 5000\n",
        "n_columns = 20\n",
        "\n",
        "data = {\n",
        "    'ID': range(1, n_rows + 1),\n",
        "    'Name': [fake.name() for _ in range(n_rows)],\n",
        "    'Age': np.random.randint(18, 70, n_rows),\n",
        "    'Salary': np.random.uniform(20000, 120000, n_rows),\n",
        "    'Department': [fake.job() for _ in range(n_rows)],\n",
        "    'Date_of_Joining': [fake.date_this_century() for _ in range(n_rows)],\n",
        "    'Performance_Score': np.random.uniform(0, 10, n_rows),\n",
        "    'Location': [fake.city() for _ in range(n_rows)],\n",
        "    'Marital_Status': np.random.choice(['Single', 'Married', 'Divorced'], n_rows),\n",
        "    'Children': np.random.randint(0, 5, n_rows),\n",
        "    'Home_Loan': np.random.choice([True, False], n_rows),\n",
        "    'Education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_rows),\n",
        "    'Work_Experience': np.random.randint(1, 30, n_rows),\n",
        "    'Health_Score': np.random.uniform(50, 100, n_rows),\n",
        "    'Bonus_Amount': np.random.uniform(1000, 15000, n_rows),\n",
        "    'Travel_Distance': np.random.uniform(1, 50, n_rows),\n",
        "    'Commute_Time': np.random.randint(10, 120, n_rows),\n",
        "    'Annual_Leave_Days': np.random.randint(10, 30, n_rows),\n",
        "    'Job_Level': np.random.choice(['Entry', 'Mid', 'Senior', 'Executive'], n_rows),\n",
        "    'Attrition_Flag': np.random.choice([True, False], n_rows)\n",
        "}\n",
        "\n",
        "# Cria o dataframe\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "hOW1UeFVMcyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Preparação dos Dados\n",
        "- Exercício: Use o dataframe criado anteriorment. Preprocessar os dados para garantir que todas as features estejam escaladas (usando `StandardScaler` ou `MinMaxScaler`). Por que é importante escalar os dados antes de aplicar PCA ou clustering?\n",
        "\n",
        "2. Aplicação de K-Means\n",
        "- Exercício: Aplique o algoritmo K-Means ao dataset com diferentes valores de `k` (número de clusters). Para cada valor de `k`, calcule a soma dos erros quadrados (SSE) e determine o melhor número de clusters usando o método do cotovelo.\n",
        "\n",
        "3. PCA para Redução de Dimensionalidade\n",
        "- Exercício: Aplique PCA ao dataset para reduzir suas dimensões para 2 componentes principais. Visualize os dados em um gráfico de dispersão (scatter plot) usando as duas primeiras componentes principais.\n",
        "\n",
        "4. Combinação de PCA e K-Means\n",
        "- Exercício: Use PCA para reduzir o dataset para 2 componentes principais e, em seguida, aplique o K-Means ao dataset reduzido. Visualize os clusters no espaço bidimensional das componentes principais.\n",
        "\n",
        "5. Interpretação da Variância Explicada\n",
        "- Exercício: Após aplicar PCA, calcule e visualize a variância explicada por cada componente principal. Quantos componentes são necessários para explicar 90% da variância nos dados?\n",
        "\n",
        "6. Comparação entre K-Means e Hierarchical Clustering\n",
        "- Exercício: Aplique tanto o K-Means quanto o agrupamento hierárquico ao mesmo dataset. Compare os resultados dos clusters formados pelas duas abordagens.\n",
        "\n",
        "7. Avaliação dos Clusters\n",
        "- Exercício: Após formar clusters com K-Means, avalie a qualidade dos clusters usando métricas como Silhouette Score e Calinski-Harabasz Index. Qual métrica é mais confiável para o seu dataset?\n",
        "\n",
        "8. Aplicação de DBSCAN (Density-Based Spatial Clustering)\n",
        "- Exercício: Use o algoritmo DBSCAN para agrupar os dados. Ajuste os parâmetros `eps` e `min_samples` para melhorar a detecção de clusters. Em quais situações DBSCAN é mais eficaz do que K-Means?\n",
        "\n",
        "9. Identificação de Outliers com PCA\n",
        "- Exercício: Após aplicar PCA, identifique e visualize outliers no espaço das componentes principais. Dica: Use um gráfico de dispersão para destacar os pontos que estão longe da média.\n",
        "\n",
        "10. Clustering com PCA e Análise de Resultados\n",
        "- Exercício: Combine PCA com um método de clustering (como K-Means ou DBSCAN) para criar grupos. Em seguida, analise cada grupo formado para identificar características comuns entre os dados em cada cluster."
      ],
      "metadata": {
        "id": "2AaVPHvhLwtY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DM9qxpVVVfZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}