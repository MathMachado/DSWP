{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "NB15_00__Machine_Learning.ipynb",
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathMachado/DSWP/blob/master/Notebooks/NB15_ML_SL_BOOTSTRAPPING_METHODS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShVXyGj9wkgN"
      },
      "source": [
        "<center><h1><b><i>MACHINE LEARNING | APRENDIZAGEM SUPERVISIONADA | BOOTSTRAPPING METHODS</i></b></h1></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvwp5UHdBiup"
      },
      "source": [
        "___\n",
        "# **NOSSO FOCO AQUI SERÁ APRENDIZAGEM SUPERVISIONADA E APRENDIZAGEM NÃO-SUPERVISIONADA\n",
        "\n",
        "![ClassicalML](https://github.com/MathMachado/Materials/blob/master/ClassicalML.jpg?raw=true)\n",
        "\n",
        "Source: [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/?source=post_page-----885aa35db58b----------------------)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBLSvJTXHBjK"
      },
      "source": [
        "___\n",
        "# **CHEETSHEET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdjR3nahUuKq"
      },
      "source": [
        "\n",
        "![Scikit-Learn](https://github.com/MathMachado/Materials/blob/master/scikit-learn-1.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_1A6lC9SF53"
      },
      "source": [
        "# **BOOTSTRAPPING METHODS**\n",
        "> Antes de falarmos de Boosting ou Bagging, precisamos entender primeiro o que é Bootstrap, pois ambos (Boosting e Bagging) são baseados em Bootstrap.\n",
        "\n",
        "* Em Estatística (e em Machine Learning), Bootstrap se refere à extrair amostras aleatórias COM reposição da população X."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX_gfsbQSdNd"
      },
      "source": [
        "___\n",
        "# **BOOSTING MODELS**\n",
        "* São algoritmos muito utilizados nas competições do Kaggle;\n",
        "* São algoritmos utilizados para melhorar a performance dos algoritmos de Machine Learning;\n",
        "* Modelos:\n",
        "    - [X] AdaBoost\n",
        "    - [X] XGBoost\n",
        "    - [X] LightGBM\n",
        "    - [X] GradientBoosting\n",
        "    - [X] CatBoost\n",
        "\n",
        "## Bagging vs Boosting\n",
        "### **Bagging**\n",
        "* **Objetivo**: é reduzir a variância;\n",
        "* **Vantagens**:\n",
        "    * Reduz overfitting;\n",
        "    * Lida bem com dataframes com muitas COLUNAS (high dimensionality);\n",
        "    * Lida automaticamente com Missing Values;\n",
        "* **Desvantagens**:\n",
        "    * A predição final é baseada na média das K Decision Trees, o que pode comprometer a acurácia final.\n",
        "\n",
        "### **Boosting**\n",
        "* **Objetivo**: é melhorar acurácia;\n",
        "* **Vantagens**:\n",
        "    * Lida bem com dataframes com muitas COLUNAS (high dimensionality);\n",
        "    * Lida automaticamente com Missing Values;\n",
        "* **Desvantagens**:\n",
        "    * Propenso a overfitting --> Recomenda-se fortemente o tratamento dos outliers previamente.\n",
        "    * Requer ajuste cuidadoso dos hyperparameters;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqJV-QGFQx90"
      },
      "source": [
        "#### **Bagging**: Como funciona\n",
        "* Seleciona várias amostras **COM REPOSIÇÃO** do dataframe de treinamento. Cada amostra é usada para treinar um modelo usando Decision Trees. Como resultado, temos um ensemble de muitas e diferentes modelos (Decision Trees). A média de desses muitos e diferentes modelos (Decision Trees) são usados para produzir o resultado final;\n",
        "* O resultado final é mais robusto do que usarmos uma simples Decision Tree.\n",
        "\n",
        "![Bagging](https://github.com/MathMachado/Materials/blob/master/Bagging.png?raw=true)\n",
        "\n",
        "Souce: [Boosting and Bagging: How To Develop A Robust Machine Learning Algorithm](https://hackernoon.com/how-to-develop-a-robust-algorithm-c38e08f32201).\n",
        "\n",
        "#### Steps\n",
        "* Suponha um dataframe X_treinamento (dataframe de treinamento) contendo N observações (instâncias, pontos, linhas) e M COLUNAS (features, atributos).\n",
        "    1. Bagging seleciona aleatoriamente uma amostra **COM REPOSIÇÃO** de X_treinamento;\n",
        "    2. Bagging seleciona aleatoriamente M2 (M2 < M) COLUNAS do dataframe extraído do passo (1);\n",
        "    3. Constroi uma Decision Tree com as M2 COLUNAS do passo (2) e o dataframe obtido no passo (1) e as COLUNAS são avaliadas pela sua habilidade de classificar as observações;\n",
        "    4. Os passos (1)--> (2)-- (3) são repetidos K vezes (ou seja, K Decision Trees), de forma que as COLUNAS são ranqueadas pelo seu poder preditivo e o resultado final (acurácia, por exemplo) é obtido pela agregação das predições dos K Decision Trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EysHMsU9Q6nQ"
      },
      "source": [
        "___\n",
        "#### **Boosting**: Como funciona\n",
        "* Os classificadores são usados sequencialmente, de forma que o classificador no passo N aprende com os erros do classificador do passo N-1. Ou seja, o objetivo é melhorar a precisão/acurácia à cada passo aprendendo com o passado.\n",
        "\n",
        "![Boosting](https://github.com/MathMachado/Materials/blob/master/Boosting.png?raw=true)\n",
        "\n",
        "Source: [Ensemble methods: bagging, boosting and stacking](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205), Joseph Rocca\n",
        ".\n",
        "\n",
        "#### Steps\n",
        "* Suponha um dataframe X_treinamento (dataframe de treinamento) contendo N observações (instâncias, pontos, linhas) e M COLUNAS (features, atributos).\n",
        "    1. Boosting seleciona aleatoriamente uma amostra D1 SEM reposição de X_treinamento;\n",
        "    2. Boosting treina o classificador C1;\n",
        "    3. Boosting seleciona aleatoriamente a SEGUNDA amostra D2 SEM reposição de X_treinamento e acrescenta à D2 50% das observações que foram classificadas incorretamente para treinar o classificador C2;\n",
        "    4. Boosting encontra em X_treinamento a amostra D3 que os classificadores C1 e C2 discordam em classificar e treina C3;\n",
        "    5. Combina (voto) as predições de C1, C2 e C3 para produzir o resultado final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48-kevGahsbk"
      },
      "source": [
        "### Bagging vs Boosting\n",
        "\n",
        "|Similaridades | Diferenças |\n",
        "|--------------|------------|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyqazmUuifkE"
      },
      "source": [
        "___\n",
        "# **ADABOOST(Adaptive Boosting)**\n",
        "* Foi um dos primeiros algoritmos de Boosting (1995);\n",
        "* AdaBoost pode ser utilizado tanto para classificação (AdaBoostClassifier) quanto para Regressão (AdaBoostRegressor);\n",
        "* AdaBoost usam algoritmos DecisionTree como base_estimator;\n",
        "* É um dos classificadores do tipo ensemble (combina vários classificadores para aumentar a precisão).\n",
        "    * AdaBoost é um classificador iterativo e forte que combina (ensemble) vários classificadores fracos para melhorar a precisão.\n",
        "* Qualquer algoritmo de aprendizado de máquina pode ser usado como um classificador de base (parâmetro base_estimator);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU-vzkXqrFVw"
      },
      "source": [
        "## Referências\n",
        "* [AdaBoost Classifier Example In Python](https://towardsdatascience.com/machine-learning-part-17-boosting-algorithms-adaboost-in-python-d00faac6c464) - Didático e explica exatamente como o AdaBoost funciona.\n",
        "* [Adaboost for Dummies: Breaking Down the Math (and its Equations) into Simple Terms](https://towardsdatascience.com/adaboost-for-dummies-breaking-down-the-math-and-its-equations-into-simple-terms-87f439757dcf) - Para quem quer entender a matemática por trás do algoritmo.\n",
        "* [Gradient Boosting and XGBoost](https://medium.com/hackernoon/gradient-boosting-and-xgboost-90862daa6c77)\n",
        "* [Understanding AdaBoost](https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe), Akash Desarda.\n",
        "* [AdaBoost Classifier Example In Python](https://towardsdatascience.com/machine-learning-part-17-boosting-algorithms-adaboost-in-python-d00faac6c464)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EMrjQDZIMl_"
      },
      "source": [
        "## Hiperparâmetros mais importantes do AdaBoost:\n",
        "* base_estimator - É um classificador usado para treinar o modelo. Como default, AdaBoost usa o DecisionTreeClassifier. Pode-se utilizar diferentes algoritmos para esse fim.\n",
        "* n_estimators - Número de base_estimator para treinar iterativamente.\n",
        "* learning_rate - Controla a contribuição do base_estimator na solução/combinação final;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrj4a4s6hMMB"
      },
      "source": [
        "## Vantagens\n",
        "* AdaBoost é fácil de implementar;\n",
        "* AdaBoost corrige os erros do base_estimator iterativamente e melhora a acurácia;\n",
        "* Faz o Feature Selection automaticamente (**Porque**?);\n",
        "* Pode-se usar muitos algoritos como base_estimator ;\n",
        "* Como é um método ensemble, então o modelo final é pouco propenso à overfitting.\n",
        "\n",
        "## Desvantagens\n",
        "* AdaBoost é sensível a ruídos nos dados;\n",
        "* Altamente impactado por outliers (contribui para overfitting), pois o algoritmo tenta se ajustr a cada ponto da mehor forma possível;\n",
        "* AdaBoost é mais lento que XGBoost;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwWkjfC8KEZH"
      },
      "source": [
        "# ENSEMBLE METHODS\n",
        "https://towardsdatascience.com/using-bagging-and-boosting-to-improve-classification-tree-accuracy-6d3bb6c95e5b\n",
        "\n",
        "![Ensemble](https://github.com/MathMachado/Materials/blob/master/Ensemble.png?raw=true)"
      ]
    }
  ]
}