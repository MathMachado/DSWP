{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP9XQm4DHTmaCINk5MPuVtz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathMachado/DSWP/blob/master/PCA%2C%20t-SNE%20and%20UMAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iris Dataset\n",
        "\n",
        "Here's an example using the Iris dataset, which contains 4 features. We will apply PCA, t-SNE, and UMAP to reduce the dimensionality to 2D and visualize the results."
      ],
      "metadata": {
        "id": "HrFIIBd5H1dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install umap-learn"
      ],
      "metadata": {
        "id": "mgL4a-GxGwVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "# UMAP\n",
        "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "X_umap = umap_reducer.fit_transform(X_scaled)\n",
        "\n",
        "# Plot the results\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axs[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "axs[0].set_title(\"PCA\")\n",
        "\n",
        "axs[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
        "axs[1].set_title(\"t-SNE\")\n",
        "\n",
        "axs[2].scatter(X_umap[:, 0], X_umap[:, 1], c=y)\n",
        "axs[2].set_title(\"UMAP\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2zyXGkMPGvtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This code will generate three scatterplots, each representing the reduced 2D representation of the Iris dataset using PCA, t-SNE, and UMAP. The colors indicate the different classes in the Iris dataset.\n",
        "\n",
        "You will notice that PCA creates a linear separation between the classes, but there is some overlap between two of the classes. On the other hand, t-SNE and UMAP produce more distinct clusters, with better separation between all three classes. This shows that t-SNE and UMAP are better at capturing the complex, non-linear structures in the data. However, PCA provides a faster and more interpretable solution, which can be useful in certain scenarios."
      ],
      "metadata": {
        "id": "YkJB6SIPH6eE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wine Dataset\n",
        "\n",
        "We will apply PCA, t-SNE, and UMAP to reduce the dimensionality to 2D and visualize the results."
      ],
      "metadata": {
        "id": "W6jIlXMMHy2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "# UMAP\n",
        "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "X_umap = umap_reducer.fit_transform(X_scaled)\n",
        "\n",
        "# Plot the results\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axs[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "axs[0].set_title(\"PCA\")\n",
        "\n",
        "axs[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
        "axs[1].set_title(\"t-SNE\")\n",
        "\n",
        "axs[2].scatter(X_umap[:, 0], X_umap[:, 1], c=y)\n",
        "axs[2].set_title(\"UMAP\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0OP-81dNHx5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will observe that PCA separates the classes reasonably well, but there is still some overlap between two of the classes. t-SNE and UMAP provide better separation between the classes and reveal more distinct clusters. This demonstrates that t-SNE and UMAP can capture complex, non-linear structures in the data more effectively than PCA. However, PCA offers a faster and more interpretable approach, which might be beneficial in certain situations."
      ],
      "metadata": {
        "id": "46WPh02hIveV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting the most important features"
      ],
      "metadata": {
        "id": "Y1ptv82fJyxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "feature_names = wine.feature_names\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Get PCA components\n",
        "components = pca.components_\n",
        "\n",
        "# Get explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Calculate the contribution of each feature to the first two principal components\n",
        "# (or another number of components based on your preference)\n",
        "num_components = 2\n",
        "feature_contributions = np.abs(components[:num_components]).sum(axis=0)\n",
        "\n",
        "# Get the indices of the top N features\n",
        "N = 5  # Number of top features to select\n",
        "top_feature_indices = np.argsort(feature_contributions)[-N:]\n",
        "\n",
        "# Get the top N features' names\n",
        "top_features = [feature_names[i] for i in top_feature_indices]\n",
        "\n",
        "print(\"Top\", N, \"features:\")\n",
        "print(top_features)\n"
      ],
      "metadata": {
        "id": "xrF8_VGEIC3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection"
      ],
      "metadata": {
        "id": "X6k8cmEsKlgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import RFE, SelectKBest, f_classif\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "feature_names = wine.feature_names\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# RFE\n",
        "svm_estimator = SVC(kernel=\"linear\")\n",
        "rfe = RFE(estimator=svm_estimator, n_features_to_select=5)\n",
        "rfe.fit(X_scaled, y)\n",
        "rfe_features = np.array(feature_names)[rfe.support_]\n",
        "\n",
        "# SelectKBest\n",
        "kbest = SelectKBest(score_func=f_classif, k=5)\n",
        "kbest.fit(X_scaled, y)\n",
        "kbest_features = np.array(feature_names)[kbest.get_support()]\n",
        "\n",
        "# LASSO\n",
        "lasso = LassoCV(cv=5)\n",
        "lasso.fit(X_scaled, y)\n",
        "lasso_features = np.array(feature_names)[np.abs(lasso.coef_) > 1e-5]\n",
        "\n",
        "# Random Forest feature importances\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_scaled, y)\n",
        "importances = rf.feature_importances_\n",
        "rf_features = np.array(feature_names)[importances > np.mean(importances)]\n",
        "\n",
        "print(\"RFE selected features:\", rfe_features)\n",
        "print(\"SelectKBest selected features:\", kbest_features)\n",
        "print(\"LASSO selected features:\", lasso_features)\n",
        "print(\"Random Forest selected features:\", rf_features)\n"
      ],
      "metadata": {
        "id": "_GCg6ujGJ8oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RFE (Recursive Feature Elimination)\n",
        "\n",
        "### Pros:\n",
        "Can be used with any estimator that exposes a coef_ or feature_importances_ attribute.\n",
        "Considers interactions between features.\n",
        "Can provide better performance when irrelevant features are present.\n",
        "\n",
        "### Cons:\n",
        "Computationally expensive as it requires fitting the model multiple times.\n",
        "\n",
        "### When to use: When you have a supervised learning problem and want to consider interactions between features.\n",
        "\n",
        "### How to interpret: The selected features are the ones that contribute the most to the model's performance according to the estimator used."
      ],
      "metadata": {
        "id": "A02WFBaELVwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SelectKBest\n",
        "\n",
        "### Pros:\n",
        "Fast and efficient for selecting the top K features based on univariate statistical tests.\n",
        "Easy to interpret.\n",
        "\n",
        "### Cons:\n",
        "Ignores interactions between features.\n",
        "Assumes that features are independent.\n",
        "\n",
        "### When to use: When you want a quick and simple way to select a subset of features based on their individual importance.\n",
        "\n",
        "### How to interpret: The selected features are the ones that have the highest scores according to the chosen statistical test."
      ],
      "metadata": {
        "id": "lz_4uOl2Lkec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LASSO (Least Absolute Shrinkage and Selection Operator)\n",
        "\n",
        "### Pros:\n",
        "Performs feature selection and regression simultaneously.\n",
        "Can handle high-dimensional datasets and multicollinearity.\n",
        "\n",
        "### Cons:\n",
        "Assumes a linear relationship between features and target variable.\n",
        "Can have difficulty selecting the correct features when there are groups of highly correlated features.\n",
        "\n",
        "### When to use: When you have a linear regression problem and want a sparse model\n",
        "\n",
        "### How to interpret: The selected features are the ones with non-zero coefficients in the LASSO model. Features with larger absolute coefficients have a stronger impact on the target variable.\n"
      ],
      "metadata": {
        "id": "eRT6Jo2NLqbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importances from tree-based models (Random Forest, XGBoost, etc.)\n",
        "\n",
        "### Pros:\n",
        "Can handle non-linear relationships and interactions between features.\n",
        "Robust to outliers.\n",
        "Provides a measure of feature importance directly from the model.\n",
        "\n",
        "### Cons:\n",
        "Random Forest can be computationally expensive for large datasets.\n",
        "The importance measure can be biased towards high-cardinality categorical features.\n",
        "\n",
        "### When to use: When you have a supervised learning problem, and you want a more robust way to assess feature importances that can handle non-linear relationships and interactions.\n",
        "\n",
        "### How to interpret: The selected features are the ones with higher importance scores according to the tree-based model. Higher importance scores indicate a stronger contribution to the model's performance."
      ],
      "metadata": {
        "id": "1KVPMaMwL0p7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When choosing a feature selection method, consider the type of problem (regression or classification), the relationship between features and the target variable (linear or non-linear), the presence of interactions between features, and the computational complexity of the method. Some methods may work better for certain datasets and problem types, so it can be helpful to experiment with multiple methods and evaluate their performance on your specific task."
      ],
      "metadata": {
        "id": "RyHk1JrVL7m0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e3dk0ZsHKrnE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}